[
  {
    "path": "posts/2021-11-01-plotting-shapefiles-in-python/",
    "title": "Plotting Shapefiles in Python",
    "description": "The basics for reading in shapefile data, plotting it, and adding a basemap.",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-11-01",
    "categories": [
      "-Python"
    ],
    "contents": "\r\nPython and Jupyter Notebooks\r\nTransitioning from R to Python can be difficult, as is learning any new language. We have to say goodbye to the exceptionally user-friendly RStudio interface with its git GUI, visible environment, and easily accessible console. Let’s dive into the objected oriented language of Python through Jupyter Notebooks and the mysterious terminal. Personally, I struggled every step of this journey with lots of syntax errors, merge conflicts, and endless Google searching. I hope this post will help hold your hand as we make this change together.\r\nFirst things first, if you’re using Jupyter Notebooks to code in Python, you’re gonna have to install Anaconda Navigator and install conda into your home directory. As a PC user, installing conda really threw me and my laptop for a loop. But what better way to learn to use the terminal besides struggling to install stuff you desperately need for grad school? Errors and trouble-shooting is what makes us stronger data scientists, even if we don’t want to recognize that as we fight the urge to throw our computer out the window.\r\nAfter installing Anaconda, start your journey plotting shapefiles in Python by opening up Jupyter Notebooks. I like to do this from the Anaconda Prompt (the Aanconda terminal), because I’m working on a PC and it’s tricky to get my normal command line or Bash to recognize that conda is indeed on my computer. My favorite flow is as follows:\r\nFrom the start menu, open a terminal through Anaconda Navigator that’s called “Anaconda Prompt”.\r\nInstall geopandas with conda install geopandas in your base environment (which is the default). If that doesn’t work (which would not surpise me if you’re on a PC), create a new environment to do so. I found the steps on this website to be helpful: https://medium.com/analytics-vidhya/fastest-way-to-install-geopandas-in-jupyter-notebook-on-windows-8f734e11fa2b. I tried installing geopandas in my base environment, but it was difficult to install all the correct versions of all the dependencies, so I decided to take the easy route and just make a new environment for geopandas and any other spatial analysis packages I’ll need. Maybe one day I’ll get familiar with version-specific terminal installations and I will be able to install whatever my heart pleases in my base environment.\r\nActivate the environment in which you have installed the geopandas package. I named that environment geo_env, so I type activate geo_env.\r\nNow that I am in my desired environment, I am going to navigate to the folder in my terminal in which I want to open and save my Jupyter Notebook. That command is cd file/path/to/folder. You know this worked if your terminal working directory now has that file path tacked onto the end. This file path step is not required if you want to include relative file paths to import data and save your notebook. Personally, I do not recommend using relative file paths if you can avoid it in any interface.\r\nDownload your spatial data files to this folder to make your life easier in 2 minutes when you import your spatial data in Jupyter Notebook.\r\nOpen Jupyter Notebooks by typing just that: jupyter notebook. This will tell your terminal to pop open Jupyter Notebook in your browser with your folder of choice already open and ready to go.\r\nIn the upper right side, open a new notebook.\r\nTime to import your packages! For plotting shapefiles, you’ll want to import the following packages:\r\nimport pandas as pd\r\nimport numpy as np\r\nimport geopandas as gpd\r\nimport matplotlib.pyplot as plt\r\nimport contextily as ctx\r\nAs a proponent of reproducibility and credditing those who provided the data, I like to include a markdown chunk following my package imports that includes a URL link to where I found my data, along with a citation if necessary and any notes about how I downloaded it:\r\nData source: US Fish and Wildlife https://ecos.fws.gov/ecp/report/table/critical-habitat.html\r\n- contains .shp, .dbf, and .shx files\r\n- I chose the first zip file you see on this site\r\nImporting Data\r\nLet’s import your data! Now is the time you’re gonna be thanking yourself for placing your jupyter notebook in the same folder as your data. We will use geopandas to read in the shapefile with your polygons or lines or points of choice (you will not find a combination of these shapes in the same shapefile, because that’s just how the world works). You might take a look at all the data files and feel a little overwhelmed at the choices due to the way that shapefiles and their metadata are stored separately (.shp, .dbf, .shx, .xml, and so on). In this example we are trying to import a shapefile of polygons, so that .shp file is the only one you need to read in:\r\nImport the data:gdf = gpd.read_file('CRITHAB_POLY.shp')\r\nTake a look at the first rows:print(gdf.head())\r\nAsk Python how many rows and columns are in this dataframe:print(gdf.shape)\r\nMy only complaint with the head() function is that it returns the first rows in a plain text format. If you want to see the first and last few rows of the dataframe in a format that looks more familiar (like how R studio presents dataframes), try just typing gdf.\r\nThis shapefile I read in contains polygons that designate the critical habitat ranges for many endangered and threatened species in the United States. I chose to name it gdf for geodataframe. While you can name objects whatever you want, it is helpful to you and to those reading your code to name things meaningfully. Expect that you will be modifying this dataframe as you go through this mapping process (subsetting columns, filtering for certain factor levels, etc.) so you will likely be tacking on more words to gdf to tell these modified versions apart. Start naming things simply and clearly, and get more specific as you process your data.\r\nIf your dataset has a lot of columns and you want to call them, you can use:\r\nprint(gdf.columns)\r\nWanna check the different factor levels in this dataset? Run the following:status_levels = gdf.listing_st.unique()status_levels\r\nNow you can subset for only “endangered” species, only “threatened” species, etc.\r\nSetting the Coordinate Reference System\r\nAs a last step before you plot, you have to make sure you are in the desired coordinate reference system (CRS). This is pretty standard for all kinds of spatial data, since your data might come with an unfamiliar CRS or have no CRS at all if you are making a mask, a raster, or executing similar processes. For information about coordinate reference systems, check out this guide:https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf\r\nBut you technically do not need to understand many details about datums and CRS’s for mapping shapefiles, so just for now you should know that two common CRS’s are:\r\nWGS84 (EPSG: 4326), which is commonly used for GIS data across the globe or across multiple countries\r\nand\r\nNAD83 (EPSG:4269), which is most commonly used for federal agencies\r\nand\r\nMercator (EPSG: 3857), which is used for tiles from Google Maps, Open Street Maps, and Stamen Maps. I will use this one because I want to overlay my polygons onto a Google basemap.\r\nSet the crs:gdf_3857=gdf.to_crs(epsg=3857)\r\nCheck that the CRS is what you want:print(gdf_3857.crs)\r\nPlotting Shapefiles\r\nUse the plot() function from matplotlib and make the fill depend on the species name: gdf_3857.plot(column='comname',figsize=(20,20))\r\nThe fig size can be whatever you want. 10-20 is usually good enough. This code should show you the polygons in the geometry column colored differently based on their species name from the “comname” column. These polygons are just floating in space, so lets add a basemap to give us geographical context:\r\ngdf_3857.plot(column='comname',figsize=(20,20))ctx.add_basemap(basic_plot)\r\nSet the axis limits to zoom in on just the lower 48 states, rather than viewing the entire world:plt.ylim([2000000,6000000])plt.xlim([-14000000,-6000000])\r\nUse the show() function in matplotlib to tell Python to show the most recently created graph:plt.show()\r\nYou did it! Welcome to the wonderful world of geospatial data in Python.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-11-05T21:11:53-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-10-the-casewhen-function/",
    "title": "Tidy data and the case_when() function in R",
    "description": "A gem within the expansive tidyverse.",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-08-23",
    "categories": [
      "-R"
    ],
    "contents": "\r\n\r\nContents\r\nTidy data\r\ncase_when() and Lobsters\r\nFirst thing first, import your data\r\n\r\n\r\n\r\n\r\nlibrary(tidyverse)\r\n# use this to tidy the data\r\nlibrary(janitor)\r\n# use this to tidy the columns names\r\n\r\n\r\n\r\nTidy data\r\nWhenever we start our journey transforming a dataset into something we can interpret and visualize, we\r\nThis lobster data set is in Tidy format, as is most of the data sets we work with. Tidy data is a structure for data sets that helps R do the most work possible when it comes to analysis, summary statistics, and combining data sets. R’s vectorized functions flourish with rows and columns in Tidy format.\r\nTidy data has each variable in columns, each observation has its own row, and each cell contains a single value. For the lobster data set, each lobster caught has its own row with each column describing one aspect of that lobster. Each column has a succinct title for the variable it contains, and ideally includes underscores where we would normally have spaces and has no capitalization to make our coding as easy as possible. There should be NA in any cells that do not have values, which is a default that many R functions recognize as default. When we utilize this data, we can easily remove these values in our code by referring to them as NA.\r\nTidy format encourages collaboration between people and data sets because we are easily able to combine data from different sources using join functions. If the data contains columns with shared variables, R can easily recognize those columns and associate its rows (observations) with the observations of the complementary data set. Using full_join() is a common join function to utilize as it maintains all data from both sources.\r\nTidy format helps you easily make subsets of your data for specific graphs and summary tables. Consider the filter() and select() functions, which help you subset to only view variable or observations of interest. In these cases, it is especially important to have only one value in each cell and standardize the way you document observations. You always want to record each lobster species with the same spelling, each size with the same number of decimal places, and each date with the same format (such as YYYY-MM-DD). For variables such as length that might need units, always include these units in the column header rather than the cell. This streamlines our coding and keeps cells to a single class. If you include numerical and character values in one cell, it will be documented as a character, which can restrict your analysis process.\r\nYour data isn’t in Tidy format? That’s alright! Check out the tidyr::pivot_longer() and tidyr::pivot_wider() functions to help you help R help you. In the example below, we have a tribble dataset that is not in Tidy format. We know this because there are multiple columns (A:C) that represent different individuals or observations of the same variable (like dog food brands). We can use pivot_longer() to put the column headers into their own column, rename that column, and pivot their values into their own column while maintaining their association with A, B, and C. Although the resulting tidy data may seem more complex at first galnce, it is easier to convert to a graph and structurally is more organized from a data science perspective.\r\nTo demonstrate some simple data tidying, lets make a tribble (which is similar to a dataframe) and manipulate it using the pivot_longer() function. In this tibble, we are\r\n\r\n\r\ndf <- tribble(\r\n  ~name, ~A, ~B, ~C,\r\n  \"dog_1\", 4, 5, 6,\r\n  \"dog_2\", 9, 10, 8\r\n)\r\n\r\ndf\r\n\r\n\r\n# A tibble: 2 x 4\r\n  name      A     B     C\r\n  <chr> <dbl> <dbl> <dbl>\r\n1 dog_1     4     5     6\r\n2 dog_2     9    10     8\r\n\r\nThis dataframe is not in tidy format, because the variable ranking is dispersed between multiple columns. We want a single variable in each column, so lets combine those columns and make it tidier:\r\n\r\n# A tibble: 6 x 3\r\n  name  dog_food_brand ranking\r\n  <chr> <chr>            <dbl>\r\n1 dog_1 A                    4\r\n2 dog_1 B                    5\r\n3 dog_1 C                    6\r\n4 dog_2 A                    9\r\n5 dog_2 B                   10\r\n6 dog_2 C                    8\r\n\r\ncase_when() and Lobsters\r\nUse this function to help tidy your data and prepare it for plotting. case_when() bins continuous data into manually defined categories and add it to your data set in the form of new column.\r\ncase_when() is used in this example to categorize lobsters into size bins based on the legal size minimum for fishing. This function processes each individual lobster we caught in this dataframe and returns if it is large enough to legally harvest from various locations along the Santa Barbara coast.\r\nFirst thing first, import your data\r\n\r\n# A tibble: 6 x 9\r\n   year month date       site  transect replicate size_mm num_ao  area\r\n  <dbl> <dbl> <date>     <chr>    <dbl> <chr>       <dbl>  <dbl> <dbl>\r\n1  2012     8 2012-08-20 IVEE         3 A              70      0   300\r\n2  2012     8 2012-08-20 IVEE         3 B              60      0   300\r\n3  2012     8 2012-08-20 IVEE         3 B              65      0   300\r\n4  2012     8 2012-08-20 IVEE         3 B              70      0   300\r\n5  2012     8 2012-08-20 IVEE         3 B              85      0   300\r\n6  2012     8 2012-08-20 IVEE         3 C              60      0   300\r\n\r\nThis data is ground-breaking! The world needs to see this and understand its implications. In order to plot this fasciating data in a meaningful way, we want to efficiently categorize our lobsters by legality status and color code their relative abundance in our visualization. Considering that the legal minimum size for a lobster is 79.76 (units), this is the threshold we will pass onto R to do the heavy lifting for us.\r\n\r\n\r\nlobsters_legality <- lobsters %>% \r\n  mutate(legal = case_when(\r\n    size_mm >= 79.76 ~ \"yes\",\r\n    size_mm < 79.76 ~ \"no\")) %>% \r\n   group_by(site, legal) %>% \r\n  summarize(site_legal_count = n())\r\n\r\n\r\n\r\nUse ggplot() to make a bar graph that color codes the lobster abundance by legality status. We communicate that we want R to color the graph by this variable by passing the argument color = legal within aes(). Manually setting colors is set outside of aes(), but here it is an argument because it is determined by a variable.\r\n\r\n\r\n\r\nDistill is a publication format for scientific and technical writing, native to the web. Learn more about using Distill at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-10-the-casewhen-function/case_when_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-11-05T21:36:15-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-10-a-function-i-learned/",
    "title": "Summary Statistics",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Unknown",
        "url": {}
      }
    ],
    "date": "2021-08-10",
    "categories": [
      "-R"
    ],
    "contents": "\r\n\r\nContents\r\nabline function\r\n\r\nabline function\r\nGiven a graph of penguin body mass as a function of penguin flipper length, the geom_smooth(method = lm) function creates a best fit line for the scatterplot.\r\nWithin the function geom_smooth(), you can specify if you want the line to show a confidence interval with se = FALSE or TRUE.\r\nFurthermore, you can add linetype = “dashed” to change the line type.\r\n\r\n\r\n\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-10-a-function-i-learned/Images/bembe.jpg",
    "last_modified": "2021-09-29T13:49:55-07:00",
    "input_file": {}
  }
]
