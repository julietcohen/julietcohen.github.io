[
  {
    "path": "posts/2021-11-25-fishingeffortbycountry/",
    "title": "Global Fishing Effort & Covid-19: A Statistical Analysis",
    "description": "A statistical approach to answer the question: How did Covid-19 impact global fishing efforts in 2020?",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-12-02",
    "categories": [
      "-R"
    ],
    "contents": "\r\nA cod fish, one of the top species captured by marine fisheries (8)How did global fishing activity change during the Covid-19 pandemic?\r\n2020 is a year the world will never forget. Covid-19 spread rapidly across the globe and forced most of humanity into a state of quarantine. The pandemic had clear devastating impacts on economies of all scales. On the other hand, the pandemic boosted some sectors of the economy and increased demand for certain goods. How did Covid-19 impact the global fishing economy? Did fisheries respond to the pandemic by sending fishermen and fisherwomen home to quarantine, and instead allocate resources towards public health initiatives? Alternatively, did some countries see this as an opportunity to fish unregulated in the high seas more than ever before? There is very limited literature that addresses this question, which is likely due to the fact that 2020 was less than a year ago, and any formal scientific studies on this topic might not have had time to be published. Pita et al. 2021 assessed Covid-19’s impact on global marine recreational fisheries via a questionnaire, but this research differs from my statistical analysis in that it did not use vessel data or quantitatively calculate how fishing hours differed over time, and the study only includes 16 countries (5).\r\nRegulating and documenting fishing activity and other vessel activities across the globe is a major challenge (4). Databases often have substantial gaps due to a lack of reliable data from automatic identification systems and voluntary vessel registration. Global Fishing Watch is an organization that aims to revolutionize the way we monitor fishing activity across the world using remote sensing techniques from satellites combined with automatic identification systems. Global Fishing Watch collects and visualizes global fishing data with the goal of embracing ocean sustainability, transparency, and open-source science. They keep track of vessels from all different countries, including their movements, boat types, and time stamps for fishing and docking at ports (9). Without such efforts to monitor, publicize, and regulate ocean activity, our marine resources are at high risk of depletion. On a global scale, we are fishing faster than fish stocks can naturally replenish. This has severe economic impacts; according to the World Bank Report, the ensuing depletion of marine fish stocks causes economic losses of 50 billion US dollars annually (4). With modern data science and applied statistics, we can better understand fishing activity on a global scale and protect our planet’s marine biodiversity.\r\nAs an aspiring wildlife biologist and data scientist, I’m interested in applying statistical analysis to Global Fishing Watch data data to learn how different countries’ fishing effort changed in 2020, relative to those countries’ fishing trends in the years leading up to 2020. In this dataset, fishing effort is defined by the amount of hours spent fishing (3). I chose to use this dataset for my statistical analysis because it is already relatively clean, and I know the data is reliable because Global Fishing Watch is a highly respected data resource with highly accurate remotely sensed data that is combined with locally collected automatic identification systems on shore. Furthermore, I am interested in working with Global Fishing Watch and spatial data in the future. This data does not have a spatial component since country is a categorical variable, and the temporal variable is limited to years. The only bias I believe might be present in this data is that it is limited to boats that either voluntarily allow their fishing hours to be tracked (such as through automatic identification systems) as well as boats that have been detected remotely by satellite. With Global Fishing Watch’s expansive open-source data collection, we can approach this question by grouping all vessels’ fishing hours by country, identifying a statistical trend up until 2019, and extrapolating that trend into 2020. By comparing this 2020 prediction to the actual fishing data available for 2020, we can glean how Covid-19 skewed large-scale fishing efforts. I chose this analysis approach because I am familiar with many of these processes through my graduate statistics course, and I believe it will be the simplest and accurate way to derive a p-value that will reveal if there is a statistically significant difference between each country’s actual mean fishing effort and their predicted mean fishing effort in 2020. Perhaps the global fishing economy sky-rocketed, plummeted into near nonexistence, or remained unscathed by the pandemic. Quantitative analysis will help provide some insight.\r\nGlobal Fishing Watch offers an interactive map that displays fishing activity across the globe through a heat map. This visualization has the potential to inspire data scientists, fish enthusiasts, environmental justice advocates, pandemic researchers, and everyone in between to examine fishing activity during a time period of interest.\r\nGlobal fishing activity from January 1, 2020 through January 1, 2021Global Fishing Watch and their partners also provide an interactive map that allows users to interact with vessels across the globe, filter by country, and overlay port locations on coastlines.\r\n\r\n\r\n\r\n\r\n\r\n\r\nData Cleaning and Wrangling\r\nGlobal Fishing Watch’s data includes fishing effort and vessel information from 167 countries over the years 2012-2020 (3). First, we select our variables of interest, group by country, and take the fishing effort means per year.\r\n\r\n\r\n\r\nOur goal is to run a linear regression on each country’s fishing effort over multiple years, but many countries have NA data for certain years. Considering that we have data available for 2012-2020, we can subset these years should we choose for the model. We want to minimize the amount of NA values because we will drop all NA rows, and we want to maintain the maximum amount of rows possible (which represent vessels and countries). We want to select a chunk of continuous years leading up to 2020 with minimal data gaps. In order to choose the start year for the time period that we will feed into the linear regression, take a look at the amount of NA values in the years leading up to 2020. It turns out that 2017, 2018, and 2019 have the least amount of NA values, so we will use 2017 to start our 3-year data period to feed to the linear regression. Next, we convert the data into Tidy format using pivot_longer so we can run a time series linear regression analysis.\r\n\r\n\r\n\r\nOur dates are in years, and currently their class is character from the original dataset. We need these years in date format in order to run a linear regression over time. We will convert these years and remove all countries that only have data for only one or two years, because we need multiple years of data to feed into the regression and we want each country to have equal amounts of data and start in the year 2017.\r\n\r\n\r\n\r\nLinear Regression\r\nNow that the data is sufficiently clean and our years are of class date, we can run a time series linear regression on every country’s fishing effort from 2017-2019 and use the output coefficients to glean which direction each country is trending, meaning if the country is fishing more or less over time. We can do this with the sapply() function, grouping by country. Then we can feed this output into a for loop! Plug in each country’s fishing effort intercept and slope coefficients into a linear equation to predict the fishing effort in 2020 based on that country’s historical trend. Subsequently, we can combine the predicted 2020 fishing effort data with the actual 2020 fishing effort data into a single dataframe to compare by country. We can make a new column that takes the difference of the actual and predicted values, and then add a column that explicitly states whether that country increased or decreased their fishing effort in 2020 relative to their trend leading up to 2020.\r\n\r\n\r\n\r\nPlotting Actual Fishing Effort versus Predicted Fishing Effort for Malaysia\r\nWhat does a single country’s fishing trend look like? Let’s consider the country of Malaysia in Southeast Asia. In 2015, Malaysia’s fisheries sector employed 175,980 people and its contribution to national gross domestic product was 1.1%. The fish trade is valued at $1.7 billion (U.S. dollars), and the estimated average consumption of fish is 56.8 kg/person/year. Malaysian fisheries primarily capture shrimp, squid, and fish. Malaysia contributes to the global fish economy through both importing and exporting fish (6).\r\nWe can make a country-specific fishing effort plot by filtering our actual fishing effort data to just that country and appending the predicted 2020 fishing effort data for that country that we produced through our linear model.\r\n\r\n\r\n\r\nMalaysia’s Fishing Effort: Actual vs Predicted 2017-2020Statistical Significance\r\nIt’s time to run a t-test to determine if there is a statistical difference between the countries’ predicted fishing effort in 2020 and their actual fishing effort in 2020. A t-test is a handy tool in statistics that reveals how significant the differences between groups are. If the difference between the means of two groups could have easily happened by chance, the p-value will be greater than 0.05 (which is the standard threshold in statistics and environmental data science). If it is highly unlikely (less than a 5% chance) that a difference in means at least this extreme could have occurred by chance, the p-value is less than 0.05 and the results are considered statistically significant. A statistically significant outcome allows us to reject our null hypothesis.\r\nNull Hypothesis: There is no difference between the predicted country-specific predicted fishing effort in 2020 and the actual country-specific fishing effort in 2020. \\[H_{0}: \\mu_{predicted} - \\mu_{actual} = 0\\] Alternative Hypothesis: There is a difference between the predicted country-specific predicted fishing effort in 2020 and the actual country-specific fishing effort in 2020. Because of the pandemic in 2020, I predict that fishing effort decreased, meaning that the actual country-specific fishing effort is less than the predicted country-specific fishing effort. \\[H_{A}: \\mu_{predicted} - \\mu_{actual} \\neq 0\\]\r\nDon’t forget to convert the data to Tidy format so we can run the t-test!\r\n\r\n\r\n\r\nt-test outputThe p-value is 0.0000000312, and 0.0000000312 < 0.05, so we can reject our null hypothesis that there is no difference between the predicted country-specific predicted fishing effort in 2020 and the actual country-specific fishing effort in 2020. Many countries clearly changed their fishing effort in 2020 relative to their historical trend!\r\nSummary: Which countries increased their fishing effort during the pandemic, relative to their trend leading up to 2020?\r\nTo best visualize this fishing effort data in a table, we can color code the countries that increased their fishing effort as red and color the countries that decreased their fishing effort in green.\r\n\r\n\r\n\r\nDifferences between actual 2020 fishing effort and predicted 2020 fishing effort by countryThis color-coded table reveals that 85% of the countries included in this analysis decreased their fishing effort during the Covid-19 pandemic in 2020 relative to their fishing trend leading up to 2020, while 15% of the countries included in this analysis increased their fishing effort. The vast majority of countries’ fishing sectors seemed to follow the same stay-at-home order that was enforced across the globe. While this may have had a detrimental impact on the global fish economy, hopefully the marine fish populations we able to recover and thrive during this period of reprieve from human predation. The results of my statistical analysis match the conclusion of a 2021 scientific study investigating the change in marine recreational fishing activity during the first year of the pandemic (5).\r\nFuture Steps\r\nIn order to improve this analysis in the future, I recommend using more than three years of fishing effort data to produce a more accurate linear model. Additionally, I would recommend using a different statistical approach instead of iterating a for loop over each country’s fishing effort data, because this method produced outliers and coefficients that did not line up with observed fishing effort data. Lastly, I recommend running this analysis on fishing effort data from other sources in addition to Global Fishing Watch’s data. This will provide certainty that the data is accurate and the results are reproducible.\r\nThank you for reading my statistical review of global fishing effort during the 2020 Covid-19 pandemic! I hope I have inspired you to run your own time series analysis, t-tests, and create visualizations that help communicate trends in environmental data science. Please feel free to contact me at jscohen@ucsb.edu with any questions, comments, or suggestions. You may also create issues or pull requests for this analysis through GitHub (repository linked below).\r\nData Availability\r\nThe data used in this analysis is openly available, but the user must make a free account on the Global Fishing watch website, which cna be accessed through this live link:Global Fishing Watch Datasets and Code\r\nGitHub Repository Live Link\r\njulietcohen’s GitHub Repository\r\nAcknowledgements:\r\nI would like to acknowledge Dr. Tamma Carleton, my professor in Statistics for Environmental Data Science at the U.C. Santa Barbara Bren School for Environmental Science and Management, for all her support throughout this project and this quarter.\r\nI would also like to thank my peers in the Master of Environmental Data Science Program for being so open to collaboration and supporting each other with resources, programming tools, and open-source science.\r\nLastly, I would like to thank Global Fishing Watch for inspiring me to give a hoot about global fishing effort by country, and for providing the data that made this project possible.\r\nResources (live links):\r\nInteractive World Map of Fishing Activity: Picture 1 and Map Link 1 - Global Fishing Watch\r\nInteractive World Map of Fishing by Country: Map Link 2 - Global Fishing Watch\r\nGlobal Fishing Watch: Datasets and Code, Fishing effort data\r\nnote: Users must make a free account in order to access datasets\r\nGlobal Fishing Effort (1950–2010): Trends, Gaps, and Implications.\r\nAnticamara, J. A., R. Watson, A. Gelchu, and D. Pauly. “Global Fishing Effort (1950–2010): Trends, Gaps, and Implications.” Fisheries Research 107, no. 1 (2011): 131–36. https://doi.org/10.1016/j.fishres.2010.10.016.\\\r\nFirst Assessment of the Impacts of the COVID-19 Pandemic on Global Marine Recreational Fisheries.:\r\nPita, Pablo, Gillian B. Ainsworth, Bernardino Alba, Antônio B. Anderson, Manel Antelo, Josep Alós, Iñaki Artetxe, et al. “First Assessment of the Impacts of the COVID-19 Pandemic on Global Marine Recreational Fisheries.” Frontiers in Marine Science 8 (2021): 1533. https://doi.org/10.3389/fmars.2021.735741.\\\r\nMalaysia’s Fisheries Economy\r\nGoogle Maps: Malaysia\r\nWikipedia: Top Marine Fisheries Species Captured\r\nGlobal Fishing Watch - About Us\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-25-fishingeffortbycountry/pictures/fishing_gfw_map_2020.png",
    "last_modified": "2021-12-02T14:11:10-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-01-plotting-shapefiles-in-python/",
    "title": "Plotting Shapefiles in Python",
    "description": "The basics for reading in shapefile data, plotting it, and adding a basemap.",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-11-05",
    "categories": [
      "-Python"
    ],
    "contents": "\r\n\r\nContents\r\nPython and Jupyter Notebooks\r\nImport Some Packages (modules)\r\nImporting Data\r\nCoordinate Reference System\r\nPlotting Shapefiles\r\n\r\nThe Florida bonneted bat, a critically endangered species that has been assigned critical habitat by U.S. Fish and Wildlife. Hopefully, the critical habita is suffificient to help this species recover and reach sustainable population levels. The world would be a better place with more bonneted bats.Let’s map the critical habitat ranges of this bat and many other species of concern on a basemap of the United States.\r\nPython and Jupyter Notebooks\r\nTransitioning from R in Rstudioto Python in Jupyter Notebook can be difficult, as is learning any new language. We have to say goodbye to the exceptionally user-friendly RStudio interface with its git GUI, visible environment, and easily accessible console and terminal windows. Let’s dive into the objected oriented language of Python through Jupyter Notebook and the mysterious terminal. Personally, I struggled every step of this journey with lots of syntax errors, merge conflicts, and endless Google searching. I hope this post will help hold your hand as we make this change together.\r\nFirst things first, if you’re using Jupyter Notebooks to code in Python, you’re gonna have to install Anaconda Navigator and install conda into your home directory. As a PC user, installing conda really threw me and my laptop for a loop. But what better way to learn to use the terminal besides struggling to install stuff you desperately need for grad school? Errors and trouble-shooting is what makes us stronger data scientists, even if we don’t want to recognize that as we fight the urge to throw our computer out the window.\r\nAfter installing Anaconda, start your journey plotting shapefiles in Python by opening up Jupyter Notebooks. I like to do this from the Anaconda Prompt (the Aanconda terminal), because I’m working on a PC and it’s tricky to get my normal command line or Bash to recognize that conda is indeed on my computer. My favorite flow is as follows:\r\nFrom the start menu, open a terminal through Anaconda Navigator that’s called “Anaconda Prompt”.\r\nInstall geopandas with conda install geopandas in your base environment (which is the default). If that doesn’t work (which would not surprise me if you’re on a PC), create a new environment to do so. I found the steps on this website to be helpful: https://medium.com/analytics-vidhya/fastest-way-to-install-geopandas-in-jupyter-notebook-on-windows-8f734e11fa2b. I tried installing geopandas in my base environment, but it was difficult to install all the correct versions of all the dependencies, so I decided to take the easy route and just make a new environment for geopandas and any other spatial analysis packages I’ll need. Maybe one day I’ll get familiar with version-specific terminal installations and I will be able to install whatever my heart pleases in my base environment.\r\nActivate the environment in which you have installed the geopandas package. I named that environment geo_env, so I type activate geo_env.\r\nNow that I am in my desired environment, I am going to navigate to the folder in my terminal in which I want to open and save my Jupyter Notebook. That command is cd file/path/to/folder. You know this worked if your terminal working directory now has that file path tacked onto the end. This file path step is not required if you want to include relative file paths to import data and save your notebook. Personally, I do not recommend using relative file paths if you can avoid it in any interface.\r\n\r\nDownload your spatial data files to this folder to make your life easier in 2 minutes when you import your spatial data in Jupyter Notebook.\r\nOpen Jupyter Notebooks by typing just that: jupyter notebook. This will tell your terminal to pop open Jupyter Notebook in your browser with your folder of choice already open and ready to go.\r\nIn the upper right side, open a new notebook.\r\n\r\nNote: the Anaconda terminal window you used to open this notebook should not be closed during your work session. It must remain open to keep your kernel connected and give you the ability to save! If you need to run any terminal commands after you have already opened this notebook, such as if you need to download a package or check a file location, just open up another terminal window and enter the geo_env environment to do so.\r\nImport Some Packages (modules)\r\nFor plotting shapefiles, you’ll want to import the following packages:\r\nimport pandas as pd\r\nimport numpy as np\r\nimport geopandas as gpd\r\nimport matplotlib.pyplot as plt\r\nimport contextily as ctx\r\nmatplotlib will allow us to plot our data and manipulate the plot as we see fit. contextily will allow us to add a default basemap under our polygons of interest. If you do not already have this installed, consider using a channel and conda forge to install it in the terminal.\r\nAs a proponent of reproducibility and credditing those who provided the data, I like to include a markdown chunk following my package imports that includes a URL link to where I found my data, along with a citation if necessary and any notes about how I downloaded it:\r\nData source: US Fish and Wildlife https://ecos.fws.gov/ecp/report/table/critical-habitat.html\r\n- contains .shp, .dbf, and .shx polygon files that relate to critical species habiata in the United States - I chose the first zip file you see on this site\r\n- good metadata available\r\nImporting Data\r\nLet’s import your data! Now is the time you’re gonna be thanking yourself for placing your jupyter notebook in the same folder as your data. We will use geopandas to read in the shapefile with your polygons or lines or points of choice (you will not find a combination of these shapes in the same shapefile, because that’s just how the world works). You might take a look at all the data files and feel a little overwhelmed at the choices due to the way that shapefiles and their metadata are stored separately (.shp, .dbf, .shx, .xml, and so on). In this example we are trying to import a shapefile of polygons, so that .shp file is the only one you need to read in:\r\nImport the data:gdf = gpd.read_file('CRITHAB_POLY.shp')\r\nTake a look at the first rows:print(gdf.head())\r\nAsk Python how many rows and columns are in this dataframe:print(gdf.shape)\r\nMy only complaint with the head() function is that it returns the first rows in a plain text format:\r\n\r\nIf you want to see the first and last few rows of the dataframe in a format that looks more familiar (like how R studio presents dataframes), try just typing the name of the data frame, gdf:\r\n\r\nThis shapefile I read in contains polygons that designate the critical habitat ranges for many endangered and threatened species in the United States. I chose to name it gdf for geodataframe. While you can name objects whatever you want, it is helpful to you and to those reading your code to name things meaningfully. Expect that you will be modifying this dataframe as you go through this mapping process (subsetting columns, filtering for certain factor levels, etc.) so you will likely be tacking on more words to gdf to tell these modified versions apart. Start naming things simply and clearly, and get more specific as you process your data.\r\nIf your dataset has a lot of columns and you want to call them, you can use:\r\nprint(gdf.columns)\r\nWanna check the different factor levels in this dataset? Run the following:status_levels = gdf.listing_st.unique()status_levels\r\nUsing U.S. Fish and Wildlife as an example, now that you know the factor levels of a categorical variable, you can subset for only “endangered” species, only “threatened” species, etc.\r\n\r\nFeel free to play around with your dataset a bit. Google some of the species, subset the columns, search for some NA values, or take the average of a numerical column. After you make a structural change, its a good habit to check the status or dimensions of your dataset.\r\nCheck the number of rows and columns: \r\nPrint the latitude and longitude pairs that make up a particular polygon: \r\nCoordinate Reference System\r\nAs a last step before you plot, you have to make sure you set the data to the desired coordinate reference system (CRS). This is pretty standard for all kinds of spatial data, since your data might come with an unfamiliar CRS or have no CRS at all if you are making a mask, a raster, or executing similar geospatial processes. For information about coordinate reference systems, check out this guide:https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf\r\nBut you technically do not need to understand many details about datums and CRS’s for mapping shapefiles, so just for now you should know 3 common CRS’s:\r\nWGS84 (EPSG: 4326), which is commonly used for GIS data across the globe or across multiple countries\r\nand\r\nNAD83 (EPSG:4269), which is most commonly used for federal agencies\r\nand\r\nMercator (EPSG: 3857), which is used for tiles from Google Maps, Open Street Maps, and Stamen Maps. I will use this one today because I want to overlay my polygons onto a Google basemap with contextily.\r\nSet the crs:gdf_3857=gdf.to_crs(epsg=3857)\r\nCheck that the CRS is what you want:print(gdf_3857.crs)\r\nThis code may take a minute to run. In Jupyter Notebook, you know that code chunk is still chuggin’ away if you see an asterisk in brackets to the left of the code chunk:\r\n\r\nPlotting Shapefiles\r\nUse the plot() function from matplotlib and make the fill depend on the species name: gdf_3857.plot(column='comname',figsize=(20,20))\r\n\r\nNote that you did not have to call the package to use the function plot(). Instead, you can name the dataframe which you want to plot, which is gdf_3857 in this case, then specify the function plot() and add arguments and supplemental plot structure changes as you go.\r\nThe fig size can be whatever you want. 10-20 is usually good enough. You have finer control over the degree of zoom of the map with the arguments xlim() and ylim(), anyway. These polygons are just floating in space, so lets add a basemap to give us geographical context:\r\ncrit_hab_polys = gdf_3857.plot(column='comname',figsize=(20,20))\r\nNotice that I used an argument in the plot function, setting the column = 'comname', which is a column within the gdf_3857 geodataframe that specifies the common name for the species in that row. This argument sets a unique color to each common name, which will help us tell the difference between each species’ habitat on the map, even if 1 species’ habitat is composed of multiple polygons. ctx.add_basemap(crit_hab_polys)\r\nSet the axis limits to zoom in on just the lower 48 states, rather than viewing the entire world:plt.ylim([2350000,6350000])\r\nSince the basemap within the contextily package is of the entire world, we need to specify the x-limitations and y-limitations for our map so we can zoom in on the United States to best understand our data. The default x and y units were in the millions, so I specified my units in millions, too. When considering if I should plug in positive or negative values, I considered the way that coordinate reference systems are designed with positive values for North and East, and negative values for South and West. I considered that the United States are north of the equator (which is 0 in the North and South directions), so I should have positive values for the min and max y. As for the magnitude of my values, I simply looked at the map for a starting point and played around with different numbers until I got the view I wanted. plt.xlim([-14000000,-6000000]) Notice that these values are negative. Along similar thinking to how we decided on our y limitation, these negative values are the result of how coordinate systems are designed. Consider the prime meridian (which lies at 0 degrees in the East and West directions) with West being negative. Since the United States are to the West of the prime meridian, we know that the x-range for our zoom should be negative. As for the magnitude, I just palyed around with the numbers until I got the East-West orientation that encompassed the United States. Use the show() function in matplotlib to tell Python to show the most recently created graph:plt.show()\r\n\r\nYou did it! Welcome to the wonderful world of geospatial data in Python. Now you can start answering questions like:\r\n- “Which endangered species live near me?”\r\n- “Which state or biome houses the most critical habitat for these species?”\r\nSources & supplemental information on listed species in the United States:\r\nFlorida bonneted bat photo\r\nhttps://www.joelsartore.com/esa002-00161/n\r\nU.S. Fish and Wildlife data\r\nhttps://ecos.fws.gov/ecp/report/table/critical-habitat.html\r\nU.S. Fish and Wildlife’s Environmental Conservation Online system\r\nhttps://ecos.fws.gov/ecp/\r\nU.S. Fish and Wildlife listed wildlife species\r\nhttps://ecos.fws.gov/ecp/report/species-listings-by-tax-group?statusCategory=Listed&groupName=All%20Animals\r\nthis includes links for data on each each species and a map of their habitat\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-01-plotting-shapefiles-in-python/crit_hab_with_basemap.png",
    "last_modified": "2021-11-30T20:36:02-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-05-sstchlorowind/",
    "title": "Visualizing Sea surface temperature, chlorophyll, and wind in the Santa Barbara Channel",
    "description": "Interpretting marine processes by combining multiple data sets.",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-11-05",
    "categories": [
      "-R"
    ],
    "contents": "\r\nData in the Santa Barbara Channel\r\nThe rocky Santa Barbara coast is known for its beautiful weather, incredible surfing, and unique geographic features. Among these are the 4 channel islands which reside between 12 and 27 miles off shore from Santa Barabara, Ventura, and Oxnard. These include Santa Cruz, Anacapa, Santa Rosa, and San Miguel. The Santa Barbara Channel is a hotspot for biodiversity, including dolphins, sunfish, and whales. During certain seasons, whale sightings are quite common from both boat and from the shore. The whales’ feed on krill in the channel, which rely on chorophyll to bloom. We can detect this chlorophyll via satellites, just as we can detect sea surface temperature. While strategizing the best time of year to spot these whales, we might want to consider the timing of these phytoplankton blooms. Do these blooms occur more often when we have warmer ocean temperatures? What time of year would that be? Is the ocean temperature impacted by wind? A few data-driven friends and I decided to combine data about wind, sea surface temperature, and chlorophyll in the Santa Barbara Channel to find the best time of year to go whale watching.\r\nQuestion: How did wind speed affect sea surface temperature and chlorophyll in the Santa Barbara Channel during 2020?\r\nThe National Oceanic Atmospheric Administration (NOAA)\r\nMethods\r\nThe National Oceanic Atmospheric Administration has the perfect datasets to help us out, and they even have a handy application programming interface (API) to do the heavy lifting. Her\r\nThe REDDAP API will pull sea surface temperature and clorophyll data from the NOAA Aquamodis Satelite into Rstudio. In addition, we manually pulled wind speed data from NOAA’s East Buoy, West Buoy, and the Santa Monica Buoy by downloading and decompressing the 2020 Standard Meteorological Data Files available online (see below for links). To coordinate all the data for our analysis, we used the function rbind() to combine the datasets with time as the primary key.\r\n\r\n\r\nlibrary(rerddap)\r\n# used to load in the data from NOAA's website\r\nlibrary(tidyverse)\r\n# used to clean and visualize data\r\nlibrary(here)\r\n# used to read in the data from the current R project\r\nlibrary(lubridate)\r\n# use lubridate to work with datetimes (parsing dates and changing the class)\r\n\r\n\r\n\r\n\r\n\r\n# Read in Aqua Modis Data from their website\r\nrequire(\"rerddap\")\r\n\r\n# Sea Surface Temperature for each Buoy\r\nE_sst <- griddap('erdMWsstd8day_LonPM180', # 8 day composite SST E_buoy\r\n time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\r\n latitude = c(34.0, 34.5), #grid surrounding buoy\r\n longitude = c(-119.5, -120), #grid surrounding buoy\r\n fmt = \"csv\")  %>% \r\n  add_column(location = \"east\") #add ID column\r\n\r\nW_sst <- griddap('erdMWsstd8day_LonPM180', # 8 day composite SST W_buoy\r\n time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\r\n latitude = c(34.0, 34.5), #grid surrounding buoy\r\n longitude = c(-120, -120.5), #grid surrounding buoy\r\n fmt = \"csv\") %>% \r\n  add_column(location = \"west\") #add ID column\r\n\r\nSM_sst <- griddap('erdMWsstd8day_LonPM180', # 8 day composite SST SM_buoy\r\n time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\r\n latitude = c(33.5, 34.0), #grid surrounding buoy\r\n longitude = c(-118.75, -119.25), #grid surrounding buoy\r\n fmt = \"csv\") %>%\r\n  add_column(location = \"SM\") #add ID column\r\n\r\nsst <- rbind(E_sst, W_sst, SM_sst) #bind data\r\n\r\n#----------------------------------------------------------------------------------------\r\n\r\n# Chloro for each Buoy\r\nE_chloro <- griddap('erdMWchla8day_LonPM180',  # 8 day composite Chlorophyll E_buoy\r\n  time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\r\n  latitude = c(34.0, 34.5), #grid surrounding buoy\r\n  longitude = c(-119.5, -120), #grid surrounding buoy\r\n  fmt = \"csv\") %>% \r\n  add_column(location = \"east\") #add location term\r\n\r\nW_chloro <- griddap('erdMWchla8day_LonPM180', # 8 day composite Chlorophyll E_buoy\r\n  time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\r\n  latitude = c(34.0, 34.5), #grid surrounding buoy\r\n  longitude = c(-120, -120.5), #grid surrounding buoy\r\n  fmt = \"csv\") %>% \r\n  add_column(location = \"west\") #add location term\r\n\r\nSM_chloro <- griddap('erdMWchla8day_LonPM180', # 8 day composite Chlorophyll SM_buoy\r\n  time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\r\n  latitude = c(33.5, 34.0), #grid surrounding buoy\r\n  longitude = c(-118.75, -119.25), #grid surrounding buoy\r\n  fmt = \"csv\")%>% \r\n  add_column(location = \"SM\") #add location term\r\n\r\nchloro <- rbind(E_chloro, W_chloro, SM_chloro) #Bind data\r\n\r\n#----------------------------------------------------------------------------------------\r\n\r\n# Wind data for each buoy and data cleaning\r\ntab_E <- read.table(here(\"data\",\"east_wind.txt\"), comment=\"\", header=TRUE) #convert .txt file to .csv\r\nwrite.csv(tab_E, \"east_wind.csv\", row.names=F, quote=F)\r\n\r\nE_wind <- read.csv(here(\"east_wind.csv\")) %>% # read in .csv, select columns and rename\r\n  add_column(location = \"east\") %>% \r\n  select(c(\"X.YY\", \"MM\", \"DD\", \"WSPD\", \"location\"))  %>% \r\n  rename(year = X.YY,\r\n         month = MM,\r\n         day = DD)\r\nE_wind <- E_wind[-c(1),]\r\n  \r\n\r\ntab_W <- read.table(here(\"data\",\"west_wind.txt\"), comment=\"\", header=TRUE) #convert .txt file to .csv\r\nwrite.csv(tab_W, \"west_wind.csv\", row.names=F, quote=F)\r\n\r\nW_wind <- read.csv(here(\"west_wind.csv\"))%>% # read in .csv, select coloumns and rename\r\n  add_column(location = \"west\") %>% \r\n  select(c(\"X.YY\", \"MM\", \"DD\", \"WSPD\", \"location\"))  %>% \r\n  rename(year = X.YY,\r\n         month = MM,\r\n         day = DD)\r\nW_wind <- W_wind[-c(1),]\r\n\r\n\r\ntab_SM <- read.table(here(\"data\",\"SM_wind.txt\"), comment=\"\", header=TRUE) #convert .txt file to .csv\r\nwrite.csv(tab_SM, \"SM_wind.csv\", row.names=F, quote=F)\r\n\r\nSM_wind <- read.csv(here(\"SM_wind.csv\"))%>% # read in .csv, select coloumns and rename\r\n  add_column(location = \"SM\") %>% \r\n  select(c(\"X.YY\", \"MM\", \"DD\", \"WSPD\", \"location\"))  %>% \r\n  rename(year = X.YY,\r\n         month = MM,\r\n         day = DD)\r\nSM_wind <- SM_wind[-c(1),]\r\n\r\nwind <- rbind(E_wind, W_wind, SM_wind) #bind data\r\n\r\n\r\n\r\nWe averaged the wind by month rather than day, as we did for the other 2 variables, because the wind varies more each day by a large margin. Therefore, the wind data on a daily basis shows lots of noise and no interpretable trends. On a monthly scale, however, we can make sense of its broader fluctuations over the year.\r\n\r\n\r\n# clean date format and summarize with daily means for wind\r\n\r\nwind <- wind %>%\r\n  unite(\"date\", year:month:day, sep = \"-\") %>% \r\n  mutate(date = ymd(date, tz = NULL)) %>% \r\n  mutate(WSPD = as.numeric(WSPD))\r\n\r\n# see the data join chunk for na.rm explanation in code comment\r\n\r\nwind_avg <- wind %>% \r\n  group_by(location, date) %>% \r\n  summarize(mean_wind = mean(WSPD, na.rm = T))\r\n\r\n\r\n\r\nHere we cleaned the remotely-sensed sea surface temperature data and summarized it by day:\r\n\r\n\r\n# clean data for sst date\r\nsst_clean <- sst %>% \r\n  mutate(date = ymd_hms(time, tz = \"UTC\")) %>% \r\n  mutate(ymd_date = ymd(date, tz = NULL)) %>% \r\n  mutate(date = ymd_date) %>% \r\n  select(c(\"latitude\", \"longitude\", \"sst\", \"location\", \"date\"))\r\n\r\n\r\n\r\n\r\n\r\n# Clean sst Data and summarize by daily means\r\nfinal_sst <- sst_clean %>% \r\n  filter(sst > 0) %>% #remove NAs\r\n  mutate(sst = (sst * (9/5) + 32 )) %>% #convert to F\r\n  mutate(sst = (sst - 3)) #accounting for SST satellite error through anecdotal and buoy comparison. Jake's field experience justifies this as he has consistently cross-referenced the satellite data with in situ measurements \r\n\r\n# see the data join chunk for na.rm explanation in code comment\r\n\r\nfinal_sst_avg <- final_sst %>% \r\n  group_by(location, date) %>% \r\n  summarize(mean_sst = mean(sst, na.rm = T))\r\n\r\n\r\n\r\nHere we cleaned the remotely-sensed chlorophyll data and summarized it by day:\r\n\r\n\r\n# clean chloro data\r\n# see the data join chunk for na.rm explanation in code comment\r\n\r\nchloro_clean <- chloro %>% \r\n  mutate(date = ymd_hms(time, tz = \"UTC\")) %>% \r\n  mutate(ymd_date = ymd(date, tz = NULL)) %>% \r\n  mutate(date = ymd_date) %>% \r\n  select(c(\"latitude\", \"longitude\", \"chlorophyll\", \"location\", \"date\"))\r\n\r\nfinal_chloro_avg <- chloro_clean %>% \r\n  group_by(location, date) %>%\r\n  summarize(mean_chloro = mean(chlorophyll, na.rm = T))\r\n\r\n\r\n\r\nWe used inner_join() in 2 steps to combine the cleaned data from the 3 variables into one dataframe:\r\n\r\n\r\n# combine daily wind and sst and chloro means\r\n# we decided to use inner join in order to not include any rows that lack values for ANY of the 3 variables. We do not want any NA values in one col and have data in another col, because when we map everything together that data would be represented as if there was a zero value for the variable that had NA. This change reduced the amount of rows by a couple hundred. This was primarily in the SST and cholorophyll data which had plenty of NA's but the wind data did not initially have NA's.\r\n\r\nwind_sst <- inner_join(wind_avg, final_sst_avg, by = c(\"date\", \"location\"))\r\n\r\nchloro_wind_sst <- inner_join(wind_sst, final_chloro_avg, by = c(\"date\", \"location\"))\r\n\r\n\r\n\r\nNow the fun part: visualization! We made 3 plots separated by variable, representing data from all 3 buoys. We marked the sea surface temperature maximum in all 3 plots since the combined data seems to reveal a temporal correlation between sea surface temperature and wind.\r\n\r\n\r\n# Daily Average Sea Surface Temperature from East, West, and Santa Monica Buoys\r\n\r\nggplot(data = chloro_wind_sst, aes(x = date, y = mean_sst, color = location)) +\r\n  geom_line() +\r\n  labs(x = \"Date\",\r\n       y = \"Daily Average Sea Surface Temperature (degC)\",\r\n       title = \"Daily Average Sea Surface Temperature from East, West, and Santa Monica Buoys\",\r\n       color = \"Location\")\r\n\r\n\r\n\r\n\r\n\r\n# Monthly Average Wind from East, West, and Santa Monica Buoys\r\n\r\nmonth_mean <- chloro_wind_sst %>%\r\n  select(location, date, mean_wind) %>%\r\n  mutate(month = month(date, label = TRUE)) %>%\r\n  mutate(month = as.numeric(month)) %>% \r\n  group_by(location, month) %>%\r\n  summarize(mean_wind = mean(mean_wind, na.rm = T)) \r\n\r\nggplot(data = month_mean, aes(x = month, y = mean_wind, color = location)) +\r\n  geom_line() +\r\n  labs(x = \"Month\",\r\n       y = \"Monthly Average Wind Speed (knots)\",\r\n       title = \"Monthly Average Wind Speeds from East, West, and Santa Monica Buoys\",\r\n       color = \"Location\") +\r\n  ylim(0,15) +\r\n  scale_x_discrete(limits=month.abb)\r\n\r\n# Daily Average Chorophyll from East, West, and Santa Monica Buoys\r\n\r\nggplot(data = chloro_wind_sst, aes(x = date, y = mean_chloro, color = location)) +\r\n  geom_line() +\r\n  labs(x = \"Date\",\r\n       y = \"Daily Average Chlorophyll (mg m^-3)\",\r\n       title = \"Daily Average Chlorophyll levels from East, West, and Santa Monica Buoys\",\r\n       color = \"Location\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-05-sstchlorowind/Images/sst.png",
    "last_modified": "2021-11-05T22:00:09-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-10-the-casewhen-function/",
    "title": "Tidy data and the case_when() function in R",
    "description": "A gem within the expansive tidyverse.",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-08-23",
    "categories": [
      "-R"
    ],
    "contents": "\r\n\r\nContents\r\nTidy data\r\ncase_when() and Lobsters\r\nFirst thing first, import your data\r\n\r\n\r\n\r\n\r\nlibrary(tidyverse)\r\n# use this to tidy the data\r\nlibrary(janitor)\r\n# use this to tidy the columns names\r\n\r\n\r\n\r\nTidy data\r\nWhenever we start our journey transforming a dataset into something we can interpret and visualize, we\r\nThis lobster data set is in Tidy format, as is most of the data sets we work with. Tidy data is a structure for data sets that helps R do the most work possible when it comes to analysis, summary statistics, and combining data sets. R’s vectorized functions flourish with rows and columns in Tidy format.\r\nTidy data has each variable in columns, each observation has its own row, and each cell contains a single value. For the lobster data set, each lobster caught has its own row with each column describing one aspect of that lobster. Each column has a succinct title for the variable it contains, and ideally includes underscores where we would normally have spaces and has no capitalization to make our coding as easy as possible. There should be NA in any cells that do not have values, which is a default that many R functions recognize as default. When we utilize this data, we can easily remove these values in our code by referring to them as NA.\r\nTidy format encourages collaboration between people and data sets because we are easily able to combine data from different sources using join functions. If the data contains columns with shared variables, R can easily recognize those columns and associate its rows (observations) with the observations of the complementary data set. Using full_join() is a common join function to utilize as it maintains all data from both sources.\r\nTidy format helps you easily make subsets of your data for specific graphs and summary tables. Consider the filter() and select() functions, which help you subset to only view variable or observations of interest. In these cases, it is especially important to have only one value in each cell and standardize the way you document observations. You always want to record each lobster species with the same spelling, each size with the same number of decimal places, and each date with the same format (such as YYYY-MM-DD). For variables such as length that might need units, always include these units in the column header rather than the cell. This streamlines our coding and keeps cells to a single class. If you include numerical and character values in one cell, it will be documented as a character, which can restrict your analysis process.\r\nYour data isn’t in Tidy format? That’s alright! Check out the tidyr::pivot_longer() and tidyr::pivot_wider() functions to help you help R help you. In the example below, we have a tribble dataset that is not in Tidy format. We know this because there are multiple columns (A:C) that represent different individuals or observations of the same variable (like dog food brands). We can use pivot_longer() to put the column headers into their own column, rename that column, and pivot their values into their own column while maintaining their association with A, B, and C. Although the resulting tidy data may seem more complex at first galnce, it is easier to convert to a graph and structurally is more organized from a data science perspective.\r\nTo demonstrate some simple data tidying, lets make a tribble (which is similar to a dataframe) and manipulate it using the pivot_longer() function. In this tibble, we are\r\n\r\n\r\ndf <- tribble(\r\n  ~name, ~A, ~B, ~C,\r\n  \"dog_1\", 4, 5, 6,\r\n  \"dog_2\", 9, 10, 8\r\n)\r\n\r\ndf\r\n\r\n\r\n# A tibble: 2 x 4\r\n  name      A     B     C\r\n  <chr> <dbl> <dbl> <dbl>\r\n1 dog_1     4     5     6\r\n2 dog_2     9    10     8\r\n\r\nThis dataframe is not in tidy format, because the variable ranking is dispersed between multiple columns. We want a single variable in each column, so lets combine those columns and make it tidier:\r\n\r\n# A tibble: 6 x 3\r\n  name  dog_food_brand ranking\r\n  <chr> <chr>            <dbl>\r\n1 dog_1 A                    4\r\n2 dog_1 B                    5\r\n3 dog_1 C                    6\r\n4 dog_2 A                    9\r\n5 dog_2 B                   10\r\n6 dog_2 C                    8\r\n\r\ncase_when() and Lobsters\r\nUse this function to help tidy your data and prepare it for plotting. case_when() bins continuous data into manually defined categories and add it to your data set in the form of new column.\r\ncase_when() is used in this example to categorize lobsters into size bins based on the legal size minimum for fishing. This function processes each individual lobster we caught in this dataframe and returns if it is large enough to legally harvest from various locations along the Santa Barbara coast.\r\nFirst thing first, import your data\r\n\r\n# A tibble: 6 x 9\r\n   year month date       site  transect replicate size_mm num_ao  area\r\n  <dbl> <dbl> <date>     <chr>    <dbl> <chr>       <dbl>  <dbl> <dbl>\r\n1  2012     8 2012-08-20 IVEE         3 A              70      0   300\r\n2  2012     8 2012-08-20 IVEE         3 B              60      0   300\r\n3  2012     8 2012-08-20 IVEE         3 B              65      0   300\r\n4  2012     8 2012-08-20 IVEE         3 B              70      0   300\r\n5  2012     8 2012-08-20 IVEE         3 B              85      0   300\r\n6  2012     8 2012-08-20 IVEE         3 C              60      0   300\r\n\r\nThis data is ground-breaking! The world needs to see this and understand its implications. In order to plot this fasciating data in a meaningful way, we want to efficiently categorize our lobsters by legality status and color code their relative abundance in our visualization. Considering that the legal minimum size for a lobster is 79.76 (units), this is the threshold we will pass onto R to do the heavy lifting for us.\r\n\r\n\r\nlobsters_legality <- lobsters %>% \r\n  mutate(legal = case_when(\r\n    size_mm >= 79.76 ~ \"yes\",\r\n    size_mm < 79.76 ~ \"no\")) %>% \r\n   group_by(site, legal) %>% \r\n  summarize(site_legal_count = n())\r\n\r\n\r\n\r\nUse ggplot() to make a bar graph that color codes the lobster abundance by legality status. We communicate that we want R to color the graph by this variable by passing the argument color = legal within aes(). Manually setting colors is set outside of aes(), but here it is an argument because it is determined by a variable.\r\n\r\n\r\n\r\nDistill is a publication format for scientific and technical writing, native to the web. Learn more about using Distill at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-10-the-casewhen-function/case_when_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-11-05T22:08:22-07:00",
    "input_file": {}
  }
]
