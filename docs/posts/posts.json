[
  {
    "path": "posts/2022-05-29-capstonezambiadei/",
    "title": "Ethics and Bias Considerations in a Zambia Case Study",
    "description": "Modern remote sensing techniques combined with machine learning can help us better understand agriculture across the globe and serve as a tool to predict future trends. Within the context of predicting maize yields in Zambia, we must ask ourselves how our machine learning approach includes bias. Furthermore, it is necessary to consider the ethics involved in producing a reproducible pipeline using Zambian data that may not be accessible to many people in Zambia.",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2022-05-29",
    "categories": [
      "R",
      "Diversity, Equity, and Inclusion"
    ],
    "contents": "\n\n\n\nFor my master’s capstone project, my team produced an open-source pipeline for\nremote sensing of crop yields with a case study focused on the country\nof Zambia. The final data products included a free programming tool and\na database of processed satellite imagery to detect maize yields in\nsub-Saharan Africa. Our vision was for these products to be utilized by\nan audience of researchers, policy-makers, farmers, or anyone else\ninterested in crop yield predictions. We published our data products on\nthe MOSAIKS\nAPI website, which serves as a portal for anyone in the world to\naccess our materials, as long as the user has access to a standard\ncomputer. The diversity, equity, and inclusion goal of this project was\nto allow more people who lack a technical background or computational\nresources to execute spatiotemporal modeling techniques. My team\npre-processed years of satellite imagery to take care of the first steps\nin the pipeline, with the help of our university-funded compute power\nand faculty expertise. As a result, the next steps in the workflow do\nnot require powerful computational resources or a deep understanding of\nthe featurization process on the user’s end. Rather, the rest of the\nanalyses was designed to be able to be executed by anyone with a desire\nto learn.\nExpanding spatiotemporal modeling to a wider community is certainly a\npowerful idea. However, my capstone team needs to take a step back to\nconsider the ethics and bias in our approach by asking\nthe question:\n“Does\neveryone in our target audience have access to a computer?”\nIn the context of diversity, equity, and inclusion,\nwe recognize that not everyone has the privileges that we enjoy at the\nUniversity of California, Santa Barbara. These privileges include access\nto the computational and financial resources of the university system,\nsupport through our graduate program and faculty, and reliable\nelectricity in our homes and work spaces.\nFurthermore, we must ask:\n“Does\nour crop model show prediction bias because of bias in the data on which\nit was trained?”\nTo attempt to address these concerns, let’s begin by examining the\nquantity of Zambians who have access to the internet.\nComputer use in Zambia\n\nData Source: Data Reportal\n(1)\nThere appears to be few internet users in Zambia relative to the\nnational population. Other regions of the world have a much higher ratio\nof internet users to the total population, which represents sub-Saharan\nAfrica as relatively low on the list of global regions with internet\nusers (2). (While these data sources likely have their own bias as well,\nwe will not dive into that in this post.) This data certainly raises a\nred flag for my teams’ data products’ accessibility to the people in\nZambia, considering that our data products require the user to have\ninternet access. This is a disappointing realization, and hopefully in\nthe future internet use will increase. Until internet use shows more\nequality across the globe, we understand that our userbase is\nbiased.\nA methodological\napproach to reduce bias\nOne approach my team and I can adopt in order to reduce bias in the\ndissemination and userbase of our data products could take the form of\nensuring our data product reaches Zambian people and policy-makers\nacross the country that do have access to the internet. Expanding access\nto electricity and computers may be an unfeasible goal for my team, but\nwe can certainly spread awareness of our tool and communicate it to\naudiences in Zambia. In turn, these personnel in leadership roles\nthroughout the country can hopefully disseminate this information to\npeople with fewer resources and advocate for their community. This is\npotentially feasible through the personnel at the Baylis Lab at UC Santa\nBarbara that provided the data originally. A member of the Baylis Lab is\nZambian, and has shown wonderful enthusiasm for connecting my team’s\ndata products with policy-makers and leaders in sub-Saharan Africa.\nAdditionally, we can potentially reach Zambians through the Global Policy Lab\nthat hosts our processed satellite imagery within its larger database on\nthe MOSAIKS\nAPI (5). By checking in with the Baylis Lab and the Global Policy\nLab on a regular basis and integrating inclusive practices in our future\nmodeling projects, we can methodically reduce bias in our userbase.\nOutreach, collaboration, and patience can be powerful tools to instigate\nmeaningful change in the scientific community and in governmental\npolicy.\nBias in Our Model\nNext, let’s consider the bias in the “ground-truth” crop data that we\nused to train our model. My team used a supervised machine learning\napproach to train a model to predict maize yields based on satellite\nimagery. My team had access to annual maize yields reported by farmers\nacross the country at district resolution, and we paired this data with\nsatellite images of farms for several several years. The model “learned”\nhow to identify maize, and therefore can serve as a supplementary tool\nto quantify maize yields over time, even where and when we lack\ndistrict-level maize yield data from the farmers themselves.\n\nMaize yields in Zambia (2014-2021):\ncomparison of annual yields reported by the Zambia Statistics Agency and\nyields predicted by satellite imagery collected by Landsat 8 and\nSentinel 2, processed via the MOSAIKS machine learning approach applied\nover time (6, 7).\nSo how should we go about identifying the bias in our model? One\nmight argue that a model is only as accurate and powerful as the data\nused to train it, and so we turn to the UC Santa Barbara Baylis Lab’s\ncrop data and the metadata. Both the contents of the metadata and what\nis not present can be useful for identifying potential bias.\nFirstly, the source and units of the data were not explicitly outlined\nby the Baylis Lab at the start of this project, and these details are\nnot well-documented by the Zambia\nStatistics Agency that hosts this data for public use (3). Missing\nmetadata includes the mode of communication through which these surveys\nwere conducted, such as physical meetings with farmers, phone calls,\nonline submissions, or mailed written reports. Differing levels of bias\ncould be associated with each type of communication. For example, if\nsubmissions are made online, perhaps this excludes farmers without\nelectricity or access to the internet. If yields are communicated\nverbally or on paper, farmers might have no way to verify that their\nreports were entered into the database and processed correctly, and they\nmight not reap the benefits that result of these compiled databases.\nOn the other hand, an important detail that was effectively\ncommunicated was the resource-intensive nature of these surveys, such as\nthe time it takes to gather the data and personnel required to process\nit (3). The farmers assess their maize fields each May to report the\narea planted, expected yields, size of the farm, and total irrigated\narea. During the harvest season in August, the farmers report the final\nyields (3). The reports of farm size and irrigated area are rough\nestimates, and the resource-intensive nature of these surveys means not\nevery farm can be included (3). All these factors imply that perhaps not\nevery farm is surveyed, especially small-scale farms. Furthermore, the\nfact that farmers might need funding for irrigation from the government\nor other investors might serve as an incentive to over-report yields to\nshow productivity and potential, or perhaps under-report yields in order\nto demonstrate a need for more funding.\nLastly, we must consider bias in the satellite data. Considering the\nresolution of our satellite imagery and its limited ability to penetrate\ncloud cover during the rainy season, are we consistently documenting\nevery field in Zambia each year? Are the satellites only picking up\nlarge farms and systematically missing smaller-acreage farms or those\nthat grow smaller maize because they do not use growth hormones?\nAssuming perfect accuracy would certainly be bold and irresponsible.\nOverall, no data is perfect because no human or satellite can\nperfectly report agricultural data with consistency. As a result, our\nmodel based on two biased data sources is certainly riddled with bias as\nwell. Acknowledging such bias and adjusting our methods in the future is\ncritical to getting closer to the truth, even if we never quite make it\nthere. When scientific modeling is applied to critically important\ntopics such as food security, agricultural funding, and job security in\nsub-Saharan Africa, it is our responsibility as scientists to ensure we\npublicize the limitations of our analyses.\nConclusion\nThe bias that exists in our crop data, satellite data, and\nthe userbase of our data products highlights bias and issues for\nadvancements in farming and data transparency across Zambia and the rest\nof sub-Saharan Africa. Farming in this region of the world is\npartially subsidized through investments from China and other countries,\nand foreign investments are expected to increase in the future (3).\nAgricultural investors likely partially base their funding allocation\ndecisions on crop production trends across Zambia and the rest of\nsub-Saharan Africa. If these investors have more insight into\nlarge-scale crop trends than the farmers themselves, or if these\ninvestors receive highly biased data, then my capstone group’s\ndiversity, equity, and inclusion goals have not been achieved.\nWe must not lose sight of error and exclusion in the scientific\ncommunity regardless of our confidence in technology or our emotional\ninvestment in the question we are trying to answer.\nAs an aspiring environmental data scientist, it is my duty to\nconsider ethics and bias in my projects and minimize it in the future.\nMy capstone project certainly has not achieved substantive change in the\nagricultural world, and it likely has not reached anyone in Zambia yet.\nHowever, we have educated a niche community of people about the power of\nremote sensing techniques and laid the foundation for a tool that has\npotential to inform agricultural policy. Substantive change must begin\nsmall before it can gain traction.\nReferences & Sources\nDatareportal\nOur World in\nData\nUniversity of\nCalifornia, Santa Barbara, Department of Geography, Baylis Lab.\nPersonal communication (April 11, 2022)\nMaize\nphotograph\nGlobal Policy\nLab\nMOSAIKS\nscientific paper:\nRolf, Esther, Jonathan Proctor, Tamma Carleton, Ian Bolliger, Vaishaal\nShankar, Miyabi Ishihara, Benjamin Recht, and Solomon Hsiang. “A\nGeneralizable and Accessible Approach to Machine Learning with Global\nSatellite Imagery.” Nature Communications 12, no. 1 (December 2021):\n4392. https://doi.org/10.1038/s41467-021-24638-z.\nZambia\nStatistics Agency\n\n\n\n",
    "preview": "posts/2022-05-29-capstonezambiadei/pics/pop_internet_plot.png",
    "last_modified": "2022-06-11T21:23:51-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-30-whalesdominica/",
    "title": "Whale Habitat & Vessel Collisions in the Caribbean",
    "description": "Whales migrate and raise their young in seascapes that are heavily utilized for cargo transport, ecotourism, and fishing. Perhaps reducing vessel speed in these whale-inhabited zones will reduce ship-whale collisions. Using spatial tools in Python, we investigate the overlap of these two components and calculate the increase in vessel commute time in a hypothetically enforced speed reduction zone.",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2022-04-30",
    "categories": [
      "Python",
      "Spatial Analysis"
    ],
    "contents": "\n\nContents\nProtecting Whales from\nShips\nLoad packages\nRead in two data files:\nWhale Sightings Data\nCreate Grid\nExtract Whale Habitat\nPlot the Speed Reduction\nZone\nVessel Data\nCalculate\nDistance and Speed of Vessels\nAcknowledgements\nRelated Resources\nPhoto Sources:\n\n\nProtecting Whales from Ships\nIn this Python analysis, we aim to determine the optimal spatial\noutline of a speed reduction zone for boats in Dominica, an island\ncountry in the Caribbean that is well-known for its year-round sperm\nwhale population (source).\nWe do this by creating a grid across Dominica’s seascape where each cell\nrepresents the number of reported whale sightings in that subregion. We\nthen use automatic identification system data to track the vessel\nmovement in this same region. Lastly, we calculate the effect of a\nhypothetical speed reduction zone on the local traffic.\n\nA sperm whale, the species that migrates\nthough the seascape around Dominica.\n“Whales\nare among the largest creatures on earth and play vital roles in\nmaintaining healthy underwater ecosystems. Yet, intensive whaling over\nthe past 200 years has brought many populations to the brink of\nextinction, and today many species remain threatened or endangered.\nAlthough hunting has decreased dramatically over the last century,\nanother danger threatens whales – massive cargo ships. Whale-ship\ncollisions are currently a leading cause of death for large whales, and\nscientists estimate that over 80 blue, humpback, and fin whales are\nkilled by vessel collisions on the West Coast of the United States each\nyear … As global maritime traffic continues to increase, it is critical\nthat we implement solutions now to protect endangered whales.” - Whale Safe,\nBenioff Ocean Initiative\n\nScotts Head, Dominica, a location\nfrequented by sperm whales\nLet’s open up a fresh Jupyter Notebook and get started investigating\nthe intersection of whale migration and human marine traffic in\nDominica.\nLoad packages\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom shapely.geometry import Polygon\nimport datetime as dt\nRead in two data files:\nA spatial data file that defines the boundaries of Dominica, the\nisland country in the Caribbean where our ship & whale investigation\ntakes place.\nWhale sightings data, which contains times and locations of\napproximately 5,000 whale sightings in Dominica between 2008 and\n2015.\n# import dominca's spatial data as a geodataframe\ndominica_outline = gpd.read_file('data/dominica/dma_admn_adm0_py_s1_dominode_v2.shp')\n\n# import sightings as a geodataframe \nsightings = gpd.read_file('data/sightings2005_2018.csv')\nSet the coordinate reference systems we will use in this\nanalysis:\nprojected_EPSG = 2002   # Dominica 1945 / British West Indies Grid\ngeodetic_EPSG  = 4326   # WGS 84 (use as default CRS for incoming lat & lon)\n\n# switch to crs 2002\ndominica_outline = dominica_outline.to_crs(projected_EPSG)\ndominica_outline\n\n\nWhale Sightings Data\nBootstrap the geometries from the latitude & longitude and set\nthe CRS to EPSG 4326 initially, and we will convert it to\nEPSG 2002 when we are done appending the geometries. This\n2-step process ensures that the latitude and longitude are projected\ncorrectly.\nsightings_geom = gpd.points_from_xy(x = sightings.Long, \n                                    y = sightings.Lat, \n                                    crs = geodetic_EPSG)\n\n# append the geometries to the sightings data\nsightings = sightings.set_geometry(sightings_geom)\n\n# set CRS to local EPSG\nsightings = sightings.to_crs(epsg = projected_EPSG)\nCreate Grid\nCreate a basic grid object that will be the starting point for the\noceanic grid around Dominica that will be tailored to whale habitat:\nfig, ax = plt.subplots(figsize=(3, 3), \n                       dpi=200)\nax.grid(True)\n\n\n\nDetermine the minimum and maximum x and y\nbounds of all sightings using the .total_bounds\nfunction:\nminx, miny, maxx, maxy = sightings.total_bounds\nminx, miny, maxx, maxy\n# total_bounds in meters:\n# (408480.65208368783, 1532792.7459409237, 498500.3049570159, 1796964.399702923)\nCreate arrays of the indices of our grid, with the\nvalues denoting the southwest corner of each grid cell. The syntax here\nis pretty abstract, but it gets the job done. The 2000\nargument represents the quantity of meters in each direction:\n# the grid cell dimensions are 2000 meters in both the x and y direction\nxs = np.arange(minx, \n               maxx, \n               2000)\n               \nys = np.arange(miny, \n               maxy, \n               2000)\n               \n# convert the corner points into proper cell polygons\ndef make_cell(x, y, cell_size):\n    ring = [\n        (x, y),\n        (x + cell_size, y),\n        (x + cell_size, y + cell_size),\n        (x, y + cell_size)\n    ]\n    cell = Polygon(ring)\n    return cell\n    \n# iterate over each combination of x and y coordinates in a nested for loop\ncells = []\nfor x in xs:\n    for y in ys:\n        cell = make_cell(x, y, 2000)\n        cells.append(cell)\n\ngrid = gpd.GeoDataFrame({'geometry': cells}, \n                        crs = projected_EPSG)\n                        \n# plot the grid and increase the figure size to 20, 20 to better visualize without a fill color\ngrid.boundary.plot(figsize=(20,20))\n\n\n\nExtract Whale Habitat\nExecute an inner spatial join of the sightings data, with the grid\nbeing the first dataset named because we want the output to be in grid\ncells rather than points:\n# the join produces a row for each intersection of each grid cell with each sighting\njoined = grid.sjoin(sightings, \n                    how = \"inner\")\n                    \n# plot the whale habitat in the form of cells\njoined.plot(figsize = (15, 15))\n\n\n\nGroup the rows of the joined dataset to be able to count the number\nof whale sightings per cell:\ngrid['count'] = joined.groupby(joined.index).count()['index_right']\njoined\nCount the minimum and maximum number of whales observed in the grid\ncells to get an idea of the range of observations over space:\n# check the max count of whales in 1 cell\nmax_count = grid['count'].max()\n# check the minimum count of whales in 1 cell\nmin_count = grid['count'].min()\n\n# check the unique values in the count column to get an idea about the range of the sightings per cell\ngrid['count'].unique()\n\n\n\nSubsample the grid cells for only those with 20 or more observations,\nbecause when defining protected species habitat we should prioritize\nprotecting the subregions that are more densely populated:\n# fitler the data for only sightings that included more than 20 whales\ngrid_subset = grid[grid['count'] > 20]\nExecute a unary union and take the convex hull of that\nunion to determine a comprehensive speed reduction zone. Next, take the\nconvex hull of the unary union in order to\ndesginate the speed reduction zone, essentially rounding out the cell\npolygons from the unary union.\n# unary union\ngrid_subset_union = grid_subset.unary_union\ngrid_subset_union\n\n\n\n# convex hull\ngrid_subset_hull_poly = grid_subset_union.convex_hull\ngrid_subset_hull_poly\n\n\n\nConvert grid_subset_hull to a geodataframe with the local CRS for\nDominica; EPSG 2002\ngrid_subset_hull_gdf = gpd.GeoDataFrame(index=[0], \n                                        crs= projected_EPSG, \n                                        geometry=[grid_subset_hull_poly])\nPlot the Speed Reduction\nZone\nNow that we have determined the grid cells with whale sightings\naround Dominica, lets move on to defining the ideal speed reduction\nzones for ships in this region. Cargo ships, large fishing vessels, and\necotourism vessels all pose a threat to whales. If the ships travel\nslower and are more vigilant of the surrounding marine life, perhaps we\ncan find a middle ground in which whales can safely feed, raise their\nyoung, and migrate alongside humans in the marine region around\nDominica. To make this compromise, we aim to determine the portion of\nthe seascape that is most critical to reduce speed, rather than trying\nto restrict speeds everywhere, which is unrealistic as it would be\nimpossible to regulate. Furthermore, if whales are not observed often in\na certain region of the ocean, restricting speeds there would be more\ndisruptive to human livelihoods than beneficial to whale movement.\nfig, ax = plt.subplots(figsize = (4,4), \n                       dpi = 200)\nax.set_facecolor('xkcd:lightblue')\ngrid_subset_hull_gdf.plot(ax = ax, \n                          facecolor = 'None')\ndominica_outline.plot(ax = ax, \n                      facecolor = \"green\")\nplt.title('Speed Reduction Zone for Whales Near Dominica', \n          y = 1.04)\nplt.xlabel('Latitude (meters)')\nplt.ylabel('Longitude (meters)')\n\n\nNow we have completed the whale habitat delineation!\nVessel Data\nMarine commercial vessels or any vessel of at least 300 gross tonnage\nare required to be fitted with an “automatic identification system”\nreceiver to monitor their movement. The beacons emitted from these\ntransmitters are received, encoded, and archived by terrestrial\nstations.\n\n\n\nSimilarly to how we used whale sightings to determine the spatial\nextent of whale habitat, we will use temporal and spatial aspects of\nautomatic identification system data to determine vessel speeds in the\nsame region.\nSource of automatic identification system data from vessels in 2015:\nthe Dominica Sperm Whale\nProject\nSource of automatic\nidentification system data for vessels worldwide\nRead in the vessel data:\nvessel = gpd.read_file('data/station1249.csv')\nSimilarly to what we processed the spatial sightings data, here we\nbootstrap the vessel geometries from the latitude & longitude and\ninitially set the CRS to EPSG 4326. We will convert it to\nthe local EPSG 2002 after we append the geometries:\nvessel_geom = gpd.points_from_xy(x = vessel.LON, \n                                 y = vessel.LAT, \n                                 crs = geodetic_EPSG)\n# this generated a geometry array of shapely point geometries from coordinates\n\n# finish bootstrpping the geometries by appending this geoseries to the whales dataset\nvessel = vessel.set_geometry(vessel_geom)\nvessel\n\n\n\n# set the local CRS\nvessel = vessel.to_crs(projected_EPSG)\n\n# visualize the raw vessel points\nvessel.plot()\n\n\n\nSubset the vessel data for only the vessels within the area of\ninterest and format the data to prepare for temporal and spatial\ncalculations:\nvessel = vessel[vessel.intersects(grid_subset_hull_poly)]\n\n# set the TIMESTAMP column to type datetime\nvessel['TIMESTAMP'] = pd.to_datetime(vessel['TIMESTAMP'], \n                                     format = '%Y-%m-%d %H:%M:%S')\nCalculate Distance and\nSpeed of Vessels\nWe will calculate the distance between two successive locations of\nthe same vessel, as well as the time difference between the time stamps\nof those two locations. We will use the tried and true equation we have\nmemorized from middle school:distance = rate * time\nFirst, sort the vessels by their Maritime Mobile Service Identity\n(MMSI), which is a unique identification number for each vessel:\nvessel_sorted = vessel.sort_values([\"MMSI\", \"TIMESTAMP\"], \n                                   ascending = True)\n\n# create a copy of the raw data because we need to shift the columns in the working copy\n# we are trying to match 2 timestamps and their respective locations to each singular vessel    \nvessel_sorted_copy = vessel_sorted.shift()\n\nvessel_joined = vessel_sorted.join(vessel_sorted_copy, \n                                   lsuffix = '_end', \n                                   rsuffix = '_start')\n                                   \n# drop rows in which the MMSI_start does not match MMSI_end, because we can only compare time stamps from the same vessel\nvessel_joined_mmsi_match = vessel_joined[vessel_joined['MMSI_start'] == vessel_joined['MMSI_end']]\n\n# take a look at the dataframe now that it is prepped for speed calculations\nvessel_joined_mmsi_match\n\n\nClean up the dataframe geometries:\n# reset one of the geometry columns as a geometry type, because the geodataframe only needs 1 geom column specified\nvessel_joined_mmsi_match = vessel_joined_mmsi_match.set_geometry('geometry_start')\nvessel_joined_mmsi_match.crs\n# note: we are still able to do spatial operations with BOTH geometry columns because they are both shapely objects\nCalculate the distance between successive locations for each vessel,\nusing the MMSI:\nvessel_joined_mmsi_match['distance_m'] = vessel_joined_mmsi_match.distance(vessel_joined_mmsi_match['geometry_end'])\nCalculate the time difference for each vessel from one observation to\nthe next by subtracting one time stamp from the other:\nvessel_joined_mmsi_match['time_diff'] = (vessel_joined_mmsi_match['TIMESTAMP_end'] - vessel_joined_mmsi_match['TIMESTAMP_start'])\nCalculate the average speed in meters per second using (distance) =\n(rate)*(time):\n# divide the distance column by the time_diff column\n# use the function `total_seconds()` to get the time in units of seconds\nvessel_joined_mmsi_match['speed_m_s'] = vessel_joined_mmsi_match['distance_m']/vessel_joined_mmsi_match['time_diff'].dt.total_seconds()\nWhat is the time that the distance would have taken at 10 knots?\n1 knot = 1 nautical mile, so we convert our meters to\nnautical miles and then convert the time to hours.\n# convert distance column units from meters to miles\nmeters_per_nm = 1852.0\nvessel_joined_mmsi_match['distance_nm'] = vessel_joined_mmsi_match['distance_m']/meters_per_nm\nvessel_joined_mmsi_match\n\n# divide the distance by 10 since we are travelling at 10 times the speed, then multiply that output by 3600 because we want units of seconds \nvessel_joined_mmsi_match['time_10_nm_s'] = vessel_joined_mmsi_match['distance_nm']/10*3600\nCalculate the difference between the time that it actually took and\nhow much it would have taken at 10 knots:\n# subtract the actual time from the predicted time at 10 knots, because we want to figure out how much MORE time would be spent if every vessel traveled at 10 knots\nvessel_joined_mmsi_match['time_diff_actual_vs_10k'] = (vessel_joined_mmsi_match['time_10_nm_s']) - vessel_joined_mmsi_match['time_diff'].dt.total_seconds()\n# we want to be able to filter out the differences in times for boats that went SLOWER than 10 knots, we don't want to sum EVERY time difference\nFilter for the times that are greater than zero, because we only want\nto calculate the extra time that is spent by those boats that were going\nfaster than 10 knots that would slow to 10 knots. We do not want to\nconsider the boats that were already travelling slower than this\nspeed:\nvessel_joined_mmsi_match = vessel_joined_mmsi_match.loc[vessel_joined_mmsi_match['time_diff_actual_vs_10k']>0]\nSum up the time difference values and convert the units from seconds\nto days:\nsum(vessel_joined_mmsi_match['time_diff_actual_vs_10k']/ 60/60/24)\nThe difference between the time that it actually took and how much it\nwould have taken at 10 knots is approximately 27.87 days, on average.\nMission accomplished.\nCan we use this analysis to help support a policy change in Dominica\nto enforce a speed reduction zone and protect the vulnerable sperm whale\npopulation? Future investigation of this data might include diving into\nthe MMSI codes for the vessels to determine which vessels are speeding\nmore often, revealing which vessels should be targeted in this policy.\nAdditionally, after such a policy is put into place, we can compare the\nwhale sightings afterward to those before, and hopefully see an increase\nin whale activity!\nAcknowledgements\nI would like to thank my collaborator on this project, Sydney Rilum from\nthe Bren School of Environmental Science and Management, for her\ncontributions. I will always remember the excessive amount of time we\nspent googling coordinate reference systems, adjusting our vessel speed\ncalculations, and iterating through different grid cell sizes.\nThank you, Dr. James Frew and Niklas Griessbaum from the Bren\nSchool of Environmental Science and Management, for prompting us to\ninvestigate this question. Your expertise in spatial data science\ncontinues to inspire me to answer more questions like this in my future\ncareer.\nThank you for following along with this analysis, and please contact\nme with any suggestions or comments at my GitHub page here or via email (jscohen@bren.ucsb.edu)\nRelated Resources\nIf you are interested in acoustic whale monitoring in the Caribbean,\ncheck out this\npaper by Heenehan el al. 2019 that discusses Caribbean soundscapes\nof these magnificent marine mammals and anthropogenic influences that\nchallenge their survival.\nIf you are interested in exploring how migratory whales and human\nvessels are being tracked and regulated in the Santa Barbara Channel,\nplease check out the novel Whale Safe\nmapping and analysis tool from the Benioff Ocean Initiative! You can\nalso subscribe to their weekly emails to stay up-to-date with recent\nvisual and acoustic whale detections of all the species that live in the\nChannel.\nPhoto Sources:\nsperm\nwhale photo\nScotts Head,\nDominca\ncargo\nvessel\n\n\n\n",
    "preview": "posts/2022-04-30-whalesdominica/outputs/speed_zone_dominica.png",
    "last_modified": "2022-05-14T10:38:21-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-03-newhorizonsinconservationconference2022/",
    "title": "New Horizons in Conservation Conference 2022 - Yale School of the Environment",
    "description": "Using remote sensing tools to predict crop yield in sub-Saharan Africa",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2022-03-16",
    "categories": [
      "Python",
      "Diversity, Equity, & Inclusion"
    ],
    "contents": "\nAn\nopen-source pipleline for remote sensing of crop yields under\nenvironmental change in sub-Saharan Africa\n\n\n\n\n\n\nAs a student in the Master of Environmental Data Science program at\nthe Bren School of Environmental Science and Management, I am working on\na capstone project aiming to create an open-source tool to predict crop\nyields over time in sub-Saharan Africa using remote sensing techniques.\nMy teammates are Grace Lewin, Cullen Molitor, and Steven Cognac, and our\nadvisor is Dr. Tamma Carleton from the Bren School and the MOSAIKS team at the\nGlobal Policy Lab. We are a collaborative group of data scientists\ninterested in the relationship between environmental trends and food\nsecurity in regions of the world that suffer from data scarcity. I am\nvery fortunate to be a part of this wonderful team working towards a\nbetter understanding of environmental implications on human\nwell-being.\nThe MOSAIKS capstone team:\n\nFrom left to right: Grace Lewin, Cullen\nMolitor, Juliet Cohen, Steven Cognac. Photo taken at the National Center\nfor Ecological Analysis and Synthesis, March 2022.\nABSTRACT:\nThe environmental and social impacts of climate change are\ndisproportionately distributed worldwide. Many highly impacted regions\nlack the assets to monitor and generate resource predictions, and\ntherefore lack high-quality environmental and social data. As a result,\nit is difficult to make predictions about the impacts of climate change\nfor these regions using conventional modeling. Recently, machine\nlearning approaches applied to high-resolution satellite imagery have\nbeen successful in making predictions of a wide range of social and\nenvironmental variables. However, generating these predictions comes\nwith significant barriers, including high computational, data storage,\nexpertise, and financial resource costs. Reducing the financial and\ncomputational burden of machine learning approaches is essential to\nincreasing the equity of environmental monitoring processes and outputs.\nSub-Saharan Africa is one of these data-limited regions and is likely to\nsuffer some of the largest impacts from climate change globally. To\nenable increased monitoring and data access across the sub-continent, we\napply the novel machine learning approach, MOSAIKS, to create tabular\nfeatures for sub-Saharan Africa using satellite imagery. These features,\npaired with ground-truth crop data from three countries, will be used to\nbuild a model that predicts crop yields over time in regions without\ncrop data. This model will provide new insights into the historical crop\nyields of the region. Furthermore, these tabular features of satellite\nimagery, and the methodology developed to create them, will enable more\npeople around the globe to build models and generate predictions of\nother social and environmental variables in this region.\n\n\n\nNew\nHorizons in Conservation Conference 2022 - Yale School of the\nEnvironment\nMy teammates and I were invited to attend the New\nHorizons in Conservation Conference 2022 at the Yale School of the\nEnvironment. Having the opportunity to participate in this\nconference is an honor and a privilege. We are grateful to the Yale\nSchool of the Environment for showing interest in our project and giving\nus the chance to explore how other equity-driven scientists are\nexpanding the horizons of the data world.\nMy teammates and I are very passionate about the tool and\ndata products we are contributing to the remote sensing field. We hope\nother scientists and users will be able to improve upon it in the\nfuture, especially as we expect to face more extreme climate\nfluctuations moving forward with climate change.\nAs part of our participation in this conference, we created a video\nand poster to share our progress as well as our vision for the\nend-products. We look forward to showcasing this novel open-source tool\nthat will hopefully increase access to powerful remote sensing\nproducts.\nThe MOSAIKS API\nMoving forward, we aim to contribute our data, code, and\ninstructional documentation to the MOSAIKS\nAPI. This user-friendly API will allow anyone to access our data and\nuse it for their needs. Users may take the form of environmental\npolicy-makers, politicians involved in food security in sub-Saharan\nAfrica, researchers interested in environmental or socioeconomic trends,\nor anyone else! The interface allows users to download “feature data”\nfor any place on earth and run analysis over space and time, for any\ntask of interest, given that the data is available for that region.\nThe MOSAIKS API database contains geolocated data files with\ntask-agnostic “features” for many regions of the world. These features\nare satellite image data converted to meaningful numbers through a\nmathematical process called “random convolutional featurization.” This\napproach is a subset of unsupervised deep learning that is praised for\nits accuracy and computational efficiency. Users can access this tabular\nfeature data and run statistical analysis (such as ridge regression) to\nanswer their environmental and socioeconomic questions.\nWhat is a task of interest?\nAn example of a task is determining how a characteristic of the\nlandscape changes over time, such as how forest cover changes in the\nCongo or how wetland habitat fluctuates. Satellite images can help us\nunderstand these trends, and a single set of feature data is\ntask-agnostic because the features were originally created using\nunsupervised machine learning, and the MOSAIKS model is then trained on\nthose features and task-specific data using supervised machine learning.\nDetailed Jupyter notebooks hold the user’s hand as they adapt Python\ncode to fit their task’s needs and expand the horizons beyond what they\never thought was possible with remote sensing data.**\nView\nthis 3-minute video to learn more about our project’s objective,\napproach, and how it relates to equity and inclusion in the world of\nenvironmental data science:\n\nVideo\n\nHow\ndoes this remote sensing tool increase equity in the field of\nenvironmental data science?\nThe MOSAIKS system and its resulting products can provide\npolicy-makers, researchers, and users in sub-Saharan Africa (an future\nregions in the future) with a tool to address environmental questions in\nless time with fewer resources. Innovative remote sensing tools and\nmachine learning can fill in current gaps in data for many regions of\nthe world. By filling in these data gaps, decision-makers can study how\na region might be affected by global climate change and develop\nmitigation efforts accordingly. With this tool, countries can compare\nenvironmental trends on a broader scale, establish collaborative food\nsecurity policies, and fight climate change from well-informed\nperspectives.\nIf you are interested in learning more about this project, or if you\nhave suggestions to improve our methodology or data products, please\nfeel free to contact my team at\ncp-cropmosaiks.bren.ucsb.edu or create an issue on our\ngithub organization, cropMOSAIKS. Thank you for\nreading, and happy coding!\n\n\n\n",
    "preview": "posts/2022-03-03-newhorizonsinconservationconference2022/poster_video/poster_png.png",
    "last_modified": "2022-06-18T18:42:40-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-25-sqlwindpower/",
    "title": "Using SQL & Python to Calculate Iowa's Wind Power Potential",
    "description": "How many wind turbines can we legally fit into the state of Iowa, and how much renewable energy would that yield?",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-12-25",
    "categories": [
      "Python",
      "SQL",
      "Spatial Analysis"
    ],
    "contents": "\n\nContents\nObjective\nApproach\nLoad Packages & Import\nData\nQueries for Siting\nConstraints\nSubqueries\nMega-Query\nWind Data\nResults\nAcknowledgements\n\n\n\n\n\nObjective\nRenewable energy is a rapidly growing industry in the United States,\nand we will only rely on it more over time. Wind energy collected by\nturbines is a wonderful example of a green source of energy that can\nhelp America move towards a sustainable future. In order to make this\ndream a reality, our country must meticulously plan where we can place\nwind farms. We can use data science to determine how many wind turbines\nwe can fit into certain plots of land while taking urban structures and\nnatural protected spaces into account.\nIn particular, the state of Iowa has ideal environmental conditions\nfor wind farms with its flat landscape and high, consistent wind speeds.\nThis project aims to evaluate the maximum potential annual wind energy\nproduction available to the state of Iowa (meaning the quantity of\nmegawatt hours that would be generated by placing as many 3.45 MW wind\nturbines as possible on all appropriate sites).\nApproach\nThis will be executed by first identifying all land suitable for wind\nturbine placement, subject to the siting constraints using Python, SQL,\nand PostGIS/OpenStreetMap. Subsequently, the area of each polygon\nsuitable for wind production can be calculated, along with the number of\nwind turbines that could be placed in each polygon based on the minimum\ndistance that they need to be from one another. This information will\nthen be used to calculate the total annual energy production that would\nbe realized by the maximum possible number of new turbines.\nWe will be hypothetically using Vestas\nV136-3.45 MW turbines, which have a 150m hub height.\nLoad Packages & Import Data\nOur goal is to apply queries to a PostGIS database that contains\nIowa’s feature data from OpenStreetMap. In order to connect our Jupyter\nNotebook to the PostGIS database, we use the package\nsqlalchemy.\nimport sqlalchemy as sa\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport math\nPostGIS Database Connection:\npg_uri_template = 'postgresql+psycopg2://{user}:{pwd}@{host}/{db_name}'\ndb_uri = pg_uri_template.format(user='eds223_students', pwd='eds223', host='128.111.89.111', db_name='osmiowa')\nengine = sa.create_engine(db_uri)\nThere are multiple tables in this engine. Shapefiles can take the\nform of points, lines, or polygons, and we need to query each of these\ntypes separately, so we need three separate PostGIS “connections”.\nosm_polygons = gpd.read_postgis('SELECT * FROM planet_osm_polygon', con = engine, geom_col = 'way')\nosm_lines = gpd.read_postgis('SELECT * FROM planet_osm_line', con = engine, geom_col = 'way')\nosm_points = gpd.read_postgis('SELECT * FROM planet_osm_point', con = engine, geom_col = 'way')\n# we use the column \"way\" because that is the title of the geometry column in this database\nWe also need to make a separate connection to the database to pull in\nthe wind speed data for each geometry, which we will utilize at the end\nof our querying in order to calculate the wind power production:\nosm_wind = gpd.read_postgis('SELECT * FROM wind_cells_10000', con = engine, geom_col = 'geom')\nQueries for Siting\nConstraints\nOne cannot legally place wind turbines anywhere they please within\nthe state of Iowa; there are restrictions in place that create\nrestricted buffer zones around homes, airports, protected natural areas,\nand more. We need to adhere to these restrictions, only considering\nresidential buildings buffer scenario 1:\n Define important variables that\nyou will utilize within the queries:\n# H = turbine height (tower base ↔ tip of vertical rotor), in meters\nH = 150\n# this value can be found on the website for Vestas wind turbines, linked above\n\n# distance required from airports, in meters\naero_dist = 7500\n\n# d = rotor diameter, in meters\nd = 136\n# this value can be found on the website for Vestas wind turbines, linked above\nSubqueries\nSeparate queries are necessary for each feature, as each requires a\ndifferent buffer zone around their geometries. We assign these SQL\nqueries to Python objects, then we will combine them in the next step.\nWe use the syntax f\"\"\" with triple quotes so we can format\nthe code within the quotes with variables and line breaks for vertical\norganization.\n# use scenario 1 with 3*H serving as the required distance\nsql_buildings_residential = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway,\nST_BUFFER(way, 3 * {H}) as way \nFROM planet_osm_polygon \nWHERE building IN ('yes', 'residential', 'apartments', 'house', 'static_caravan', 'detached')\nOR landuse = 'residential'\nOR place = 'town'\"\"\"\n\nsql_buildings_non_residential = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway,\nST_BUFFER(way, (3 * {H})) as way\nFROM planet_osm_polygon \nWHERE building NOT IN ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\"\"\n\nsql_aeroway = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway,\nST_BUFFER(way, {aero_dist}) as way\nFROM planet_osm_polygon \nWHERE aeroway IS NOT NULL\"\"\"\n\nsql_military = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway, way\nFROM planet_osm_polygon\nWHERE (landuse = 'military') \nOR military IS NOT NULL\"\"\"\n\nsql_highway = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway,\nST_BUFFER(way, (2 * {H})) as way\nFROM planet_osm_line \nWHERE (railway NOT IN ('abandoned', 'disused')) \nOR highway IN ('motorway', 'trunk', 'primary', 'seconday', 'primary_link', 'second'\"\"\"\n\nsql_leisure = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway, way\nFROM planet_osm_polygon\nWHERE leisure IS NOT NULL \nOR \"natural\" IS NOT NULL\"\"\"\n\nsql_river = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway,\nST_BUFFER(way, (1 * {H})) as way\nFROM planet_osm_line \nWHERE waterway IS NOT NULL\"\"\"\n\nsql_lake = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway, way \nFROM planet_osm_polygon \nWHERE water IS NOT NULL\"\"\"\n\nsql_power_lines = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway,\nST_BUFFER(way, (2 * {H})) as way\nFROM planet_osm_line\nWHERE power IS NOT NULL\"\"\"\n\nsql_power_plants = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway,\nST_BUFFER(way, (1 * {H})) as way\nFROM planet_osm_polygon \nWHERE power IS NOT NULL\"\"\"\n\nsql_wind_turbines = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway,\nST_BUFFER(way, (5 * {d})) as way\nFROM planet_osm_point\nWHERE \"generator:source\" IS NOT NULL\"\"\"\nMega-Query\nCombine the subqueries into one mega-query so we can subtract all\nthese restricted geometries as one unit. These subtracted geometries can\nbe considered a “mask” in spatial data science jargon.\nmask_1 = f\"\"\"{sql_buildings_residential}\nUNION\n{sql_buildings_non_residential}\nUNION\n{sql_aeroway}\nUNION\n{sql_military}\nUNION\n{sql_highway}\nUNION\n{sql_leisure}\nUNION\n{sql_river}\nUNION \n{sql_lake}\nUNION \n{sql_power_lines}\nUNION \n{sql_power_plants}\nUNION\n{sql_wind_turbines}\"\"\"\n\nmask_1_df = gpd.read_postgis(mask_1, con = db_uri, geom_col = 'way')\n\n# take a look at the state of Iowa without this \"mask\" of geometries\nmask_1_df.plot()\n\nWind Data\nSubtract the union of the string constraints from the wind cells so\nwe are left with only those (fractions of) cells that could accommodate\nnew wind turbines. These are “suitable cells”.\nsuitable_cells_1 = osm_wind.overlay(mask_1_df, how = 'difference')\n\n#Find area of each suitable cell/geom in the database\nsuitable_cells_1['suitable_cell_area'] = suitable_cells_1.geom.area\nNow we can calculate the area of the turbine footprint as a circle\nwith a radius of 5 rotor diameters, with the goal of calculating a\nscenario in which turbine towers must be at least 10 rotor diameters\napart.\nCalculate a buffer circle around each wind turbine, called a “turbine\nfootprint”.\nturbine_footprint = math.pi*((5*d)**2)\nCalculate the number of wind turbines that could be placed in each\npolygon, by dividing each suitable cell by the turbine footprint.\nsuitable_cells_1['n_turbines'] = suitable_cells_1['suitable_cell_area'] / turbine_footprint\nCalculate the total wind energy produced per cell by multiplying the\namount of turbines in each suitable cell by the annual wind production\nfor each turbine.\nFormula:E = 2.6 s m-1 v + -5\nGWhE = energy production per turbine in GWhv = average annual wind speed in m s-1\nsuitable_cells_1['energy_prod_per_cell'] = suitable_cells_1['n_turbines'] * ((2.6 * suitable_cells_1['wind_speed']) - 5)\nResults\nSum the energy production over all the cells into a single statewide\nnumber for residential exclusion distance scenario 1.\ntotal_energy_product_1 = sum(suitable_cells_1['energy_prod_per_cell'])\nThe\nmaximum annual potential wind energy production available to the state\nof Iowa is 1036574.26 GWh.\nPlease feel free to reach out with any similar applications for this\nnotebook, and I welcome all questions and suggestions about my code. May\nthe wind empower us to continue to study sustainable energy through the\nlens fo data science!\nAcknowledgements\nI would like to thank my collaborator on this project, Sydney Rilum from\nthe Bren School of Environmental Science and Management, for her\ncontributions. I will always remember the excessive amount of time we\nspent drawing wind turbine circles on scratch paper and adjusting the\nmathematical calculations to reflect different rotor diameters and how\nthat changes the wind power potential for the state of Iowa.\nThank you, Dr. James Frew and Niklas Griessbaum from the Bren\nSchool of Environmental Science and Management, for prompting us to\ninvestigate this question. Your expertise in spatial data science has\ninspired me to answer more questions like this in my future\ncareer.\nwind\nturtbine photo source\n\n\n\n",
    "preview": "posts/2021-12-25-sqlwindpower/pics/iowa_mask.png",
    "last_modified": "2022-05-01T11:33:26-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-24-sstsicpython/",
    "title": "Plotting Sea Surface Temperature and Sea Ice Concentration Over Time in Python",
    "description": "Using satellite data from the National Oceanic and Atmospheric Administration to detect global trends in sea surface temperature and sea ice concentration from 1981-2021.",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-12-24",
    "categories": [
      "Python"
    ],
    "contents": "\n\nContents\nDatasets:\nLoad necessary packages\nImport Sea Surface Temperature Data\nSea Surface Temperature Dataframe Wrangling\nPlot Sea Surface Temperature Over Time\nImport Sea Ice Data\nSea Ice Data Wrangling\nPlotting Sea Ice Data\nCombine the Visualizations\n\nGlobal sea surface temperature and sea ice concentration as raster images\n\nPlotting time series data from 2 different datasets can be tricky in Python. Here, we will plot sea surface temperature (SST) and sea ice concentration (SIC) in the same plot from the years 1981-2021 to visualize how climate change has impacted these environmental variables over time.\nDatasets:\n1. Sea Surface Tmnerpature: NOAA’s 1/4° Daily Optimum Interpolation Sea Surface Temperature (OISST) version 2, or OISSTv2\nData source and metadata: NOAA National Centers for Environmental Information\nNOAA Sea Surface Temperature Optimum Interpolation methods\n2. Sea Ice Concentration: Monthly Mean Hadley Centre Sea Ice and SST dataset version 1 (HadISST1)\nData source and metadata: UCAR/NCAR - DASH Repository\nJournal of Climate article explaining why the datasets were merged\nLoad necessary packages\nimport xarray as xr\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport glob\nImport Sea Surface Temperature Data\nI manually downloaded 40 years of datasets from the sea surface temperature data source, one day from each year (October 17th) and now I will import them with a for loop. Each dataset has a standardized name that only differs in the year it was collected, so the glob() function makes importing them all at once a piece of cake. We can use * as a wild card to represent any year in the file names.\nfilenames = sorted(glob.glob('Data/oisst-avhrr-v02r01*'))\n#Use a for loop to open all data files as datasets (prior to this step, data files were manually pulled from source and stored locally)\n#Take the means of the 'lat' and 'long' because we want to decrease the quantity of this extraneous data so we can easily convert these datasets to dataframes\n#Converting the datasets to dataframes within this for loop will allow us to concatenate them and view the combined data in a way we can easily understand\nnew = [xr.open_dataset(f).mean('lon').mean('lat').to_dataframe(dim_order=None) for f in filenames]\ndf = pd.concat(new)\ndf\n We need to organize the dataframe further before creating matplotlib visualizations for sea surface temperature over time. This dataframe defaulted to using time as an index, so we need to convert time to a column and subset the columns to just time and sea surface temperature.\nSea Surface Temperature Dataframe Wrangling\n#Subset the columns: we choose only 'sst' here because 'time' is considered the index (note: subsetting this dataframe turns the type into a pandas 'series')\ndf_subset = df['sst']\n#reset the 'time' from an index to a column\nsst_means = df_subset.reset_index()\n#subset to get rid of the useless variable \"zlev\"\nsst_means = sst_means[['time', 'sst']]\nsst_means\n\nEnsure that the type of the time variable is datetime64.\n As a last step in processing this dataframe, we will pull the year out of the time variable. We do this because the day and time is consistent for each sea surface temperature file, and therefore it is negligible. Furthermore, will process the sea ice concentration data to only include year, and we need the sea surface temperature time variable to be of the same format as the sea ice concentration time variable.\n# add a column of just the year, so when we pair the sst data with the ice data, the time variable will match\nsst_means['year'] = pd.DatetimeIndex(sst_means['time']).year\nsst_means\n\nPlot Sea Surface Temperature Over Time\nNow that we finished processing the sea surface temperature data, we can plot the sea surface temperature over our time period using matplotlib to see if can recognize a trend. It appears that sea surface temperature fluctuates over short time periods spanning a few years, but over the course of the full 40-year time period there is a clear positive correlation between time and temperature. This trend is what we would expect based on our knowledge of climate change and other sources of data that show rising sea temperatures across the globe.\nplt.figure(figsize=(10,7))\nplt.plot(sst_means['year'], sst_means['sst'], color='red')\nplt.xlabel('Year', fontsize=14)\nplt.ylabel('Sea Surface Temperature (degrees C)', fontsize=14)\nplt.title('Mean global Sea Surface Temperature in October between 1981 and 2020', fontsize=15)\n\nImport Sea Ice Data\nThe sea ice data import process is similar to my code for importing the sea surface temperature data. One of my collaborators on this project, Peter Menzies, wrote the following code to import the sea ice data.\n# read in ice dataset with xarray (prior to this step, data files were manually pulled from source and compiled into a single local netCDF file)\nds_ice = xr.open_dataset(\"Data/MODEL.ICE.HAD187001-198110.OI198111-202109.nc\")\nSea Ice Data Wrangling\n# create empty dataframe with columns year and ice_conc\nice_time = pd.DataFrame(columns=['year', 'ice_conc'])\n# create empty lists to populate with values from dataset\nice_year = []\nice_conc = []\nUse another for loop to pull sea ice values from October of each year 1981 to 2020, and compile them in a dataframe:\nfor i in range(0, 40):\n    # indexing the SEAICE attributes of our years of interest, finding the mean value, and outputting value as a float\n    # 1341 is the index for October 1981 - we need to add 12 in each iteration to get the month from the next year\n    ice_i = float(ice['SEAICE'][1341 + (12 * i)].mean())\n    **# storing the year associated with ice concentration value\n    year_i = 1981 + i\n    # add year to list\n    ice_year.append(year_i)\n    # add ice concentration to list\n    ice_conc.append(ice_i)\n\n# populated dataframe with the two lists as columns\nice_time['year'] = ice_year\nice_time['ice_conc'] = ice_conc\n# check out our resulting dataframe\nice_time\n\nPlotting Sea Ice Data\nplt.figure(figsize=(10,7))\nplt.plot(ice_time['year'], ice_time['ice_conc'])\nplt.xlabel('Year', fontsize=14)\nplt.ylabel('Seaice concentration (%)', fontsize=14)\nplt.title('Mean global seaice concentration in October between 1981 and 2020', fontsize=15)\n\nCombine the Visualizations\nUse matplotlib to combine the sea surface temperature and sea ice concentration data into one figure.\n# use the fig, ax method so that we have flexibility plotting each variable\nfig, ax = plt.subplots(figsize=(15,10))\n# assign a label to the ice data so we can reference it when we create the legend\nice, = ax.plot(ice_time['year'], ice_time['ice_conc'], color='blue')\n# ax represents the axis for the ice data\nax.tick_params(axis='y', labelcolor='blue')\nplt.xlabel('Year', fontsize = 18)\nax.set_ylabel('Sea Ice Concentration (%)', fontsize = 18)\n# ax2 represents the axis for the sea surface temperature data\nax2 = ax.twinx()\n# assign a label to the sst data so we can reference it when we create the legend\nsst, = ax2.plot(sst_means['year'], sst_means['sst'], color='red')\nax2.tick_params(axis='y', labelcolor='red')\nax2.set_ylabel('Sea Surface Temperature (degrees C)', fontsize = 18)\n# create an initial legend so that we can overwrite it with legend with both variables\nleg1 = ax.legend(loc='center left')\nleg2 = ax.legend([ice, sst],['ice','sst'], loc='center left', fontsize = 18)\nax.set_title('Sea Ice Concentration and Sea Surface Temperature 1981-2020', fontsize = 18)\nplt.show()\n\nThis plot and this code is just a portion of a larger environmental programming project completed by myself and my two collaborators Peter Menzies and Ryan Munnikhuis. See our complete repository here, which contains a binder in the README so anyone can run all the code with ease. Check out the anntoated code to learn how to turn these two datasets into rasterized images and GIF’s!\n\n\n\n",
    "preview": "posts/2021-12-24-sstsicpython/pics/combined_graph.png",
    "last_modified": "2022-01-15T11:54:47-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-25-fishingeffortbycountry/",
    "title": "Global Fishing Effort & Covid-19: A Statistical Analysis",
    "description": "A statistical approach to answer the question: How did Covid-19 impact global fishing efforts in 2020?",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-12-02",
    "categories": [
      "R",
      "Statistics"
    ],
    "contents": "\nA cod fish, one of the top species\ncaptured by marine fisheries (8)How\ndid global fishing activity change during the Covid-19 pandemic?\n2020 is a year the world will never forget. Covid-19 spread rapidly\nacross the globe and forced most of humanity into a state of quarantine.\nThe pandemic had clear devastating impacts on economies of all scales.\nOn the other hand, the pandemic boosted some sectors of the economy and\nincreased demand for certain goods. How did Covid-19 impact the global\nfishing economy? Did fisheries respond to the pandemic by sending\nfishermen and fisherwomen home to quarantine, and instead allocate\nresources towards public health initiatives? Alternatively, did some\ncountries see this as an opportunity to fish unregulated in the high\nseas more than ever before? There is very limited literature that\naddresses this question, which is likely due to the fact that 2020 was\nless than a year ago, and any formal scientific studies on this topic\nmight not have had time to be published. Pita\net al. 2021 assessed Covid-19’s impact on global marine recreational\nfisheries via a questionnaire, but this research differs from my\nstatistical analysis in that it did not use vessel data or\nquantitatively calculate how fishing hours differed over time, and the\nstudy only includes 16 countries (5).\nRegulating and documenting fishing activity and other vessel\nactivities across the globe is a major challenge (4). Databases often\nhave substantial gaps due to a lack of reliable data from automatic\nidentification systems and voluntary vessel registration. Global Fishing Watch is an\norganization that aims to revolutionize the way we monitor fishing\nactivity across the world using remote sensing techniques from\nsatellites combined with automatic identification systems. Global\nFishing Watch collects and visualizes global fishing data with the goal\nof embracing ocean sustainability, transparency, and open-source\nscience. They keep track of vessels from all different countries,\nincluding their movements, boat types, and time stamps for fishing and\ndocking at ports (9). Without such efforts to monitor, publicize, and\nregulate ocean activity, our marine resources are at high risk of\ndepletion. On a global scale, we are fishing faster than fish stocks can\nnaturally replenish. This has severe economic impacts; according to the\nWorld Bank Report, the ensuing depletion of marine fish stocks causes\neconomic losses of 50 billion US dollars annually (4). With modern data\nscience and applied statistics, we can better understand fishing\nactivity on a global scale and protect our planet’s marine\nbiodiversity.\nAs an aspiring wildlife biologist and data scientist, I’m interested\nin applying statistical analysis to Global Fishing\nWatch data data to learn how different countries’ fishing effort\nchanged in 2020, relative to those countries’ fishing trends in the\nyears leading up to 2020. In this dataset, fishing effort is defined by\nthe amount of hours spent fishing (3). I chose to use this dataset for\nmy statistical analysis because it is already relatively clean, and I\nknow the data is reliable because Global Fishing Watch is a highly\nrespected data resource with highly accurate remotely sensed data that\nis combined with locally collected automatic identification systems on\nshore. Furthermore, I am interested in working with Global Fishing Watch\nand spatial data in the future. This data does not have a spatial\ncomponent since country is a categorical variable, and the temporal\nvariable is limited to years. The only bias I believe might be present\nin this data is that it is limited to boats that either voluntarily\nallow their fishing hours to be tracked (such as through automatic\nidentification systems) as well as boats that have been detected\nremotely by satellite. With Global Fishing Watch’s expansive open-source\ndata collection, we can approach this question by grouping all vessels’\nfishing hours by country, identifying a statistical trend up until 2019,\nand extrapolating that trend into 2020. By comparing this 2020\nprediction to the actual fishing data available for 2020, we can glean\nhow Covid-19 skewed large-scale fishing efforts. I chose this analysis\napproach because I am familiar with many of these processes through my\ngraduate statistics course, and I believe it will be a simple and\naccurate way to derive a p-value that will reveal if there is a\nstatistically significant difference between each country’s actual mean\nfishing effort and their predicted mean fishing effort in 2020. Perhaps\nthe global fishing economy sky-rocketed, plummeted into near\nnonexistence, or remained unscathed by the pandemic. Quantitative\nanalysis will help provide some insight.\nGlobal Fishing Watch offers an interactive\nheat map that displays fishing activity across the globe. This\nvisualization has the potential to inspire data scientists, fish\nenthusiasts, environmental justice advocates, pandemic researchers, and\neveryone in between to examine fishing activity during a time period of\ninterest.\nGlobal fishing activity from January 1,\n2020 through January 1, 2021Global Fishing Watch and their partners also provide an interactive\nvessel map that allows users to interact with vessel activity across\nthe globe, filter by country, and overlay port locations on\ncoastlines.\n\n\n\n\n\n\nData Cleaning and Wrangling\nGlobal Fishing Watch’s data includes fishing effort and vessel\ninformation from 167 countries over the years 2012-2020 (3). First, we\nselect our variables of interest, group by country, and take the fishing\neffort means per year.\n\n\n\nOur goal is to run a linear regression on each country’s fishing\neffort over multiple years, but many countries have NA data\nfor certain years. Considering that we have data available for\n2012-2020, we can subset these years for the model. We want to minimize\nthe amount of NA values because we will drop all\nNA rows, and we want to maintain the maximum amount of rows\npossible (which represent vessels and countries). We want to select a\nchunk of continuous years leading up to 2020 with minimal data gaps. In\norder to choose the start year for the time period that we will feed\ninto the linear regression, take a look at the amount of NA\nvalues in the years leading up to 2020. It turns out that 2017, 2018,\nand 2019 have the least amount of NA values, so we will use\n2017 to start our 3-year data period to feed to the linear regression.\nNext, we convert the data into Tidy format using the\npivot_longer() function so we can run a time series linear\nregression analysis.\n\n\n\nOur dates are in years, and currently their class is\ncharacter from the original dataset. We need these years in\ndate format in order to run a linear regression over time.\nWe will convert these years and remove all countries that only have data\nfor only one or two years, because we need multiple years of data to\nfeed into the regression and we want each country to have equal amounts\nof data and start in the year 2017.\n\n\n\nLinear Regression\nNow that the data is sufficiently clean and our years are of class\ndate, we can run a time series linear regression on every\ncountry’s fishing effort from 2017-2019 and use the output coefficients\nto glean which direction each country is trending, meaning if the\ncountry is fishing more or less over time. We can do this with the\ndo() function, grouping by country. Then we can feed this\noutput into a for loop! Plug in each country’s fishing effort intercept\nand slope coefficients into a linear equation to predict the fishing\neffort in 2020 based on that country’s historical trend. Subsequently,\nwe can combine the predicted 2020 fishing effort data with the actual\n2020 fishing effort data into a single dataframe to compare by country.\nWe can make a new column that takes the difference of the actual and\npredicted values, and then add a column that explicitly states whether\nthat country increased or decreased their fishing effort in 2020\nrelative to their trend leading up to 2020.\n\n\n\nPlotting\nActual Fishing Effort versus Predicted Fishing Effort for Malaysia\nWhat does a single country’s fishing trend look like? Let’s consider\nthe country of Malaysia in Southeast Asia. In 2015, Malaysia’s fisheries\nsector employed 175,980 people and its contribution to national gross\ndomestic product was 1.1%. The fish trade is valued at $1.7 billion\n(U.S. dollars), and the estimated average consumption of fish is 56.8\nkg/person/year. Malaysian fisheries primarily capture shrimp, squid, and\nfish. Malaysia contributes to the global fish economy through both\nimporting and exporting fish (6).\nWe can make a country-specific fishing effort plot by filtering our\ncombined actual and predicted fishing effort dataframe to just\nthat country and using ggplot().\n\n\n\nMalaysia’s Fishing Effort: Actual vs\nPredicted 2017-2020Statistical Significance\nIt’s time to run a t-test to determine if there is a statistical\ndifference between the countries’ predicted fishing effort in 2020 and\ntheir actual fishing effort in 2020. A t-test is a handy tool in\nstatistics that reveals how significant the differences between groups\nare. If the difference between the means of two groups could have easily\nhappened by chance, the p-value will be greater than 0.05 (which is the\nstandard threshold in statistics and environmental data science). If it\nis highly unlikely (less than a 5% chance) that a difference in means at\nleast this extreme could have occurred by chance, the p-value is less\nthan 0.05 and the results are considered statistically significant. A\nstatistically significant outcome allows us to reject our null\nhypothesis.\nNull Hypothesis: There is no difference between the\npredicted country-specific predicted fishing effort in 2020 and the\nactual country-specific fishing effort in 2020. \\[H_{0}: \\mu_{predicted} - \\mu_{actual} =\n0\\] Alternative Hypothesis: There is a\ndifference between the predicted country-specific predicted fishing\neffort in 2020 and the actual country-specific fishing effort in 2020.\nBecause of the pandemic in 2020, I predict that fishing effort\ndecreased, meaning that the actual country-specific fishing effort is\nless than the predicted country-specific fishing effort. \\[H_{A}: \\mu_{predicted} - \\mu_{actual} \\neq\n0\\]\nDon’t forget to convert the data to Tidy format so\nwe can run the t-test!\n\n\n\nt-test\noutputThe p-value is 0.0000000312, and 0.0000000312 < 0.05, so we can\nreject our null hypothesis that there is no difference between the\npredicted country-specific predicted fishing effort in 2020 and the\nactual country-specific fishing effort in 2020. Many countries clearly\nchanged their fishing effort in 2020 relative to their historical\ntrend!\nSummary:\nWhich countries increased their fishing effort during the pandemic,\nrelative to their trend leading up to 2020?\nTo best visualize this fishing effort data in a table, we can color\ncode the countries that increased their fishing effort\nas red and color the countries that decreased their\nfishing effort in green. We can represent this data both on a map as\nwell as a table.\n\n\n\n\n\n\n\n\n\n\n\nDifferences between actual 2020 fishing\neffort and predicted 2020 fishing effort by country\nThis color-coded table reveals that 85% of the countries included in\nthis analysis decreased their fishing effort during the\nCovid-19 pandemic in 2020 relative to their fishing trend leading up to\n2020, while 15% of the countries included in this analysis\nincreased their fishing effort. The vast majority of countries’\nfishing sectors seemed to follow the same stay-at-home order that was\nenforced across the globe. While this may have had a detrimental impact\non the global fish economy, hopefully the marine fish populations we\nable to recover and thrive during this period of reprieve from human\npredation. The results of my statistical analysis match the conclusion\nof a 2021 scientific study investigating the change in marine\nrecreational fishing activity during the first year of the pandemic\n(5).\nFuture Steps\nIn order to improve this analysis in the future, I recommend using\nmore than three years of fishing effort data to produce a more accurate\nlinear model. Additionally, I would recommend using a different\nstatistical approach instead of iterating a for loop over each country’s\nfishing effort data, because this method produced outliers and\ncoefficients that did not line up with observed fishing effort data.\nLastly, I recommend running this analysis on fishing effort data from\nother sources in addition to Global Fishing Watch’s data. This will\nprovide certainty that the data is accurate and the results are\nreproducible.\nThank you for reading my statistical review of global fishing effort\nduring the 2020 Covid-19 pandemic! I hope I have inspired you to run\nyour own time series analysis, t-tests, and create visualizations that\nhelp communicate trends in environmental data science. Please feel free\nto contact me at jscohen@ucsb.edu with any questions,\ncomments, or suggestions. You may also create issues or pull requests\nfor this analysis through GitHub (repository linked below).\nData Availability\nThe data used in this analysis is openly available, but the user must\nmake a free account on the Global Fishing watch website, which can be\naccessed through this live link:Global\nFishing Watch Datasets and Code\nGitHub Repository Live Link\njulietcohen’s\nGitHub Repository\nAcknowledgements:\nI would like to acknowledge Dr. Tamma Carleton, my professor in\nStatistics for Environmental Data Science at the U.C. Santa Barbara Bren\nSchool for Environmental Science and Management, for all her support\nthroughout this project and this quarter.\nI would also like to thank my peers in the Master of Environmental\nData Science Program for being so open to collaboration and supporting\neach other with resources, programming tools, and open-source\nscience.\nLastly, I would like to thank Global Fishing Watch for inspiring me\nto give a hoot about global fishing effort by country, and for providing\nthe data that made this project possible.\nResources (live links):\nInteractive\nWorld Map of Fishing Activity: Picture 1 and Map Link 1 - Global Fishing\nWatch\nInteractive\nWorld Map of Fishing by Country: Map Link 2 - Global Fishing\nWatch\nGlobal\nFishing Watch: Datasets and Code, Fishing effort data\nnote: Users must make a free account in order to access datasets\nGlobal\nFishing Effort (1950–2010): Trends, Gaps, and Implications.\nAnticamara, J. A., R. Watson, A. Gelchu, and D. Pauly. “Global Fishing\nEffort (1950–2010): Trends, Gaps, and Implications.” Fisheries Research\n107, no. 1 (2011): 131–36. https://doi.org/10.1016/j.fishres.2010.10.016.\\\nFirst\nAssessment of the Impacts of the COVID-19 Pandemic on Global Marine\nRecreational Fisheries.:\nPita, Pablo, Gillian B. Ainsworth, Bernardino Alba, Antônio B. Anderson,\nManel Antelo, Josep Alós, Iñaki Artetxe, et al. “First Assessment of the\nImpacts of the COVID-19 Pandemic on Global Marine Recreational\nFisheries.” Frontiers in Marine Science 8 (2021): 1533. https://doi.org/10.3389/fmars.2021.735741.\\\nMalaysia’s\nFisheries Economy\nGoogle\nMaps: Malaysia\nWikipedia:\nTop Marine Fisheries Species Captured\nGlobal Fishing\nWatch - About Us\nDistill is a publication format for scientific and technical writing,\nnative to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": "posts/2021-11-25-fishingeffortbycountry/pictures/fishing_gfw_map_2020.png",
    "last_modified": "2022-05-03T22:05:00-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-01-plotting-shapefiles-in-python/",
    "title": "Plotting Shapefiles on a Basemap in Python: endangered species habitat",
    "description": "The basics for reading in shapefile data, plotting it, and adding a basemap.",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-11-05",
    "categories": [
      "Python",
      "Spatial Analysis"
    ],
    "contents": "\n\nContents\nGetting\nstarted with Python and Jupyter Notebooks\nImporting Packages\n(modules)\nImporting Data\nSetting the\nCoordinate Reference System\nPlotting Shapefiles on a\nBasemap\nFuture Analysis\nA Local California\nUse Case Example\nSources\n& supplemental information on listed endangered species in the\nUnited States:\n\n\n\n\n\nPython is a powerful programming language for spatial analyses. In\nthis tutorial, we learn the basics of plotting shapefiles overlaid on\ntop of a basemap, which gives us spatial context and opens doors for\ndeeper analysis. Understanding where points, lines, and polygons exist\nin the world empowers data scientists to answer questions about\ncorrelated variables, consider new questions that arise throughout the\nanalysis process, and communicate conclusions to audiences in an\neasily-digestible way.\nWe start with the basics; installing the integrated development\nenvironment (IDE) Jupyter Notebook and installing the necessary\npackages. If you prefer to use another IDE such as Visual Studio Code\nand have already installed the packages geopandas, feel\nfree to jump ahead to the Importing Packages\nsection.\nAs a conservation biologist and an aspiring data scientist, I’m\ninterested in plotting endangered species’ critical habitat ranges\nthroughout the United States. I sourced my data from the U.S. Geological\nSurvey here.\nI’m a big fan of USGS datasets because they are usually relatively clean\nand provide comprehensive metadata.\nLet’s map the critical habitat ranges of the Florida Bonneted bat and\nmany other species of concern on a basemap of the United States.\n\nThe Florida bonneted bat, a critically\nendangered species that has been assigned critical habitat by U.S. Fish\nand Wildlife. Hopefully, the critical habita is suffificient to help\nthis species recover and reach sustainable population levels. The world\nwould be a better place with more bonneted bats.\nGetting\nstarted with Python and Jupyter Notebooks\nTransitioning from R and Rstudio to\nPython and Jupyter can be difficult, as is\nlearning any new language or IDE. We have to say goodbye to the\nexceptionally user-friendly RStudio interface with its\nconvenient graphical user interface and easily accessible console and\nterminal windows. Let’s dive into the objected-oriented language of\nPython and the mysterious terminal. Personally, I struggled every step\nof this journey with lots of syntax errors, merge conflicts, and endless\nGoogling I hope this post will help hold your hand as we make this\ntransition together.\nFirst things first, if you’re using Jupyter Notebooks to code in\nPython, you’re gonna have to install Python and Anaconda Navigator.\nInstall Anaconda into your home directory. (Note: there are\nplenty of other IDE’s for Python, such as visual studio code, which I\nadopted later in my data science journey).\nAs I had a PC when I first started coding in Python, installing\nAnaconda really threw me and my laptop for a loop. But what\nbetter way to learn to use the terminal besides struggling to install\nstuff you desperately need for graduate school? Errors and\ntrouble-shooting is what makes us stronger data scientists, even if we\ndon’t want to recognize that as we fight the urge to throw our computer\nout the window. Click here\nfor a great resource to walk you through installing\nAnaconda. Either Anaconda or\nMiniconda will work fine whether you have a PC or Mac.\nAfter installing Anaconda, start your journey plotting shapefiles in\nPython by opening up Jupyter Notebooks. On a Mac, I can\nactivate conda in the regular terminal\n(conda activate). On a PC, I like to do this from the\nAnaconda Prompt (the Aanconda terminal) because it’s tricky\nto get my normal command line or Bash to recognize that\nconda is indeed on my computer. My favorite workflow is as\nfollows:\nFrom the start menu, open a terminal through Anaconda Navigator\nthat’s called “Anaconda Prompt”.\nInstall geopandas with\nconda install geopandas in your base environment (which is\nthe default). If that doesn’t work (which would not surprise me if\nyou’re on a PC), create a new environment to do so. I found the steps on\nthis website to be helpful: https://medium.com/analytics-vidhya/fastest-way-to-install-geopandas-in-jupyter-notebook-on-windows-8f734e11fa2b.\nI tried installing geopandas in my base environment, but it\nwas difficult to install all the correct versions of all the\ndependencies, so I decided to take the easy route and just make a new\nenvironment for geopandas and any other spatial analysis\npackages I’ll need. - Activate the environment in which you have\ninstalled the geopandas package. I named that environment\ngeo_env, so I type activate geo_env.\nNow that I am in my desired environment, I am going to navigate to\nthe folder in my terminal in which I want to open and save my Jupyter\nNotebook. That command is cd file/path/to/folder. You know\nthis worked if your terminal working directory now has that file path\ntacked onto the end. This file path step is not required if you\nwant to include relative file paths to import data and save\nyour notebook.\n\nDownload your spatial data files to this folder to make your life\neasier in 2 minutes when you import your spatial data in Jupyter\nNotebook.\nOpen Jupyter Notebooks by typing just that:\njupyter notebook. This will tell your terminal to pop open\nJupyter Notebook in your browser with your folder of choice already open\nand ready to go.\nIn the upper right side, open a new notebook.\n\nNote: the Anaconda terminal window you used to open this notebook\nshould not be closed during your work session. It must remain open to\nkeep your kernel connected and give you the ability to save! If you need\nto run any terminal commands after you have already opened this\nnotebook, such as if you need to download a package or check a file\nlocation, just open up another terminal window and enter the\ngeo_env environment to do so.\nImporting Packages\n(modules)\nFor plotting shapefiles, you’ll want to import the following\npackages:\nimport pandas as pd  \nimport numpy as np  \nimport geopandas as gpd  \nimport matplotlib.pyplot as plt # plot our data and manipulate the plot\nimport contextily as ctx # add a default basemap under our polygons of interest\nIf you do not already have contextily installed, use a\nconda forge command to install it in the terminal in your\ngeospatial environment.\nAs a proponent of reproducibility and crediting those who provided\nthe data, I like to include a markdown chunk following my package\nimports that includes a URL link to where I found my data, along with a\ncitation if necessary and any notes about how I downloaded it for myself\nor anyone else working through my code:\nData source: US\nFish and Wildlife  - contains .shp, .dbf, and .shx polygon files\nthat relate to critical species habiata in the United States\n- I chose the first zip file you see on this site\n- USFW provided great metadata \nImporting Data\nLet’s import your data! Now is the time you’re gonna thank yourself\nfor placing your data in the same folder as your Jupyter notebook. We\nwill use geopandas to read in the\nshapefile with your polygons or lines or points of\nchoice (you will not find a combination of these shapes in the same\nshapefile, because that’s just how the world works). You might take a\nlook at all the data files and feel a little overwhelmed at the choices\ndue to the way that shapefiles and their metadata are stored separately\n(.shp, .dbf, .shx, .xml, and so on). In this example we are trying to\nimport a shapefile of polygons, so that\n.shp file is the only one you need to read in:\ngdf = gpd.read_file('CRITHAB_POLY.shp')\n# Take a look at the first rows:\nprint(gdf.head())  \n# Ask Python how many rows and columns are in this dataframe\nprint(gdf.shape)\nMy only complaint with the head() function is that it\nreturns the first rows in a plain text format:\n\nIf you want to see the first and last few rows of the dataframe in a\nformat that looks more familiar (like how Rstudio presents dataframes),\ntry just typing the name of the data frame, gdf:\n\nThis shapefile I read in contains polygons that designate the\ncritical habitat ranges for many endangered and threatened species in\nthe United States. I chose to name it gdf for\ngeodataframe. Expect that you will be modifying this dataframe\nas you go through this mapping process (subsetting columns, filtering\nfor certain factor levels, etc.) so you will likely be tacking on more\nwords to gdf to tell these modified versions apart. Start\nnaming things simply and clearly, and get more specific as you process\nyour data.\nPractice cimple commans like calling all the column names in the\ndataframe:\nprint(gdf.columns)\nCheck the factor levels for the columns listing_st:\nstatus_levels = gdf.listing_st.unique()\nstatus_levels\nUsing U.S. Fish and Wildlife as an example, now that you know the\nfactor levels of a categorical variable, you can subset for only\n“endangered” species, only “threatened” species,\netc.\n\nPlay around with your dataframe a bit; Google some of the species,\nsubset the columns, search for NA values, or take the\naverage of a column. After you make a structural change, its a good\nhabit to check the status or dimensions of your dataframe.\nCheck the number of rows and columns: \nPrint the latitude and longitude pairs that make up a particular\npolygon: \nSetting the Coordinate\nReference System\nAs a last step before you plot, you have to make sure you set the\ndata to the desired coordinate reference system (CRS). This is pretty\nstandard for all kinds of spatial data, since your data might come with\nan unfamiliar CRS or have no CRS at all if you are making a mask, a\nraster, or executing similar geospatial processes. For information about\ncoordinate reference systems, check out this\nguide.\nThree common CRS’s are as follows:\nWGS84 (EPSG: 4326), which is commonly used for GIS\ndata across the globe or across multiple countries\nand\nNAD83 (EPSG:4269), which is most commonly used for\nfederal agencies\nand\nMercator (EPSG: 3857), which is used for tiles from\nGoogle Maps, Open Street Maps, and Stamen Maps. I will use this one\ntoday because I want to overlay my polygons onto a Google basemap with\ncontextily.\nSet the CRS to ESPG 3857. This code may take a minute to run. In\nJupyter Notebook, you know that code chunk is still chuggin’ away if you\nsee an asterisk in brackets to the left of the code chunk:\n\nPlotting Shapefiles on a\nBasemap\nUse the plot() function from matplotlib and make the polygon color\nrepresent each species:\ngdf_3857.plot(column='comname',\n             figsize=(20,20))\n\nNote that you did not have to call the package to use the function\nplot(). Instead, you can name the dataframe which you want\nto plot, which is gdf_3857 in this case, then\nplot() and add arguments and supplemental plot structure\nchanges as you go.\nThe fig size can be whatever you want. 10-20 is usually good enough.\nYou have finer control over the degree of zoom of the map with the\narguments xlim() and ylim(), anyway. These\npolygons are just floating in space, so lets add a basemap to give us\ngeographical context:\ncrit_hab_polys = gdf_3857.plot(column='comname',\n             figsize=(20,20))\nNotice that I used an argument in the plot function, setting the\ncolumn = 'comname', which is a column within the\ngdf_3857 geodataframe that specifies the common name for\nthe species in that row. This argument sets a unique color to each\ncommon name, which will help us tell the difference between each\nspecies’ habitat on the map, even if 1 species’ habitat is composed of\nmultiple polygons.\nctx.add_basemap(crit_hab_polys)\n\n# Set the axis limits to zoom in on just the lower 48 states, rather than viewing the entire world:  \nplt.ylim([2350000,6350000])\nSince the basemap within the contextily package is of\nthe entire world, we need to specify the x-limitations and y-limitations\nfor our map so we can zoom in on the United States to best understand\nour data. The default x and y units were in the millions, so I specified\nmy units in millions, too. When considering if I should plug in positive\nor negative values, I considered the way that coordinate reference\nsystems are designed with positive values for North and East, and\nnegative values for South and West. I considered that the United States\nis north of the equator, so I should have positive values for the min\nand max y. As for the magnitude of my values, I simply looked at the map\nfor a starting point and played around with different numbers until I\ngot the view I wanted.\nplt.xlim([-14000000,-6000000])\nNotice that these values are negative. Along similar thinking to how\nwe decided on our y limitation, these negative values are the result of\nhow coordinate systems are designed. Consider the prime meridian (which\nlies at 0 degrees in the East and West directions) with West being\nnegative. Since the United States are to the West of the prime meridian,\nwe know that the x-range for our zoom should be negative. As for the\nmagnitude, I just played around with the numbers until I got the\nEast-West orientation that encompassed the United States. Use the\nshow() function in matplotlib to tell Python\nto show the most recently created graph:plt.show()\n\nYou did it! Welcome to the wonderful world of geospatial data in\nPython.\nFuture Analysis\nWith this basic skill mastered, you can now dive deeper into this\ndata to determine if variables are correlated across space. Considering\nstate borders, you might ask which endangered species occupy\nciritical habitat in your home state? and investigate the\ndifferent wildlife protection policies across the United States.\nAlternatively, you could approach this data from an environmental\nperspective and ask which natural biomes contain the most\ncritical habitat for these endangered species? Are these habitats\ndegrading at a faster rate than those that contain less critical\nhabitat?\nA Local California Use Case\nExample\nLiving in Santa Barbara, I’m interested in the critical habitat of\nthe Southern\nCalifornia Steelhead Trout. Steelhead trout is a beautiful fish\nspecies that interbreeds with resident rainbow trout in many coastal\nregions throughout California, which are split into distinct population\nsegments:\nSouthern California Steelhead Trout\ndistinct population segment map with watershed landmarksI was lucky enough to conduct field work with these migratory fish in\n2020-2021 through the California Department of Fish and Wildlife and the\nPacific States Marine Fisheries Committee. I grew to appreciate their\nvital role in ecosystem processes and the culture of indigenous people\nwho have interacted with them for centuries. This fish is currently in\nthe process of being listed as a California endangered species starting\nin December of 2021, which will hopefully expand critical habitat range,\nincrease monitoring of the populations, and help enforce illegal fishing\nand pollution regulation in their habitat. We can check out the\nsteelhead’s critical habitat range before and after 2021 to see how it\nexpands over time and space.\n\nSouthern California Steelhead Trout with\nan invasive crayfish in a Southern California freshwater\nstream\nSources\n& supplemental information on listed endangered species in the\nUnited States:\nFlorida\nbonneted bat photo\nU.S.\nFish and Wildlife data\nU.S. Fish and Wildlife’s\nEnvironmental Conservation Online system\nU.S.\nFish and Wildlife listed wildlife species\nthis includes links for data on each each species and a map of their\nhabitat\nSouthern\nCalifornia Steelhead Trout\nSouthern\nCalifornia Steelhead Trout distinct population segement map\nSouthern\nCalifornia Steelhead Trout\nPython\nlogo\n\n\n\n",
    "preview": "posts/2021-11-01-plotting-shapefiles-in-python/images/crit_hab_with_basemap.png",
    "last_modified": "2022-05-14T10:37:53-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-05-sstchlorowind/",
    "title": "Using an API to visualize environmental conditions in the Santa Barbara Channel",
    "description": "Interpretting marine processes by combining data sets of sea surface temperature, chlorophyll, and wind.",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-11-05",
    "categories": [
      "R",
      "Spatial Analysis"
    ],
    "contents": "\nThe Santa Barbara Channel\nA basin of\noceanic data just waiting to be analyzed\nThe beautiful, biodiverse Santa Barbara coast is known for its\npleasant mediterranean climate, gnarly swell, and unique geographic\nfeatures. Among these are the four northern channel islands, Santa Cruz,\nAnacapa, Santa Rosa, and San Miguel, which reside between 12 and 27\nmiles off shore. The Santa Barbara Channel lies between these islands\nand the coastline, stretching from Los Angeles in the south to Pt.\nConception in the north.\nOcean Currents\nThis channel hosts a clash of different ocean currents that causes\nheterogeneity in environmental conditions across the islands and\ncirculates nutrients throughout the ocean depths in seasonal\npatterns.\nSanta Barbara Channel currents and all\neight channel islands (from west to east): San Miguel Island, Santa Rosa\nIsland, Santa Cruz Island, Anacapa Island, San Nicolas Island, Santa\nBarbara Island, Santa Catalina Island, San Clemente Island\n(1).The California current brings a cold swell from the Gulf of Alaska\ndown the coast, providing ideal temporal conditions for black rockfish,\nsunflower sea otters, red abalone, and other creatures around San Miguel\nand Santa Rosa Islands. In contrast to the southeast-bound California\nCurrent is the northwest-bound Southern California Counter-current from\nBaja California. This warmer and relatively nutrient-poor water supports\ndifferent marine species such as spiny lobsters, moray eels, and\ndamselfish such as California’s state fish: the Garibaldi. These species\nare more commonly found near the southeast islands (1:\nChannel Islands National Park, 2:\nNational Park Service).\n\nA garibaldi fish (Hypsypops\nrubicundus), California’s state fish and a species found in the\nSanta Barbara Channel. This fish is protected from fishing, lives in\nkelp forest habitat, and males utilize red algae to build nests\n(2)\nMarine Wildlife\nThis clashing current rendezvous turns the Santa Barbara Channel into\na hotspot for biodiversity for marine mammals like dolphins and whales,\nbenthic invertebrates like purple urchins, plants such as giant kelp,\nand charismatic fish species such as the sunfish (the most majestic\nmarine beast known to man).\n\nA sunfish (Mola mola), the\nworld’s largest bony fish, are found throughout the Santa Barbara\nChannel and are so large (up to 11 feet in length) can even be spotted\nby Channel Islands field crew members communting the to islands by\nhelicopter. These atypical fish can even be found as far north as Alaska\nduring El Nino years. These fish often bask nearly motionless near the\nocean surface and sometimes breach the surface in an apparent attempt to\nrid their bodies of external parasites (3).\nFrom late November through April, whale sightings are quite common in\nSanta Barbara. Thousands of Pacific gray whales migrate south towards\nthe warm waters of Baja California and feed on krill in the channel\nalong the way, which are tiny organisms that thrive on oceanic\nchlorophyll blooms (4). Modern remote-sensing techniques can detect\nchlorophyll and sea surface temperature via satellites. In order to\nstrategically determine the best time of year to spot these whales, we\nmight consider the timing of these phytoplankton blooms.\nDo\nthese blooms occur more often when we have warmer ocean temperatures?\nWhat time of year would that be, and does it align with the famous\n“whale season” known to be November through April?\nIs the ocean\ntemperature impacted by wind?\nA few data-driven friends and I decided to combine data about wind,\nsea surface temperature, and chlorophyll in the Santa Barbara Channel to\nfind the best time of year to go whale watching. My collaborators\ninclude Grace Lewin, Jake Eisaguirre, and Connor Flynnfrom the\nEnvironmental Data Science program at the Bren School of Environmental\nScience and Management.\nPrimary\nQuestion: How did wind speed affect sea surface temperature and\nchlorophyll in the Santa Barbara Channel during 2020?\nThe\nNational Oceanic Atmospheric Administration (NOAA)\nMethods\nThe National Oceanic Atmospheric Administration has the perfect\ndatasets to help us out, and they even have a handy application\nprogramming interface (API) to do the heavy lifting for us. The NOAA\nAquamodis Satellite data can be found here.\nThe REDDAP API will import sea surface temperature and chlorophyll\ndata directly from the NOAA Aquamodis Satellite. To complement this\ndata, we manually pulled wind speed data from NOAA’s East\nBuoy, West\nBuoy, and the Santa\nMonica Buoy by downloading and decompressing the 2020 Standard\nMeteorological Data Files.\nStart by loading the necessary packages for downloading the data and\npreparing it for analysis:\n\n\nlibrary(rerddap)\n# used to load in the data from NOAA's website\nlibrary(tidyverse)\n# used to clean and visualize data\nlibrary(here)\n# used to read in the data from the current R project\nlibrary(lubridate)\n# use lubridate to work with datetimes (parsing dates and changing the class)\n\n\n\nUse the rerddap API to read in the sea surface and\nchlorophyll data from NOAA. Assign the respective temperature and\nchlorophyll data to its respective buoy, then bind the Tidy\ndata together into one dataframe using rbind().\n\n\n# Read in Aqua Modis Data from their website\nrequire(\"rerddap\")\n\n# Sea Surface Temperature for each Buoy\nE_sst <- griddap('erdMWsstd8day_LonPM180', # 8 day composite SST E_buoy\n time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\n latitude = c(34.0, 34.5), #grid surrounding buoy\n longitude = c(-119.5, -120), #grid surrounding buoy\n fmt = \"csv\")  %>% \n  add_column(location = \"east\") #add ID column\n\nW_sst <- griddap('erdMWsstd8day_LonPM180', # 8 day composite SST W_buoy\n time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\n latitude = c(34.0, 34.5), #grid surrounding buoy\n longitude = c(-120, -120.5), #grid surrounding buoy\n fmt = \"csv\") %>% \n  add_column(location = \"west\") #add ID column\n\nSM_sst <- griddap('erdMWsstd8day_LonPM180', # 8 day composite SST SM_buoy\n time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\n latitude = c(33.5, 34.0), #grid surrounding buoy\n longitude = c(-118.75, -119.25), #grid surrounding buoy\n fmt = \"csv\") %>%\n  add_column(location = \"SM\") #add ID column\n\nsst <- rbind(E_sst, W_sst, SM_sst) #bind data\n\n\n\nNow for chlorophyll:\n\n\n# Chloro for each Buoy\nE_chloro <- griddap('erdMWchla8day_LonPM180',  # 8 day composite Chlorophyll E_buoy\n  time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\n  latitude = c(34.0, 34.5), #grid surrounding buoy\n  longitude = c(-119.5, -120), #grid surrounding buoy\n  fmt = \"csv\") %>% \n  add_column(location = \"east\") #add location term\n\nW_chloro <- griddap('erdMWchla8day_LonPM180', # 8 day composite Chlorophyll E_buoy\n  time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\n  latitude = c(34.0, 34.5), #grid surrounding buoy\n  longitude = c(-120, -120.5), #grid surrounding buoy\n  fmt = \"csv\") %>% \n  add_column(location = \"west\") #add location term\n\nSM_chloro <- griddap('erdMWchla8day_LonPM180', # 8 day composite Chlorophyll SM_buoy\n  time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\n  latitude = c(33.5, 34.0), #grid surrounding buoy\n  longitude = c(-118.75, -119.25), #grid surrounding buoy\n  fmt = \"csv\")%>% \n  add_column(location = \"SM\") #add location term\n\nchloro <- rbind(E_chloro, W_chloro, SM_chloro) #Bind data\n\n\n\nWe downlaoded the wind data manually from NOAA’s\nwebsite, so we import it from the local Rproj, using\nthe here() function. This file path will work on your local\nmachine, helping make this log post as reproducible as possible.\n\n\n# Wind data for each buoy and data cleaning\ntab_E <- read.table(here(\"data\",\"east_wind.txt\"), comment=\"\", header=TRUE) #convert .txt file to .csv\nwrite.csv(tab_E, \"east_wind.csv\", row.names=F, quote=F)\n\nE_wind <- read.csv(here(\"east_wind.csv\")) %>% # read in .csv, select columns and rename\n  add_column(location = \"east\") %>% \n  select(c(\"X.YY\", \"MM\", \"DD\", \"WSPD\", \"location\"))  %>% \n  rename(year = X.YY,\n         month = MM,\n         day = DD)\nE_wind <- E_wind[-c(1),]\n  \n\ntab_W <- read.table(here(\"data\",\"west_wind.txt\"), comment=\"\", header=TRUE) #convert .txt file to .csv\nwrite.csv(tab_W, \"west_wind.csv\", row.names=F, quote=F)\n\nW_wind <- read.csv(here(\"west_wind.csv\"))%>% # read in .csv, select coloumns and rename\n  add_column(location = \"west\") %>% \n  select(c(\"X.YY\", \"MM\", \"DD\", \"WSPD\", \"location\"))  %>% \n  rename(year = X.YY,\n         month = MM,\n         day = DD)\nW_wind <- W_wind[-c(1),]\n\n\ntab_SM <- read.table(here(\"data\",\"SM_wind.txt\"), comment=\"\", header=TRUE) #convert .txt file to .csv\nwrite.csv(tab_SM, \"SM_wind.csv\", row.names=F, quote=F)\n\nSM_wind <- read.csv(here(\"SM_wind.csv\"))%>% # read in .csv, select coloumns and rename\n  add_column(location = \"SM\") %>% \n  select(c(\"X.YY\", \"MM\", \"DD\", \"WSPD\", \"location\"))  %>% \n  rename(year = X.YY,\n         month = MM,\n         day = DD)\nSM_wind <- SM_wind[-c(1),]\n\nwind <- rbind(E_wind, W_wind, SM_wind) #bind data\n\n\n\nMy team averaged the wind by month rather than by day because the\nwind varies more each day by a large margin. Therefore, the wind data on\na daily basis shows lots of noise and no interpretable trends. On a\nmonthly scale, however, we can make sense of its broader fluctuations\nover the year.\n\n\n# clean date format and summarize with daily means for wind\nwind <- wind %>%\n  unite(\"date\", year:month:day, sep = \"-\") %>% \n  mutate(date = ymd(date, tz = NULL)) %>% \n  mutate(WSPD = as.numeric(WSPD))\n\n# see the data join chunk for na.rm explanation in code comment\nwind_avg <- wind %>% \n  group_by(location, date) %>% \n  summarize(mean_wind = mean(WSPD, na.rm = T))\n\n\n\nHere we cleaned the remotely-sensed sea surface temperature data,\nsummarizing it by day:\n\n\n# clean data for sst date\nsst_clean <- sst %>% \n  mutate(date = ymd_hms(time, tz = \"UTC\")) %>% \n  mutate(ymd_date = ymd(date, tz = NULL)) %>% \n  mutate(date = ymd_date) %>% \n  select(c(\"latitude\", \"longitude\", \"sst\", \"location\", \"date\"))\n\n# Clean sst Data and summarize by daily means\nfinal_sst <- sst_clean %>% \n  filter(sst > 0) %>% # remove NAs\n  mutate(sst = (sst * (9/5) + 32 )) %>% # convert to F...there's probably a function for this\n  mutate(sst = (sst - 3)) # accounting for SST satellite error through anecdotal and buoy comparison.\n# A team member's field experience justifies this as he has consistently cross-referenced the satellite data with in situ measurements \n\n# see the data join chunk for na.rm explanation in code comment\nfinal_sst_avg <- final_sst %>% \n  group_by(location, date) %>% \n  summarize(mean_sst = mean(sst, na.rm = T))\n\n\n\nHere we cleaned the remotely-sensed chlorophyll data, summarizing it\nby day:\n\n\n# clean chloro data\n# see the data join chunk for na.rm explanation in code comment\nchloro_clean <- chloro %>% \n  mutate(date = ymd_hms(time, tz = \"UTC\")) %>%  # never forget to check the timezone!\n  mutate(ymd_date = ymd(date, tz = NULL)) %>% \n  mutate(date = ymd_date) %>% \n  select(c(\"latitude\", \"longitude\", \"chlorophyll\", \"location\", \"date\"))\n\nfinal_chloro_avg <- chloro_clean %>% \n  group_by(location, date) %>%\n  summarize(mean_chloro = mean(chlorophyll, na.rm = T))\n\n\n\nWe used inner_join() in two steps to combine the cleaned\ndata from the three variables into one dataframe:\n\n\n# combine daily wind and sst and chloro means\n# we decided to use inner join in order to not include any rows that lack values for ANY of the 3 variables.\n# We do not want any NA values in one col and have data in another col, because when we map everything together\n# that data would be represented as if there was a zero value for the variable that had NA. \n# his change reduced the amount of rows by a couple hundred. This was primarily in the SST and cholorophyll data\n# which had plenty of NA's but the wind data did not initially have NA's.\n\nwind_sst <- inner_join(wind_avg, final_sst_avg, by = c(\"date\", \"location\"))\n\nchloro_wind_sst <- inner_join(wind_sst, final_chloro_avg, by = c(\"date\", \"location\"))\n\n\n\nNow the fun part: visualization! My team and I made three plots, one\nfor each variable. Each plot represents data from all three buoys. We\nmarked the sea surface temperature maximum in all plots since the\ncombined data reveals a probable temporal correlation between sea\nsurface temperature and wind.\n\n\n# Daily Average Sea Surface Temperature from East, West, and Santa Monica Buoys\nggplot(data = chloro_wind_sst, aes(x = date, y = mean_sst, color = location)) +\n  geom_line() +\n  labs(x = \"Date\",\n       y = \"Daily Average Sea Surface Temperature (degC)\",\n       title = \"Daily Average Sea Surface Temperature from East, West, and Santa Monica Buoys\",\n       color = \"Location\")\n\n# Monthly Average Wind from East, West, and Santa Monica Buoys\nmonth_mean <- chloro_wind_sst %>%\n  select(location, date, mean_wind) %>%\n  mutate(month = month(date, label = TRUE)) %>%\n  mutate(month = as.numeric(month)) %>% \n  group_by(location, month) %>%\n  summarize(mean_wind = mean(mean_wind, na.rm = T)) \n\nggplot(data = month_mean, aes(x = month, y = mean_wind, color = location)) +\n  geom_line() +\n  labs(x = \"Month\",\n       y = \"Monthly Average Wind Speed (knots)\",\n       title = \"Monthly Average Wind Speeds from East, West, and Santa Monica Buoys\",\n       color = \"Location\") +\n  ylim(0,15) +\n  scale_x_discrete(limits=month.abb)\n\n# Daily Average Chorophyll from East, West, and Santa Monica Buoys\nggplot(data = chloro_wind_sst, aes(x = date, y = mean_chloro, color = location)) +\n  geom_line() +\n  labs(x = \"Date\",\n       y = \"Daily Average Chlorophyll (mg m^-3)\",\n       title = \"Daily Average Chlorophyll levels from East, West, and Santa Monica Buoys\",\n       color = \"Location\")\n\n\n\nMonthly wind in the Santa Barbara Channel\nin 2020, recorded by in-situ NOAA buoysSea surface temperature in the Santa\nBarbara Channel in 2020, remotely sensed by satellitesChlorophyll in the Santa Barbara Channel\nin 2020, remotely sensed by satellitesInterpretation\nIn the Santa Barbara Channel, the wind peaks in July. This aligns\nwith low chlorophyll levels and about average sea surface\ntemperature.\nThe sea surface temperature peaked in October. This somewhat aligns\nwith the start of the well-known whale watching season that spans from\nNovember to April. The whales are following warm water and food, after\nall!\nThe chlorophyll peaked in April. This aligns with the well-known\nwhale watching season that spans from November to April. The data shows\nthat we would have the best luck whale watching in Santa Barbara in\nApril.\nAcknowledgements\nI would like to thank my exceptionally driven and creative team of\ncollaborators on this project, Grace Lewin, Jake Eisaguirre, and Connor\nFlynn, good friends of mine from the Environmental Data Science program\nat the Bren School of Environmental Science and Management. Thank you\nfor your time, resources, and all the energy you put into this code and\nanalysis.\nThank you to Julien Brun, a Senior Data Scientist at the National Center for Ecological\nAnalysis and Synthesis, for teaching us about how to use API’s as\nwel as locate and utilize metadata. We admire Julien for his dedication\nto open-source science and instilling reproducible habits in the next\ngeneration of scientists.\nThank you to NOAA for providing the data that made this analysis\npossible and for providing an API to import it with ease.\nReferences\nChannel\nIslands National Park\nNational\nPark Service\nOcean\nSunfish\nSanta\nBarbara: The American Riveria\nNOAA\nAquamodis Satellite\nNOAA\nAquamodis Satellite metadata\nEast\nBuoy\nEast Buoy\nmetadata\nWest\nBuoy\nWest Buoy\nMetadata\nSanta\nMonica Buoy\nSanta Monica Buoy\nMetadata\n\n\n\n",
    "preview": "posts/2021-11-05-sstchlorowind/Images/sst.png",
    "last_modified": "2022-05-17T18:26:03-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-10-the-casewhen-function/",
    "title": "Tidy data and the case_when() function in R",
    "description": "A gem within the expansive tidyverse.",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-08-23",
    "categories": [
      "R"
    ],
    "contents": "\n\nContents\nTidy data\ncase_when() and Lobster Data\nFirst thing first, import your data\n\n\n\n\nlibrary(tidyverse)\n# use this to tidy the data\nlibrary(janitor)\n# use this to tidy the columns names\n\n\n\nTidy data\nWhenever we start our journey transforming a dataset into an organized format we can interpret and visualize, it helps to to have the data in Tidy format. Tidy data is a structure for data sets that helps R do the most work possible when it comes to analysis, summary statistics, and combining data sets. R’s vectorized functions flourish with rows and columns in Tidy format.\nTidy data has each variable in columns, each observation has its own row, and each cell contains a single value. For the lobster data set, each lobster caught has its own row with each column describing one aspect of that lobster. Each column has a succinct title for the variable it contains, and ideally includes underscores where we would normally have spaces and has no capitalization to make our coding as easy as possible. There should be NA in any cells that do not have values, which is a default that many R functions recognize as default. When we utilize this data, we can easily remove these values in our code by referring to them as NA.\nTidy format encourages collaboration between people and data sets because we are easily able to combine data from different sources using join functions. If the data contains columns with shared variables, R can easily recognize those columns and associate its rows (observations) with the observations of the complementary data set. Using full_join() is a common join function to utilize as it maintains all data from both sources.\nTidy format helps you easily make subsets of your data for specific graphs and summary tables. Consider the filter() and select() functions, which help you subset to only view variable or observations of interest. In these cases, it is especially important to have only one value in each cell and standardize the way you document observations. You always want to record each lobster species with the same spelling, each size with the same number of decimal places, and each date with the same format (such as YYYY-MM-DD). For variables such as length that might need units, always include these units in the column header rather than the cell. This streamlines our coding and keeps cells to a single class. If you include numerical and character values in one cell, it will be documented as a character, which can restrict your analysis process.\nYour data isn’t in Tidy format? That’s alright! Check out the tidyr::pivot_longer() and tidyr::pivot_wider() functions to help you help R help you. In the example below, we have a tribble dataset that is not in Tidy format. We know this because there are multiple columns (A:C) that represent different individuals or observations of the same variable (like dog food brands). We can use pivot_longer() to put the column headers into their own column, rename that column, and pivot their values into their own column while maintaining their association with A, B, and C. Although the resulting tidy data may seem more complex at first galnce, it is easier to convert to a graph and structurally is more organized from a data science perspective.\nTo demonstrate some simple data tidying, lets make a tribble (which is similar to a dataframe) and manipulate it using the pivot_longer() function. In this example tibble, we are comparing the food preferences of 2 dogs. The dog food types are labeled as A, B, and C.\n\n\ndf <- tribble(\n  ~name, ~A, ~B, ~C,\n  \"dog_1\", 4, 5, 6,\n  \"dog_2\", 9, 10, 8\n)\n\ndf\n\n\n# A tibble: 2 × 4\n  name      A     B     C\n  <chr> <dbl> <dbl> <dbl>\n1 dog_1     4     5     6\n2 dog_2     9    10     8\n\nThis dataframe is not in tidy format, because the variable ranking is dispersed between multiple columns. We want a single variable in each column, so lets combine those columns and make it tidier:\n\n# A tibble: 6 × 3\n  name  dog_food_brand ranking\n  <chr> <chr>            <dbl>\n1 dog_1 A                    4\n2 dog_1 B                    5\n3 dog_1 C                    6\n4 dog_2 A                    9\n5 dog_2 B                   10\n6 dog_2 C                    8\n\nWonderful, now let’s apply these Tidy tools to a substantial dataset!\ncase_when() and Lobster Data\nFirst thing first, import your data\n\n# A tibble: 6 × 9\n   year month date       site  transect replicate size_mm num_ao  area\n  <dbl> <dbl> <date>     <chr>    <dbl> <chr>       <dbl>  <dbl> <dbl>\n1  2012     8 2012-08-20 IVEE         3 A              70      0   300\n2  2012     8 2012-08-20 IVEE         3 B              60      0   300\n3  2012     8 2012-08-20 IVEE         3 B              65      0   300\n4  2012     8 2012-08-20 IVEE         3 B              70      0   300\n5  2012     8 2012-08-20 IVEE         3 B              85      0   300\n6  2012     8 2012-08-20 IVEE         3 C              60      0   300\n\nUse the case_when() function to tidy this lobster data and prepare it for visualization. The case_when() function bins continuous data into manually defined categories and adds this categorization it to your data set in the form of new column. It doesn’t change the values in the cells that already exist, and it does not delete any existing data.\nLobsters must be a minimum size in order to harvest, and we can use case_when() to categorize lobsters into size bins based on the legal size minimum for fishing. This function processes each individual lobster (each row) in this dataframe and returns if it is large enough to legally harvest from various locations along the Santa Barbara coast.\n\n\nlobsters_legality <- lobsters %>% \n  mutate(legal = case_when(\n    size_mm >= 79.76 ~ \"yes\",\n    size_mm < 79.76 ~ \"no\")) %>% \n   group_by(site, legal) %>% \n  summarize(site_legal_count = n())\n\n\n\nThis data is ground-breaking! The world needs to see this and understand its implications. In order to plot this fascinating data in a meaningful way, we want to efficiently categorize our lobsters by legality status and color code their relative abundance in our visualization. Considering that the legal minimum size for a lobster is 79.76 (units), this is the threshold we will pass onto R to do the heavy lifting for us.\nUse ggplot() to make a bar graph that color codes the lobster abundance by legality status. We communicate that we want R to color the graph by this variable by passing the argument color = legal within aes(). Manually setting colors is set outside of aes(), but here it is an argument because it is determined by a variable.\n\n\n\nDistill is a publication format for scientific and technical writing, native to the web. Learn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": "posts/2021-08-10-the-casewhen-function/case_when_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-12-27T17:20:34-08:00",
    "input_file": {}
  }
]
