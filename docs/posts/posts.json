[
  {
    "path": "posts/2021-12-25-sqlwindpower/",
    "title": "Using SQL & Python to Calculate Iowa's Wind Power Potential",
    "description": "How many wind turbines can we legally fit into the state of Iowa, and how much renewable energy would that yield?",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-12-25",
    "categories": [
      "Python",
      "SQL"
    ],
    "contents": "\n\nContents\nObjective\nApproach\nLoad Packages & Import Data\nQueries for Siting Constraints\nSubqueries\nMega-Query\nWind Data\nResults\nAcknowledgements\n\n\n\n\n\nObjective\nRenewable energy is a rapidly growing industry in the United States, and we will only rely on it more over time. Wind energy collected by turbines is a wonderful example of a green source of energy that can help America move towards a sustainable future. In order to make this dream a reality, our country must meticulously plan where we can place wind farms. We can use data science to determine how many wind turbines we can fit into certain plots of land while taking urban structures and natural protected spaces into account.\nIn particular, the state of Iowa has ideal environmental conditions for wind farms with its flat landscape and high, consistent wind speeds. This project aims to evaluate the maximum potential annual wind energy production available to the state of Iowa (meaning the quantity of megawatt hours that would be generated by placing as many 3.45 MW wind turbines as possible on all appropriate sites).\nApproach\nThis will be executed by first identifying all land suitable for wind turbine placement, subject to the siting constraints using Python, SQL, and PostGIS/OpenStreetMap. Subsequently, the area of each polygon suitable for wind production can be calculated, along with the number of wind turbines that could be placed in each polygon based on the minimum distance that they need to be from one another. This information will then be used to calculate the total annual energy production that would be realized by the maximum possible number of new turbines.\nWe will be hypothetically using Vestas V136-3.45 MW turbines, which have a 150m hub height.\nLoad Packages & Import Data\nOur goal is to apply queries to a PostGIS database that contains Iowa’s feature data from OpenStreetMap. In order to connect our Jupyter Notebook to the PostGIS database, we use the package sqlalchemy.\nimport sqlalchemy as sa\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport math\nPostGIS Database Connection:\npg_uri_template = 'postgresql+psycopg2://{user}:{pwd}@{host}/{db_name}'\ndb_uri = pg_uri_template.format(user='eds223_students', pwd='eds223', host='128.111.89.111', db_name='osmiowa')\nengine = sa.create_engine(db_uri)\nThere are multiple tables in this engine. Shapefiles can take the form of points, lines, or polygons, and we need to query each of these types separately, so we need three separate PostGIS “connections”.\nosm_polygons = gpd.read_postgis('SELECT * FROM planet_osm_polygon', con = engine, geom_col = 'way')\nosm_lines = gpd.read_postgis('SELECT * FROM planet_osm_line', con = engine, geom_col = 'way')\nosm_points = gpd.read_postgis('SELECT * FROM planet_osm_point', con = engine, geom_col = 'way')\n# we use the column \"way\" because that is the title of the geometry column in this database\nWe also need to make a separate connection to the database to pull in the wind speed data for each geometry, which we will utilize at the end of our querying in order to calculate the wind power production:\nosm_wind = gpd.read_postgis('SELECT * FROM wind_cells_10000', con = engine, geom_col = 'geom')\nQueries for Siting Constraints\nOne cannot legally place wind turbines anywhere they please within the state of Iowa; there are restrictions in place that create restricted buffer zones around homes, airports, protected natural areas, and more. We need to adhere to these restrictions, only considering residential buildings buffer scenario 1:\n Define important variables that you will utilize within the queries:\n# H = turbine height (tower base ↔ tip of vertical rotor), in meters\nH = 150\n# this value can be found on the website for Vestas wind turbines, linked above\n\n# distance required from airports, in meters\naero_dist = 7500\n\n# d = rotor diameter, in meters\nd = 136\n# this value can be found on the website for Vestas wind turbines, linked above\nSubqueries\nSeparate queries are necessary for each feature, as each requires a different buffer zone around their geometries. We assign these SQL queries to Python objects, then we will combine them in the next step. We use the syntax f\"\"\" with triple quotes so we can format the code within the quotes with variables and line breaks for vertical organization.\n# use scenario 1 with 3*H serving as the required distance\nsql_buildings_residential = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway,\nST_BUFFER(way, 3 * {H}) as way \nFROM planet_osm_polygon \nWHERE building IN ('yes', 'residential', 'apartments', 'house', 'static_caravan', 'detached')\nOR landuse = 'residential'\nOR place = 'town'\"\"\"\n\nsql_buildings_non_residential = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway,\nST_BUFFER(way, (3 * {H})) as way\nFROM planet_osm_polygon \nWHERE building NOT IN ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\"\"\n\nsql_aeroway = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway,\nST_BUFFER(way, {aero_dist}) as way\nFROM planet_osm_polygon \nWHERE aeroway IS NOT NULL\"\"\"\n\nsql_military = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway, way\nFROM planet_osm_polygon\nWHERE (landuse = 'military') \nOR military IS NOT NULL\"\"\"\n\nsql_highway = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway,\nST_BUFFER(way, (2 * {H})) as way\nFROM planet_osm_line \nWHERE (railway NOT IN ('abandoned', 'disused')) \nOR highway IN ('motorway', 'trunk', 'primary', 'seconday', 'primary_link', 'second'\"\"\"\n\nsql_leisure = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway, way\nFROM planet_osm_polygon\nWHERE leisure IS NOT NULL \nOR \"natural\" IS NOT NULL\"\"\"\n\nsql_river = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway,\nST_BUFFER(way, (1 * {H})) as way\nFROM planet_osm_line \nWHERE waterway IS NOT NULL\"\"\"\n\nsql_lake = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway, way \nFROM planet_osm_polygon \nWHERE water IS NOT NULL\"\"\"\n\nsql_power_lines = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway,\nST_BUFFER(way, (2 * {H})) as way\nFROM planet_osm_line\nWHERE power IS NOT NULL\"\"\"\n\nsql_power_plants = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway,\nST_BUFFER(way, (1 * {H})) as way\nFROM planet_osm_polygon \nWHERE power IS NOT NULL\"\"\"\n\nsql_wind_turbines = f\"\"\"SELECT osm_id, building, landuse, aeroway, military, highway, railway, leisure, \"natural\",\nwater, waterway,\nST_BUFFER(way, (5 * {d})) as way\nFROM planet_osm_point\nWHERE \"generator:source\" IS NOT NULL\"\"\"\nMega-Query\nCombine the subqueries into one mega-query so we can subtract all these restricted geometries as one unit. These subtracted geometries can be considered a “mask” in spatial data science jargon.\nmask_1 = f\"\"\"{sql_buildings_residential}\nUNION\n{sql_buildings_non_residential}\nUNION\n{sql_aeroway}\nUNION\n{sql_military}\nUNION\n{sql_highway}\nUNION\n{sql_leisure}\nUNION\n{sql_river}\nUNION \n{sql_lake}\nUNION \n{sql_power_lines}\nUNION \n{sql_power_plants}\nUNION\n{sql_wind_turbines}\"\"\"\n\nmask_1_df = gpd.read_postgis(mask_1, con = db_uri, geom_col = 'way')\n\n# take a look at the state of Iowa without this \"mask\" of geometries\nmask_1_df.plot()\n\nWind Data\nSubtract the union of the string constraints from the wind cells so we are left with only those (fractions of) cells that could accommodate new wind turbines. These are “suitable cells”.\nsuitable_cells_1 = osm_wind.overlay(mask_1_df, how = 'difference')\n\n#Find area of each suitable cell/geom in the database\nsuitable_cells_1['suitable_cell_area'] = suitable_cells_1.geom.area\nNow we can calculate the area of the turbine footprint as a circle with a radius of 5 rotor diameters, with the goal of calculating a scenario in which turbine towers must be at least 10 rotor diameters apart.\nCalculate a buffer circle around each wind turbine, called a “turbine footprint”.\nturbine_footprint = math.pi*((5*d)**2)\nCalculate the number of wind turbines that could be placed in each polygon, by dividing each suitable cell by the turbine footprint.\nsuitable_cells_1['n_turbines'] = suitable_cells_1['suitable_cell_area'] / turbine_footprint\nCalculate the total wind energy produced per cell by multiplying the amount of turbines in each suitable cell by the annual wind production for each turbine.\nFormula:E = 2.6 s m-1 v + -5 GWhE = energy production per turbine in GWhv = average annual wind speed in m s-1\nsuitable_cells_1['energy_prod_per_cell'] = suitable_cells_1['n_turbines'] * ((2.6 * suitable_cells_1['wind_speed']) - 5)\nResults\nSum the energy production over all the cells into a single statewide number for residential exclusion distance scenario 1.\ntotal_energy_product_1 = sum(suitable_cells_1['energy_prod_per_cell'])\nThe maximum annual potential wind energy production available to the state of Iowa is 1036574.26 GWh.\nPlease feel free to reach out with any similar applications for this notebook, and I welcome all questions and suggestions about my code. May the wind empower us to continue to study sustainable energy through the lens fo data science!\nAcknowledgements\nI would like to thank my collaborator on this project, Sydney Rilum from the Bren School of Environmental Science and Management, for her contributions. I will always remember the excessive amount of time we spent drawing wind turbine circles on scratch paper and adjusting the mathematical calculations to reflect different rotor diameters and how that changes the wind power potential for the state of Iowa.\nThank you, Dr. James Frew from the Bren School of Environmental Science and Management, for prompting us to investigate this question.\nwind turtbine photo source\n\n\n\n",
    "preview": "posts/2021-12-25-sqlwindpower/pics/iowa_mask.png",
    "last_modified": "2021-12-27T17:19:35-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-24-sstsicpython/",
    "title": "Plotting Sea Surface Temperature and Sea Ice Concentration Over Time in Python",
    "description": "Using satellite data from the National Oceanic and Atmospheric Administration to detect global trends in sea surface temperature and sea ice concentration from 1981-2021.",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-12-24",
    "categories": [
      "Python"
    ],
    "contents": "\n\nContents\nDatasets:\nLoad necessary packages\nImport Sea Surface Temperature Data\nSea Surface Temperature Dataframe Wrangling\nPlot Sea Surface Temperature Over Time\nImport Sea Ice Data\nSea Ice Data Wrangling\nPlotting Sea Ice Data\nCombine the Visualizations\n\nGlobal sea surface temperature and sea ice concentration as raster images\n\nPlotting time series data from 2 different datasets can be tricky in Python. Here, we will plot sea surface temperature (SST) and sea ice concentration (SIC) in the same plot from the years 1981-2021 to visualize how climate change has impacted these environmental variables over time.\nDatasets:\n1. Sea Surface Tmnerpature: NOAA’s 1/4° Daily Optimum Interpolation Sea Surface Temperature (OISST) version 2, or OISSTv2\nData source and metadata: NOAA National Centers for Environmental Information\nNOAA Sea Surface Temperature Optimum Interpolation methods\n2. Sea Ice Concentration: Monthly Mean Hadley Centre Sea Ice and SST dataset version 1 (HadISST1)\nData source and metadata: UCAR/NCAR - DASH Repository\nJournal of Climate article explaining why the datasets were merged\nLoad necessary packages\nimport xarray as xr\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport glob\nImport Sea Surface Temperature Data\nI manually downloaded 40 years of datasets from the sea surface temperature data source, one day from each year (October 17th) and now I will import them with a for loop. Each dataset has a standardized name that only differs in the year it was collected, so the glob() function makes importing them all at once a piece of cake. We can use * as a wild card to represent any year in the file names.\nfilenames = sorted(glob.glob('Data/oisst-avhrr-v02r01*'))\n#Use a for loop to open all data files as datasets (prior to this step, data files were manually pulled from source and stored locally)\n#Take the means of the 'lat' and 'long' because we want to decrease the quantity of this extraneous data so we can easily convert these datasets to dataframes\n#Converting the datasets to dataframes within this for loop will allow us to concatenate them and view the combined data in a way we can easily understand\nnew = [xr.open_dataset(f).mean('lon').mean('lat').to_dataframe(dim_order=None) for f in filenames]\ndf = pd.concat(new)\ndf\n We need to organize the dataframe further before creating matplotlib visualizations for sea surface temperature over time. This dataframe defaulted to using time as an index, so we need to convert time to a column and subset the columns to just time and sea surface temperature.\nSea Surface Temperature Dataframe Wrangling\n#Subset the columns: we choose only 'sst' here because 'time' is considered the index (note: subsetting this dataframe turns the type into a pandas 'series')\ndf_subset = df['sst']\n#reset the 'time' from an index to a column\nsst_means = df_subset.reset_index()\n#subset to get rid of the useless variable \"zlev\"\nsst_means = sst_means[['time', 'sst']]\nsst_means\n\nEnsure that the type of the time variable is datetime64.\n As a last step in processing this dataframe, we will pull the year out of the time variable. We do this because the day and time is consistent for each sea surface temperature file, and therefore it is negligible. Furthermore, will process the sea ice concentration data to only include year, and we need the sea surface temperature time variable to be of the same format as the sea ice concentration time variable.\n# add a column of just the year, so when we pair the sst data with the ice data, the time variable will match\nsst_means['year'] = pd.DatetimeIndex(sst_means['time']).year\nsst_means\n\nPlot Sea Surface Temperature Over Time\nNow that we finished processing the sea surface temperature data, we can plot the sea surface temperature over our time period using matplotlib to see if can recognize a trend. It appears that sea surface temperature fluctuates over short time periods spanning a few years, but over the course of the full 40-year time period there is a clear positive correlation between time and temperature. This trend is what we would expect based on our knowledge of climate change and other sources of data that show rising sea temperatures across the globe.\nplt.figure(figsize=(10,7))\nplt.plot(sst_means['year'], sst_means['sst'], color='red')\nplt.xlabel('Year', fontsize=14)\nplt.ylabel('Sea Surface Temperature (degrees C)', fontsize=14)\nplt.title('Mean global Sea Surface Temperature in October between 1981 and 2020', fontsize=15)\n\nImport Sea Ice Data\nThe sea ice data import process is similar to my code for importing the sea surface temperature data. One of my collaborators on this project, Peter Menzies, wrote the following code to import the sea ice data.\n# read in ice dataset with xarray (prior to this step, data files were manually pulled from source and compiled into a single local netCDF file)\nds_ice = xr.open_dataset(\"Data/MODEL.ICE.HAD187001-198110.OI198111-202109.nc\")\nSea Ice Data Wrangling\n# create empty dataframe with columns year and ice_conc\nice_time = pd.DataFrame(columns=['year', 'ice_conc'])\n# create empty lists to populate with values from dataset\nice_year = []\nice_conc = []\nUse another for loop to pull sea ice values from October of each year 1981 to 2020, and compile them in a dataframe:\nfor i in range(0, 40):\n    # indexing the SEAICE attributes of our years of interest, finding the mean value, and outputting value as a float\n    # 1341 is the index for October 1981 - we need to add 12 in each iteration to get the month from the next year\n    ice_i = float(ice['SEAICE'][1341 + (12 * i)].mean())\n    **# storing the year associated with ice concentration value\n    year_i = 1981 + i\n    # add year to list\n    ice_year.append(year_i)\n    # add ice concentration to list\n    ice_conc.append(ice_i)\n\n# populated dataframe with the two lists as columns\nice_time['year'] = ice_year\nice_time['ice_conc'] = ice_conc\n# check out our resulting dataframe\nice_time\n\nPlotting Sea Ice Data\nplt.figure(figsize=(10,7))\nplt.plot(ice_time['year'], ice_time['ice_conc'])\nplt.xlabel('Year', fontsize=14)\nplt.ylabel('Seaice concentration (%)', fontsize=14)\nplt.title('Mean global seaice concentration in October between 1981 and 2020', fontsize=15)\n\nCombine the Visualizations\nUse matplotlib to combine the sea surface temperature and sea ice concentration data into one figure.\n# use the fig, ax method so that we have flexibility plotting each variable\nfig, ax = plt.subplots(figsize=(15,10))\n# assign a label to the ice data so we can reference it when we create the legend\nice, = ax.plot(ice_time['year'], ice_time['ice_conc'], color='blue')\n# ax represents the axis for the ice data\nax.tick_params(axis='y', labelcolor='blue')\nplt.xlabel('Year', fontsize = 18)\nax.set_ylabel('Sea Ice Concentration (%)', fontsize = 18)\n# ax2 represents the axis for the sea surface temperature data\nax2 = ax.twinx()\n# assign a label to the sst data so we can reference it when we create the legend\nsst, = ax2.plot(sst_means['year'], sst_means['sst'], color='red')\nax2.tick_params(axis='y', labelcolor='red')\nax2.set_ylabel('Sea Surface Temperature (degrees C)', fontsize = 18)\n# create an initial legend so that we can overwrite it with legend with both variables\nleg1 = ax.legend(loc='center left')\nleg2 = ax.legend([ice, sst],['ice','sst'], loc='center left', fontsize = 18)\nax.set_title('Sea Ice Concentration and Sea Surface Temperature 1981-2020', fontsize = 18)\nplt.show()\n\nThis plot and this code is just a portion of a larger environmental programming project completed by myself and my two collaborators Peter Menzies and Ryan Munnikhuis. See our complete repository here, which contains a binder in the README so anyone can run all the code with ease. Check out the anntoated code to learn how to turn these two datasets into rasterized images and GIF’s!\n\n\n\n",
    "preview": "posts/2021-12-24-sstsicpython/pics/combined_graph.png",
    "last_modified": "2022-01-15T11:54:47-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-25-fishingeffortbycountry/",
    "title": "Global Fishing Effort & Covid-19: A Statistical Analysis",
    "description": "A statistical approach to answer the question: How did Covid-19 impact global fishing efforts in 2020?",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-12-02",
    "categories": [
      "R"
    ],
    "contents": "\nA cod fish, one of the top species captured by marine fisheries (8)How did global fishing activity change during the Covid-19 pandemic?\n2020 is a year the world will never forget. Covid-19 spread rapidly across the globe and forced most of humanity into a state of quarantine. The pandemic had clear devastating impacts on economies of all scales. On the other hand, the pandemic boosted some sectors of the economy and increased demand for certain goods. How did Covid-19 impact the global fishing economy? Did fisheries respond to the pandemic by sending fishermen and fisherwomen home to quarantine, and instead allocate resources towards public health initiatives? Alternatively, did some countries see this as an opportunity to fish unregulated in the high seas more than ever before? There is very limited literature that addresses this question, which is likely due to the fact that 2020 was less than a year ago, and any formal scientific studies on this topic might not have had time to be published. Pita et al. 2021 assessed Covid-19’s impact on global marine recreational fisheries via a questionnaire, but this research differs from my statistical analysis in that it did not use vessel data or quantitatively calculate how fishing hours differed over time, and the study only includes 16 countries (5).\nRegulating and documenting fishing activity and other vessel activities across the globe is a major challenge (4). Databases often have substantial gaps due to a lack of reliable data from automatic identification systems and voluntary vessel registration. Global Fishing Watch is an organization that aims to revolutionize the way we monitor fishing activity across the world using remote sensing techniques from satellites combined with automatic identification systems. Global Fishing Watch collects and visualizes global fishing data with the goal of embracing ocean sustainability, transparency, and open-source science. They keep track of vessels from all different countries, including their movements, boat types, and time stamps for fishing and docking at ports (9). Without such efforts to monitor, publicize, and regulate ocean activity, our marine resources are at high risk of depletion. On a global scale, we are fishing faster than fish stocks can naturally replenish. This has severe economic impacts; according to the World Bank Report, the ensuing depletion of marine fish stocks causes economic losses of 50 billion US dollars annually (4). With modern data science and applied statistics, we can better understand fishing activity on a global scale and protect our planet’s marine biodiversity.\nAs an aspiring wildlife biologist and data scientist, I’m interested in applying statistical analysis to Global Fishing Watch data data to learn how different countries’ fishing effort changed in 2020, relative to those countries’ fishing trends in the years leading up to 2020. In this dataset, fishing effort is defined by the amount of hours spent fishing (3). I chose to use this dataset for my statistical analysis because it is already relatively clean, and I know the data is reliable because Global Fishing Watch is a highly respected data resource with highly accurate remotely sensed data that is combined with locally collected automatic identification systems on shore. Furthermore, I am interested in working with Global Fishing Watch and spatial data in the future. This data does not have a spatial component since country is a categorical variable, and the temporal variable is limited to years. The only bias I believe might be present in this data is that it is limited to boats that either voluntarily allow their fishing hours to be tracked (such as through automatic identification systems) as well as boats that have been detected remotely by satellite. With Global Fishing Watch’s expansive open-source data collection, we can approach this question by grouping all vessels’ fishing hours by country, identifying a statistical trend up until 2019, and extrapolating that trend into 2020. By comparing this 2020 prediction to the actual fishing data available for 2020, we can glean how Covid-19 skewed large-scale fishing efforts. I chose this analysis approach because I am familiar with many of these processes through my graduate statistics course, and I believe it will be a simple and accurate way to derive a p-value that will reveal if there is a statistically significant difference between each country’s actual mean fishing effort and their predicted mean fishing effort in 2020. Perhaps the global fishing economy sky-rocketed, plummeted into near nonexistence, or remained unscathed by the pandemic. Quantitative analysis will help provide some insight.\nGlobal Fishing Watch offers an interactive heat map that displays fishing activity across the globe. This visualization has the potential to inspire data scientists, fish enthusiasts, environmental justice advocates, pandemic researchers, and everyone in between to examine fishing activity during a time period of interest.\nGlobal fishing activity from January 1, 2020 through January 1, 2021Global Fishing Watch and their partners also provide an interactive vessel map that allows users to interact with vessel activity across the globe, filter by country, and overlay port locations on coastlines.\n\n\n\n\n\n\nData Cleaning and Wrangling\nGlobal Fishing Watch’s data includes fishing effort and vessel information from 167 countries over the years 2012-2020 (3). First, we select our variables of interest, group by country, and take the fishing effort means per year.\n\n\n\nOur goal is to run a linear regression on each country’s fishing effort over multiple years, but many countries have NA data for certain years. Considering that we have data available for 2012-2020, we can subset these years for the model. We want to minimize the amount of NA values because we will drop all NA rows, and we want to maintain the maximum amount of rows possible (which represent vessels and countries). We want to select a chunk of continuous years leading up to 2020 with minimal data gaps. In order to choose the start year for the time period that we will feed into the linear regression, take a look at the amount of NA values in the years leading up to 2020. It turns out that 2017, 2018, and 2019 have the least amount of NA values, so we will use 2017 to start our 3-year data period to feed to the linear regression. Next, we convert the data into Tidy format using the pivot_longer() function so we can run a time series linear regression analysis.\n\n\n\nOur dates are in years, and currently their class is character from the original dataset. We need these years in date format in order to run a linear regression over time. We will convert these years and remove all countries that only have data for only one or two years, because we need multiple years of data to feed into the regression and we want each country to have equal amounts of data and start in the year 2017.\n\n\n\nLinear Regression\nNow that the data is sufficiently clean and our years are of class date, we can run a time series linear regression on every country’s fishing effort from 2017-2019 and use the output coefficients to glean which direction each country is trending, meaning if the country is fishing more or less over time. We can do this with the do() function, grouping by country. Then we can feed this output into a for loop! Plug in each country’s fishing effort intercept and slope coefficients into a linear equation to predict the fishing effort in 2020 based on that country’s historical trend. Subsequently, we can combine the predicted 2020 fishing effort data with the actual 2020 fishing effort data into a single dataframe to compare by country. We can make a new column that takes the difference of the actual and predicted values, and then add a column that explicitly states whether that country increased or decreased their fishing effort in 2020 relative to their trend leading up to 2020.\n\n\n\nPlotting Actual Fishing Effort versus Predicted Fishing Effort for Malaysia\nWhat does a single country’s fishing trend look like? Let’s consider the country of Malaysia in Southeast Asia. In 2015, Malaysia’s fisheries sector employed 175,980 people and its contribution to national gross domestic product was 1.1%. The fish trade is valued at $1.7 billion (U.S. dollars), and the estimated average consumption of fish is 56.8 kg/person/year. Malaysian fisheries primarily capture shrimp, squid, and fish. Malaysia contributes to the global fish economy through both importing and exporting fish (6).\nWe can make a country-specific fishing effort plot by filtering our combined actual and predicted fishing effort dataframe to just that country and using ggplot().\n\n\n\nMalaysia’s Fishing Effort: Actual vs Predicted 2017-2020Statistical Significance\nIt’s time to run a t-test to determine if there is a statistical difference between the countries’ predicted fishing effort in 2020 and their actual fishing effort in 2020. A t-test is a handy tool in statistics that reveals how significant the differences between groups are. If the difference between the means of two groups could have easily happened by chance, the p-value will be greater than 0.05 (which is the standard threshold in statistics and environmental data science). If it is highly unlikely (less than a 5% chance) that a difference in means at least this extreme could have occurred by chance, the p-value is less than 0.05 and the results are considered statistically significant. A statistically significant outcome allows us to reject our null hypothesis.\nNull Hypothesis: There is no difference between the predicted country-specific predicted fishing effort in 2020 and the actual country-specific fishing effort in 2020. \\[H_{0}: \\mu_{predicted} - \\mu_{actual} = 0\\] Alternative Hypothesis: There is a difference between the predicted country-specific predicted fishing effort in 2020 and the actual country-specific fishing effort in 2020. Because of the pandemic in 2020, I predict that fishing effort decreased, meaning that the actual country-specific fishing effort is less than the predicted country-specific fishing effort. \\[H_{A}: \\mu_{predicted} - \\mu_{actual} \\neq 0\\]\nDon’t forget to convert the data to Tidy format so we can run the t-test!\n\n\n\nt-test outputThe p-value is 0.0000000312, and 0.0000000312 < 0.05, so we can reject our null hypothesis that there is no difference between the predicted country-specific predicted fishing effort in 2020 and the actual country-specific fishing effort in 2020. Many countries clearly changed their fishing effort in 2020 relative to their historical trend!\nSummary: Which countries increased their fishing effort during the pandemic, relative to their trend leading up to 2020?\nTo best visualize this fishing effort data in a table, we can color code the countries that increased their fishing effort as red and color the countries that decreased their fishing effort in green. We can represent this data both on a map as well as a table.\n\n\n\n\n\n\n\n\n\n\n\nDifferences between actual 2020 fishing effort and predicted 2020 fishing effort by country\nThis color-coded table reveals that 85% of the countries included in this analysis decreased their fishing effort during the Covid-19 pandemic in 2020 relative to their fishing trend leading up to 2020, while 15% of the countries included in this analysis increased their fishing effort. The vast majority of countries’ fishing sectors seemed to follow the same stay-at-home order that was enforced across the globe. While this may have had a detrimental impact on the global fish economy, hopefully the marine fish populations we able to recover and thrive during this period of reprieve from human predation. The results of my statistical analysis match the conclusion of a 2021 scientific study investigating the change in marine recreational fishing activity during the first year of the pandemic (5).\nFuture Steps\nIn order to improve this analysis in the future, I recommend using more than three years of fishing effort data to produce a more accurate linear model. Additionally, I would recommend using a different statistical approach instead of iterating a for loop over each country’s fishing effort data, because this method produced outliers and coefficients that did not line up with observed fishing effort data. Lastly, I recommend running this analysis on fishing effort data from other sources in addition to Global Fishing Watch’s data. This will provide certainty that the data is accurate and the results are reproducible.\nThank you for reading my statistical review of global fishing effort during the 2020 Covid-19 pandemic! I hope I have inspired you to run your own time series analysis, t-tests, and create visualizations that help communicate trends in environmental data science. Please feel free to contact me at jscohen@ucsb.edu with any questions, comments, or suggestions. You may also create issues or pull requests for this analysis through GitHub (repository linked below).\nData Availability\nThe data used in this analysis is openly available, but the user must make a free account on the Global Fishing watch website, which can be accessed through this live link:Global Fishing Watch Datasets and Code\nGitHub Repository Live Link\njulietcohen’s GitHub Repository\nAcknowledgements:\nI would like to acknowledge Dr. Tamma Carleton, my professor in Statistics for Environmental Data Science at the U.C. Santa Barbara Bren School for Environmental Science and Management, for all her support throughout this project and this quarter.\nI would also like to thank my peers in the Master of Environmental Data Science Program for being so open to collaboration and supporting each other with resources, programming tools, and open-source science.\nLastly, I would like to thank Global Fishing Watch for inspiring me to give a hoot about global fishing effort by country, and for providing the data that made this project possible.\nResources (live links):\nInteractive World Map of Fishing Activity: Picture 1 and Map Link 1 - Global Fishing Watch\nInteractive World Map of Fishing by Country: Map Link 2 - Global Fishing Watch\nGlobal Fishing Watch: Datasets and Code, Fishing effort data\nnote: Users must make a free account in order to access datasets\nGlobal Fishing Effort (1950–2010): Trends, Gaps, and Implications.\nAnticamara, J. A., R. Watson, A. Gelchu, and D. Pauly. “Global Fishing Effort (1950–2010): Trends, Gaps, and Implications.” Fisheries Research 107, no. 1 (2011): 131–36. https://doi.org/10.1016/j.fishres.2010.10.016.\\\nFirst Assessment of the Impacts of the COVID-19 Pandemic on Global Marine Recreational Fisheries.:\nPita, Pablo, Gillian B. Ainsworth, Bernardino Alba, Antônio B. Anderson, Manel Antelo, Josep Alós, Iñaki Artetxe, et al. “First Assessment of the Impacts of the COVID-19 Pandemic on Global Marine Recreational Fisheries.” Frontiers in Marine Science 8 (2021): 1533. https://doi.org/10.3389/fmars.2021.735741.\\\nMalaysia’s Fisheries Economy\nGoogle Maps: Malaysia\nWikipedia: Top Marine Fisheries Species Captured\nGlobal Fishing Watch - About Us\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": "posts/2021-11-25-fishingeffortbycountry/pictures/fishing_gfw_map_2020.png",
    "last_modified": "2022-01-09T15:16:24-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-01-plotting-shapefiles-in-python/",
    "title": "Plotting Shapefiles on a Basemap in Python: endangered species habitat",
    "description": "The basics for reading in shapefile data, plotting it, and adding a basemap.",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-11-05",
    "categories": [
      "Python"
    ],
    "contents": "\n\nContents\nGetting started with Python and Jupyter Notebooks\nImporting Packages (modules)\nImporting Data\nSetting the Coordinate Reference System\nPlotting Shapefiles on a Basemap\nFuture Analysis\nA Local California Use Case Example\nSources & supplemental information on listed endangered species in the United States:\n\n\n\n\n\nPython is a wonderful programming language to plot shapefiles overlaid on top of basemaps, which gives us the user spatial context and opens doors for deeper analysis. Understanding where points, lines, and polygons exist in the world empowers data scientists to answer questions about correlated variables, consider new questions that arise throughout the analysis process, and communicate conclusions to audiences in an easily-digestible way.\nThis blog post will walk you through this process, starting with installing the an integrated development environment (IDE) Jupyter Notebook and installing the necessary packages. If you prefer to use another IDE such as Visual Studio Code and have already installed the packages geopandas, feel free to jump ahead to the Importing Packages section.\nAs a conservation biologist and an aspiring data scientist, I’m interested in plotting endangered species’ critical habitat ranges throughout the United States. I sourced my data from the U.S. Geological Survey here. I’m a big fan of USGS datasets because they are usually relatively clean and provide comprehensive metadata.\nLet’s map the critical habitat ranges of the Florida Bonneted bat and many other species of concern on a basemap of the United States.\n\nThe Florida bonneted bat, a critically endangered species that has been assigned critical habitat by U.S. Fish and Wildlife. Hopefully, the critical habita is suffificient to help this species recover and reach sustainable population levels. The world would be a better place with more bonneted bats.\nGetting started with Python and Jupyter Notebooks\nTransitioning from R in Rstudioto Python in Jupyter Notebook can be difficult, as is learning any new language. We have to say goodbye to the exceptionally user-friendly RStudio interface with its git graphical user interface, visible environment, and easily accessible console and terminal windows. Let’s dive into the objected oriented language of Python through Jupyter Notebook and the mysterious terminal. Personally, I struggled every step of this journey with lots of syntax errors, merge conflicts, and endless Google searching. I hope this post will help hold your hand as we make this change together.\nFirst things first, if you’re using Jupyter Notebooks to code in Python, you’re gonna have to install Python and Anaconda Navigator and install Anaconda into your home directory or an environment (I recommend making an environment!). (Note: there are plenty of other IDE’s for python, such as visual studio code, which I adopted later in my data science career).\nAs I had a PC when I first started coding in Python, installing Anaconda really threw me and my laptop for a loop. But what better way to learn to use the terminal besides struggling to install stuff you desperately need for graduate school? Errors and trouble-shooting is what makes us stronger data scientists, even if we don’t want to recognize that as we fight the urge to throw our computer out the window. Click here for a great resource to walk you through installing Anaconda. Either Anaconda or Miniconda will work fine whether you have a PC or Mac.\nAfter installing Anaconda, start your journey plotting shapefiles in Python by opening up Jupyter Notebooks. On a Mac, I can activate conda in the regular ternimal with no issue (conda activate) On a PC, I like to do this from the Anaconda Prompt (the Aanconda terminal) because it’s tricky to get my normal command line or Bash to recognize that conda is indeed on my computer. My favorite flow is as follows:\nFrom the start menu, open a terminal through Anaconda Navigator that’s called “Anaconda Prompt”.\nInstall geopandas with conda install geopandas in your base environment (which is the default). If that doesn’t work (which would not surprise me if you’re on a PC), create a new environment to do so. I found the steps on this website to be helpful: https://medium.com/analytics-vidhya/fastest-way-to-install-geopandas-in-jupyter-notebook-on-windows-8f734e11fa2b. I tried installing geopandas in my base environment, but it was difficult to install all the correct versions of all the dependencies, so I decided to take the easy route and just make a new environment for geopandas and any other spatial analysis packages I’ll need. Maybe one day I’ll get familiar with version-specific terminal installations and I will be able to install whatever my heart pleases in my base environment.\nActivate the environment in which you have installed the geopandas package. I named that environment geo_env, so I type activate geo_env.\nNow that I am in my desired environment, I am going to navigate to the folder in my terminal in which I want to open and save my Jupyter Notebook. That command is cd file/path/to/folder. You know this worked if your terminal working directory now has that file path tacked onto the end. This file path step is not required if you want to include relative file paths to import data and save your notebook.\n\nDownload your spatial data files to this folder to make your life easier in 2 minutes when you import your spatial data in Jupyter Notebook.\nOpen Jupyter Notebooks by typing just that: jupyter notebook. This will tell your terminal to pop open Jupyter Notebook in your browser with your folder of choice already open and ready to go.\nIn the upper right side, open a new notebook.\n\nNote: the Anaconda terminal window you used to open this notebook should not be closed during your work session. It must remain open to keep your kernel connected and give you the ability to save! If you need to run any terminal commands after you have already opened this notebook, such as if you need to download a package or check a file location, just open up another terminal window and enter the geo_env environment to do so.\nImporting Packages (modules)\nFor plotting shapefiles, you’ll want to import the following packages:\nimport pandas as pd  \nimport numpy as np  \nimport geopandas as gpd  \nimport matplotlib.pyplot as plt  \nimport contextily as ctx  \nmatplotlib will allow us to plot our data and manipulate the plot as we see fit. contextily will allow us to add a default basemap under our polygons of interest. If you do not already have this installed, consider using a channel and conda forge to install it in the terminal.\nAs a proponent of reproducibility and credditing those who provided the data, I like to include a markdown chunk following my package imports that includes a URL link to where I found my data, along with a citation if necessary and any notes about how I downloaded it:\nData source: US Fish and Wildlife  - contains .shp, .dbf, and .shx polygon files that relate to critical species habiata in the United States\n- I chose the first zip file you see on this site\n- USFW provided great metadata \nImporting Data\nLet’s import your data! Now is the time you’re gonna be thanking yourself for placing your jupyter notebook in the same folder as your data. We will use geopandas to read in the shapefile with your polygons or lines or points of choice (you will not find a combination of these shapes in the same shapefile, because that’s just how the world works). You might take a look at all the data files and feel a little overwhelmed at the choices due to the way that shapefiles and their metadata are stored separately (.shp, .dbf, .shx, .xml, and so on). In this example we are trying to import a shapefile of polygons, so that .shp file is the only one you need to read in:\nImport the data:\ngdf = gpd.read_file('CRITHAB_POLY.shp')\n# Take a look at the first rows:\nprint(gdf.head())  \n# Ask Python how many rows and columns are in this dataframe\nprint(gdf.shape)\nMy only complaint with the head() function is that it returns the first rows in a plain text format:\n\nIf you want to see the first and last few rows of the dataframe in a format that looks more familiar (like how R studio presents dataframes), try just typing the name of the data frame, gdf:\n\nThis shapefile I read in contains polygons that designate the critical habitat ranges for many endangered and threatened species in the United States. I chose to name it gdf for geodataframe. While you can name objects whatever you want, it is helpful to you and to those reading your code to name things meaningfully. Expect that you will be modifying this dataframe as you go through this mapping process (subsetting columns, filtering for certain factor levels, etc.) so you will likely be tacking on more words to gdf to tell these modified versions apart. Start naming things simply and clearly, and get more specific as you process your data.\nCall the column names in the dataset:\nprint(gdf.columns)\nCheck the factor levels for the columns listing_st:\nstatus_levels = gdf.listing_st.unique()\nstatus_levels\nUsing U.S. Fish and Wildlife as an example, now that you know the factor levels of a categorical variable, you can subset for only “endangered” species, only “threatened” species, etc.\n\nPlay around with your dataset a bit; Google some of the species, subset the columns, search for some NA values, or take the average of a numerical column. After you make a structural change, its a good habit to check the status or dimensions of your dataset.\nCheck the number of rows and columns: \nPrint the latitude and longitude pairs that make up a particular polygon: \nSetting the Coordinate Reference System\nAs a last step before you plot, you have to make sure you set the data to the desired coordinate reference system (CRS). This is pretty standard for all kinds of spatial data, since your data might come with an unfamiliar CRS or have no CRS at all if you are making a mask, a raster, or executing similar geospatial processes. For information about coordinate reference systems, check out this guide:https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf\nBut you technically do not need to understand many details about datums and CRS’s for mapping shapefiles, so just for now you should know 3 common CRS’s:\nWGS84 (EPSG: 4326), which is commonly used for GIS data across the globe or across multiple countries\nand\nNAD83 (EPSG:4269), which is most commonly used for federal agencies\nand\nMercator (EPSG: 3857), which is used for tiles from Google Maps, Open Street Maps, and Stamen Maps. I will use this one today because I want to overlay my polygons onto a Google basemap with contextily.\nSet the CRS to ESPG 3857:\ngdf_3857=gdf.to_crs(epsg=3857)\n#Check that the CRS is what you want:  \nprint(gdf_3857.crs)\nThis code may take a minute to run. In Jupyter Notebook, you know that code chunk is still chuggin’ away if you see an asterisk in brackets to the left of the code chunk:\n\nPlotting Shapefiles on a Basemap\nUse the plot() function from matplotlib and make the fill depend on the species name:\ngdf_3857.plot(column='comname',\n             figsize=(20,20))\n\nNote that you did not have to call the package to use the function plot(). Instead, you can name the dataframe which you want to plot, which is gdf_3857 in this case, then specify the function plot() and add arguments and supplemental plot structure changes as you go.\nThe fig size can be whatever you want. 10-20 is usually good enough. You have finer control over the degree of zoom of the map with the arguments xlim() and ylim(), anyway. These polygons are just floating in space, so lets add a basemap to give us geographical context:\ncrit_hab_polys = gdf_3857.plot(column='comname',\n             figsize=(20,20))\nNotice that I used an argument in the plot function, setting the column = 'comname', which is a column within the gdf_3857 geodataframe that specifies the common name for the species in that row. This argument sets a unique color to each common name, which will help us tell the difference between each species’ habitat on the map, even if 1 species’ habitat is composed of multiple polygons.\nctx.add_basemap(crit_hab_polys)\n\n# Set the axis limits to zoom in on just the lower 48 states, rather than viewing the entire world:  \nplt.ylim([2350000,6350000])\nSince the basemap within the contextily package is of the entire world, we need to specify the x-limitations and y-limitations for our map so we can zoom in on the United States to best understand our data. The default x and y units were in the millions, so I specified my units in millions, too. When considering if I should plug in positive or negative values, I considered the way that coordinate reference systems are designed with positive values for North and East, and negative values for South and West. I considered that the United States are north of the equator (which is 0 in the North and South directions), so I should have positive values for the min and max y. As for the magnitude of my values, I simply looked at the map for a starting point and played around with different numbers until I got the view I wanted.\nplt.xlim([-14000000,-6000000])\nNotice that these values are negative. Along similar thinking to how we decided on our y limitation, these negative values are the result of how coordinate systems are designed. Consider the prime meridian (which lies at 0 degrees in the East and West directions) with West being negative. Since the United States are to the West of the prime meridian, we know that the x-range for our zoom should be negative. As for the magnitude, I just palyed around with the numbers until I got the East-West orientation that encompassed the United States. Use the show() function in matplotlib to tell Python to show the most recently created graph:plt.show()\n\nYou did it! Welcome to the wonderful world of geospatial data in Python.\nFuture Analysis\nWith this basic skill mastered, you can now dive deeper into this data to determine if variables are correlated across space. Considering state borders, you might ask which endangered species occupy ciritical habitat in your home state? and investigate the different wildlife protection policies across the United States. Alternatively, you could approach this data from an environmental perspective and ask which natural biomes contain the most critical habitat for these endangered species? Are these habitats degrading at a faster rate than those that contain less critical habitat?\nA Local California Use Case Example\nLiving in Santa Barbara, I’m interested in the critical habitat of the Southern California Steelhead Trout. Steelhead trout is a beautiful fish species that interbreeds with resident rainbow trout in many coastal regions throughout California, which are split into distinct population segments and managed as such.\nSouthern California Steelhead Trout distinct population segment map with watershed landmarksI was lucky enough to conduct field work with these migratory fish in 2020-2021 through the California Department of Fish and Wildlife and the Pacific States Marine Fisheries Committee. I grew to appreciate their vital role in ecosystem processes and the culture of indigenous people who have interacted with them for centuries. This fish is currently in the process of being listed as a California endangered species starting in December of 2021, which will hopefully expand critical habitat range, increase monitoring of the populations, and help enforce illegal fishing and pollution regulation in their habitat. We can check out the steelhead’s critical habitat range before and after 2021 to see how it expands over time and space.\n\nSouthern California Steelhead Trout with an invasive crayfish in a Southern California freshwater stream\nSources & supplemental information on listed endangered species in the United States:\nFlorida bonneted bat photo\nU.S. Fish and Wildlife data\nU.S. Fish and Wildlife’s Environmental Conservation Online system\nU.S. Fish and Wildlife listed wildlife species\nthis includes links for data on each each species and a map of their habitat\nSouthern California Steelhead Trout\nSouthern California Steelhead Trout distinct population segement map\nSouthern California Steelhead Trout\nPython logo\n\n\n\n",
    "preview": "posts/2021-11-01-plotting-shapefiles-in-python/images/crit_hab_with_basemap.png",
    "last_modified": "2022-01-15T23:34:50-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-05-sstchlorowind/",
    "title": "Using an API to visualize environmental conditions in the Santa Barbara Channel",
    "description": "Interpretting marine processes by combining data sets of sea surface temperature, chlorophyll, and wind.",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-11-05",
    "categories": [
      "R"
    ],
    "contents": "\nThe Santa Barbara Channel\nA basin of oceanic data just waiting to be analyzed\nThe beautiful, biodiverse Santa Barbara coast is known for its sunny weather, gnarly swell, and unique geographic features. Among these are the four northern channel islands, Santa Cruz, Anacapa, Santa Rosa, and San Miguel, which reside between 12 and 27 miles off shore. The Santa Barbara Channel lies between these islands and the coastline, stretching from Los Angeles in the south to Pt. Conception in the north.\nOcean Currents\nThis channel harbors a unique combination of ocean currents that causes heterogeneity in environmental conditions across the islands and circulates nutrients throughout the ocean depths in seaosnal patterns.\nSanta Barbara Channel currents and all eight channel islands (from west to east): San Miguel Island, Santa Rosa Island, Santa Cruz Island, Anacapa Island, San Nicolas Island, Santa Barbara Island, Santa Catalina Island, San Clemente Island (1).The California current brings a cold swell from the Gulf of Alaska down the coast, providing ideal temporal conditions for black rockfish, sunflower sea otters, red abalone, and other creatures around San Miguel and Santa Rosa Islands. In contrast to the southeast-bound California Current is the northwest-bound Southern California Counter-current from Baja California. This warm, relatively nutrient-poor water supports different marine species such as spiny lobsters, moray eels, and damselfish such as California’s state fish: the Garibaldi. These species are more commonly found near the southeast islands (1: Channel Islands National Park, 2: National Park Service).\n\nA garibaldi fish (Hypsypops rubicundus), California’s state fish and a species found in the Santa Barbara Channel. This fish is protected from fishing, lives in kelp forest habitat, and males utilize red algae to build nests (2)\nMarine Wildlife\nThis clashing current rendezvous turns the Santa Barbara Channel into a hotspot for biodiversity such as marine mammals like dolphins and whales, benthic invertebrates like purple urchins, plants such as giant kelp and and unique fish species such as the sunfish (the most majestic fish of all).\n\nA sunfish (Mola mola), the world’s largest bony fish, are found throughout the Santa Barbara Channel and are so large (up to 11 feet in length) can even be spotted by Channel Islands field crew members communting the to islands by helicopter. These atypical fish can even be found as far north as Alaska during El Nino years. These fish often bask nearly motionless near the ocean surface and sometimes breach the surface in an apparent attempt to rid their bodies of external parasites (3).\nDuring late November through April, whale sightings are quite common in Santa Barbara. Thousands of Pacific gray whales migrate south towards the warm waters of Baja California and feed on krill in the channel along the way, which are tiny organisms that rely on oceanic chlorophyll blooms (4). We can detect chlorophyll and sea surface temperature via remote-sensing satellites. While trying to strategically determine the best time of year to spot these whales, we might want to consider the timing of these phytoplankton blooms. Do these blooms occur more often when we have warmer ocean temperatures? What time of year would that be, and does it align with the famous “whale season” known to be November through April? Is the ocean temperature impacted by wind? A few data-driven friends and I decided to combine data about wind, sea surface temperature, and chlorophyll in the Santa Barbara Channel to find the best time of year to go whale watching. My collaborators include Grace Lewin, Jake Eisaguirre, and Connor Flynn, good friends of mine from the Environmental Data Science program at the Bren School of Environmental Science and Management.\nQuestion: How did wind speed affect sea surface temperature and chlorophyll in the Santa Barbara Channel during 2020?\nThe National Oceanic Atmospheric Administration (NOAA)\nMethods\nThe National Oceanic Atmospheric Administration has the perfect datasets to help us out, and they even have a handy application programming interface (API) to do the heavy lifting for us. The NOAA Aquamodis Satellite data can be found here.\nThe REDDAP API will import sea surface temperature and chlorophyll data directly from the NOAA Aquamodis Satellite. In addition, we manually pulled wind speed data from NOAA’s East Buoy, West Buoy, and the Santa Monica Buoy by downloading and decompressing the 2020 Standard Meteorological Data Files.\n\n\nlibrary(rerddap)\n# used to load in the data from NOAA's website\nlibrary(tidyverse)\n# used to clean and visualize data\nlibrary(here)\n# used to read in the data from the current R project\nlibrary(lubridate)\n# use lubridate to work with datetimes (parsing dates and changing the class)\n\n\n\nUse the rerddap API to read in the sea surface and chlorophyll data from NOAA. Assign the respective temperature and chlorophyll data to its respective buoy, then bind the Tidy data together into one dataframe using rbind().\n\n\n# Read in Aqua Modis Data from their website\nrequire(\"rerddap\")\n\n# Sea Surface Temperature for each Buoy\nE_sst <- griddap('erdMWsstd8day_LonPM180', # 8 day composite SST E_buoy\n time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\n latitude = c(34.0, 34.5), #grid surrounding buoy\n longitude = c(-119.5, -120), #grid surrounding buoy\n fmt = \"csv\")  %>% \n  add_column(location = \"east\") #add ID column\n\nW_sst <- griddap('erdMWsstd8day_LonPM180', # 8 day composite SST W_buoy\n time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\n latitude = c(34.0, 34.5), #grid surrounding buoy\n longitude = c(-120, -120.5), #grid surrounding buoy\n fmt = \"csv\") %>% \n  add_column(location = \"west\") #add ID column\n\nSM_sst <- griddap('erdMWsstd8day_LonPM180', # 8 day composite SST SM_buoy\n time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\n latitude = c(33.5, 34.0), #grid surrounding buoy\n longitude = c(-118.75, -119.25), #grid surrounding buoy\n fmt = \"csv\") %>%\n  add_column(location = \"SM\") #add ID column\n\nsst <- rbind(E_sst, W_sst, SM_sst) #bind data\n\n\n\nNow for chlorophyll:\n\n\n# Chloro for each Buoy\nE_chloro <- griddap('erdMWchla8day_LonPM180',  # 8 day composite Chlorophyll E_buoy\n  time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\n  latitude = c(34.0, 34.5), #grid surrounding buoy\n  longitude = c(-119.5, -120), #grid surrounding buoy\n  fmt = \"csv\") %>% \n  add_column(location = \"east\") #add location term\n\nW_chloro <- griddap('erdMWchla8day_LonPM180', # 8 day composite Chlorophyll E_buoy\n  time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\n  latitude = c(34.0, 34.5), #grid surrounding buoy\n  longitude = c(-120, -120.5), #grid surrounding buoy\n  fmt = \"csv\") %>% \n  add_column(location = \"west\") #add location term\n\nSM_chloro <- griddap('erdMWchla8day_LonPM180', # 8 day composite Chlorophyll SM_buoy\n  time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\n  latitude = c(33.5, 34.0), #grid surrounding buoy\n  longitude = c(-118.75, -119.25), #grid surrounding buoy\n  fmt = \"csv\")%>% \n  add_column(location = \"SM\") #add location term\n\nchloro <- rbind(E_chloro, W_chloro, SM_chloro) #Bind data\n\n\n\nWe downlaoded the wind data manually from NOAA’s website, so we import it from the local Rproj, using the here() function. This file path will work on your local machine, helping make this log post as reproducible as possible.\n\n\n# Wind data for each buoy and data cleaning\ntab_E <- read.table(here(\"data\",\"east_wind.txt\"), comment=\"\", header=TRUE) #convert .txt file to .csv\nwrite.csv(tab_E, \"east_wind.csv\", row.names=F, quote=F)\n\nE_wind <- read.csv(here(\"east_wind.csv\")) %>% # read in .csv, select columns and rename\n  add_column(location = \"east\") %>% \n  select(c(\"X.YY\", \"MM\", \"DD\", \"WSPD\", \"location\"))  %>% \n  rename(year = X.YY,\n         month = MM,\n         day = DD)\nE_wind <- E_wind[-c(1),]\n  \n\ntab_W <- read.table(here(\"data\",\"west_wind.txt\"), comment=\"\", header=TRUE) #convert .txt file to .csv\nwrite.csv(tab_W, \"west_wind.csv\", row.names=F, quote=F)\n\nW_wind <- read.csv(here(\"west_wind.csv\"))%>% # read in .csv, select coloumns and rename\n  add_column(location = \"west\") %>% \n  select(c(\"X.YY\", \"MM\", \"DD\", \"WSPD\", \"location\"))  %>% \n  rename(year = X.YY,\n         month = MM,\n         day = DD)\nW_wind <- W_wind[-c(1),]\n\n\ntab_SM <- read.table(here(\"data\",\"SM_wind.txt\"), comment=\"\", header=TRUE) #convert .txt file to .csv\nwrite.csv(tab_SM, \"SM_wind.csv\", row.names=F, quote=F)\n\nSM_wind <- read.csv(here(\"SM_wind.csv\"))%>% # read in .csv, select coloumns and rename\n  add_column(location = \"SM\") %>% \n  select(c(\"X.YY\", \"MM\", \"DD\", \"WSPD\", \"location\"))  %>% \n  rename(year = X.YY,\n         month = MM,\n         day = DD)\nSM_wind <- SM_wind[-c(1),]\n\nwind <- rbind(E_wind, W_wind, SM_wind) #bind data\n\n\n\nMy team averaged the wind by month rather than by day because the wind varies more each day by a large margin. Therefore, the wind data on a daily basis shows lots of noise and no interpretable trends. On a monthly scale, however, we can make sense of its broader fluctuations over the year.\n\n\n# clean date format and summarize with daily means for wind\nwind <- wind %>%\n  unite(\"date\", year:month:day, sep = \"-\") %>% \n  mutate(date = ymd(date, tz = NULL)) %>% \n  mutate(WSPD = as.numeric(WSPD))\n\n# see the data join chunk for na.rm explanation in code comment\nwind_avg <- wind %>% \n  group_by(location, date) %>% \n  summarize(mean_wind = mean(WSPD, na.rm = T))\n\n\n\nHere we cleaned the remotely-sensed sea surface temperature data, summarizing it by day:\n\n\n# clean data for sst date\nsst_clean <- sst %>% \n  mutate(date = ymd_hms(time, tz = \"UTC\")) %>% \n  mutate(ymd_date = ymd(date, tz = NULL)) %>% \n  mutate(date = ymd_date) %>% \n  select(c(\"latitude\", \"longitude\", \"sst\", \"location\", \"date\"))\n\n# Clean sst Data and summarize by daily means\nfinal_sst <- sst_clean %>% \n  filter(sst > 0) %>% # remove NAs\n  mutate(sst = (sst * (9/5) + 32 )) %>% # convert to F...there's probably a function for this\n  mutate(sst = (sst - 3)) # accounting for SST satellite error through anecdotal and buoy comparison. A team member's field experience justifies this as he has consistently cross-referenced the satellite data with in situ measurements \n\n# see the data join chunk for na.rm explanation in code comment\nfinal_sst_avg <- final_sst %>% \n  group_by(location, date) %>% \n  summarize(mean_sst = mean(sst, na.rm = T))\n\n\n\nHere we cleaned the remotely-sensed chlorophyll data, summarizing it by day:\n\n\n# clean chloro data\n# see the data join chunk for na.rm explanation in code comment\nchloro_clean <- chloro %>% \n  mutate(date = ymd_hms(time, tz = \"UTC\")) %>%  # sometimes datetime conversions can be tricky, never forget to check the timezone!\n  mutate(ymd_date = ymd(date, tz = NULL)) %>% \n  mutate(date = ymd_date) %>% \n  select(c(\"latitude\", \"longitude\", \"chlorophyll\", \"location\", \"date\"))\n\nfinal_chloro_avg <- chloro_clean %>% \n  group_by(location, date) %>%\n  summarize(mean_chloro = mean(chlorophyll, na.rm = T))\n\n\n\nWe used inner_join() in two steps to combine the cleaned data from the three variables into one dataframe:\n\n\n# combine daily wind and sst and chloro means\n# we decided to use inner join in order to not include any rows that lack values for ANY of the 3 variables. We do not want any NA values in one col and have data in another col, because when we map everything together that data would be represented as if there was a zero value for the variable that had NA. This change reduced the amount of rows by a couple hundred. This was primarily in the SST and cholorophyll data which had plenty of NA's but the wind data did not initially have NA's.\n\nwind_sst <- inner_join(wind_avg, final_sst_avg, by = c(\"date\", \"location\"))\n\nchloro_wind_sst <- inner_join(wind_sst, final_chloro_avg, by = c(\"date\", \"location\"))\n\n\n\nNow the fun part: visualization! My team and I made three plots, one for each variable. Each plot represents data from all three buoys. We marked the sea surface temperature maximum in all plots since the combined data reveals a temporal correlation between sea surface temperature and wind.\n\n\n# Daily Average Sea Surface Temperature from East, West, and Santa Monica Buoys\nggplot(data = chloro_wind_sst, aes(x = date, y = mean_sst, color = location)) +\n  geom_line() +\n  labs(x = \"Date\",\n       y = \"Daily Average Sea Surface Temperature (degC)\",\n       title = \"Daily Average Sea Surface Temperature from East, West, and Santa Monica Buoys\",\n       color = \"Location\")\n\n# Monthly Average Wind from East, West, and Santa Monica Buoys\nmonth_mean <- chloro_wind_sst %>%\n  select(location, date, mean_wind) %>%\n  mutate(month = month(date, label = TRUE)) %>%\n  mutate(month = as.numeric(month)) %>% \n  group_by(location, month) %>%\n  summarize(mean_wind = mean(mean_wind, na.rm = T)) \n\nggplot(data = month_mean, aes(x = month, y = mean_wind, color = location)) +\n  geom_line() +\n  labs(x = \"Month\",\n       y = \"Monthly Average Wind Speed (knots)\",\n       title = \"Monthly Average Wind Speeds from East, West, and Santa Monica Buoys\",\n       color = \"Location\") +\n  ylim(0,15) +\n  scale_x_discrete(limits=month.abb)\n\n# Daily Average Chorophyll from East, West, and Santa Monica Buoys\nggplot(data = chloro_wind_sst, aes(x = date, y = mean_chloro, color = location)) +\n  geom_line() +\n  labs(x = \"Date\",\n       y = \"Daily Average Chlorophyll (mg m^-3)\",\n       title = \"Daily Average Chlorophyll levels from East, West, and Santa Monica Buoys\",\n       color = \"Location\")\n\n\n\nMonthly wind in the Santa Barbara Channel in 2020, recorded by in-situ NOAA buoysSea surface temperature in the Santa Barbara Channel in 2020, remotely sensed by satellitesChlorophyll in the Santa Barbara Channel in 2020, remotely sensed by satellitesInterpretation\nIn the Santa Barbara Channel, the wind peaks in July. This aligns with low chlorophyll levels and about average sea surface temperature.\nThe sea surface temperature peaked in October. This somewhat aligns with the start of the well-known whale watching season that spans from November to April. The whales are following warm water and food, after all!\nThe chlorophyll peaked in April. This aligns with the well-known whale watching season that spans from November to April. The data shows that we would have the best luck whale watching in Santa Barbara in April.\nAcknowledgements\nI would like to thank my exceptionally driven and creative team of collaborators on this project, Grace Lewin, Jake Eisaguirre, and Connor Flynn, good friends of mine from the Environmental Data Science program at the Bren School of Environmental Science and Management. Thank you for your time, resources, and all the energy you put into this code and analysis.\nThank you to Julien Brun, a Senior Data Scientist at the National Center for Ecological Analysis and Synthesis, for teaching us about how to use API’s and locate and utilize metadata. He helped us with our data cleaning and wrangling throughout this project, and we admire him for his dedication to open-source science and teaching the next generation of data scientists to program in with reproducible habits.\nThank you to NOAA for providing the data that made this analysis possible and for providing an API to import it with ease.\nReferences\nChannel Islands National Park\nNational Park Service\nOcean Sunfish\nSanta Barbara: The American Riveria\nNOAA Aquamodis Satellite\nNOAA Aquamodis Satellite metadata\nEast Buoy\nEast Buoy metadata\nWest Buoy\nWest Buoy Metadata\nSanta Monica Buoy\nSanta Monica Buoy Metadata\n\n\n\n",
    "preview": "posts/2021-11-05-sstchlorowind/Images/sst.png",
    "last_modified": "2022-01-15T22:56:05-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-10-the-casewhen-function/",
    "title": "Tidy data and the case_when() function in R",
    "description": "A gem within the expansive tidyverse.",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-08-23",
    "categories": [
      "R"
    ],
    "contents": "\n\nContents\nTidy data\ncase_when() and Lobster Data\nFirst thing first, import your data\n\n\n\n\nlibrary(tidyverse)\n# use this to tidy the data\nlibrary(janitor)\n# use this to tidy the columns names\n\n\n\nTidy data\nWhenever we start our journey transforming a dataset into an organized format we can interpret and visualize, it helps to to have the data in Tidy format. Tidy data is a structure for data sets that helps R do the most work possible when it comes to analysis, summary statistics, and combining data sets. R’s vectorized functions flourish with rows and columns in Tidy format.\nTidy data has each variable in columns, each observation has its own row, and each cell contains a single value. For the lobster data set, each lobster caught has its own row with each column describing one aspect of that lobster. Each column has a succinct title for the variable it contains, and ideally includes underscores where we would normally have spaces and has no capitalization to make our coding as easy as possible. There should be NA in any cells that do not have values, which is a default that many R functions recognize as default. When we utilize this data, we can easily remove these values in our code by referring to them as NA.\nTidy format encourages collaboration between people and data sets because we are easily able to combine data from different sources using join functions. If the data contains columns with shared variables, R can easily recognize those columns and associate its rows (observations) with the observations of the complementary data set. Using full_join() is a common join function to utilize as it maintains all data from both sources.\nTidy format helps you easily make subsets of your data for specific graphs and summary tables. Consider the filter() and select() functions, which help you subset to only view variable or observations of interest. In these cases, it is especially important to have only one value in each cell and standardize the way you document observations. You always want to record each lobster species with the same spelling, each size with the same number of decimal places, and each date with the same format (such as YYYY-MM-DD). For variables such as length that might need units, always include these units in the column header rather than the cell. This streamlines our coding and keeps cells to a single class. If you include numerical and character values in one cell, it will be documented as a character, which can restrict your analysis process.\nYour data isn’t in Tidy format? That’s alright! Check out the tidyr::pivot_longer() and tidyr::pivot_wider() functions to help you help R help you. In the example below, we have a tribble dataset that is not in Tidy format. We know this because there are multiple columns (A:C) that represent different individuals or observations of the same variable (like dog food brands). We can use pivot_longer() to put the column headers into their own column, rename that column, and pivot their values into their own column while maintaining their association with A, B, and C. Although the resulting tidy data may seem more complex at first galnce, it is easier to convert to a graph and structurally is more organized from a data science perspective.\nTo demonstrate some simple data tidying, lets make a tribble (which is similar to a dataframe) and manipulate it using the pivot_longer() function. In this example tibble, we are comparing the food preferences of 2 dogs. The dog food types are labeled as A, B, and C.\n\n\ndf <- tribble(\n  ~name, ~A, ~B, ~C,\n  \"dog_1\", 4, 5, 6,\n  \"dog_2\", 9, 10, 8\n)\n\ndf\n\n\n# A tibble: 2 × 4\n  name      A     B     C\n  <chr> <dbl> <dbl> <dbl>\n1 dog_1     4     5     6\n2 dog_2     9    10     8\n\nThis dataframe is not in tidy format, because the variable ranking is dispersed between multiple columns. We want a single variable in each column, so lets combine those columns and make it tidier:\n\n# A tibble: 6 × 3\n  name  dog_food_brand ranking\n  <chr> <chr>            <dbl>\n1 dog_1 A                    4\n2 dog_1 B                    5\n3 dog_1 C                    6\n4 dog_2 A                    9\n5 dog_2 B                   10\n6 dog_2 C                    8\n\nWonderful, now let’s apply these Tidy tools to a substantial dataset!\ncase_when() and Lobster Data\nFirst thing first, import your data\n\n# A tibble: 6 × 9\n   year month date       site  transect replicate size_mm num_ao  area\n  <dbl> <dbl> <date>     <chr>    <dbl> <chr>       <dbl>  <dbl> <dbl>\n1  2012     8 2012-08-20 IVEE         3 A              70      0   300\n2  2012     8 2012-08-20 IVEE         3 B              60      0   300\n3  2012     8 2012-08-20 IVEE         3 B              65      0   300\n4  2012     8 2012-08-20 IVEE         3 B              70      0   300\n5  2012     8 2012-08-20 IVEE         3 B              85      0   300\n6  2012     8 2012-08-20 IVEE         3 C              60      0   300\n\nUse the case_when() function to tidy this lobster data and prepare it for visualization. The case_when() function bins continuous data into manually defined categories and adds this categorization it to your data set in the form of new column. It doesn’t change the values in the cells that already exist, and it does not delete any existing data.\nLobsters must be a minimum size in order to harvest, and we can use case_when() to categorize lobsters into size bins based on the legal size minimum for fishing. This function processes each individual lobster (each row) in this dataframe and returns if it is large enough to legally harvest from various locations along the Santa Barbara coast.\n\n\nlobsters_legality <- lobsters %>% \n  mutate(legal = case_when(\n    size_mm >= 79.76 ~ \"yes\",\n    size_mm < 79.76 ~ \"no\")) %>% \n   group_by(site, legal) %>% \n  summarize(site_legal_count = n())\n\n\n\nThis data is ground-breaking! The world needs to see this and understand its implications. In order to plot this fascinating data in a meaningful way, we want to efficiently categorize our lobsters by legality status and color code their relative abundance in our visualization. Considering that the legal minimum size for a lobster is 79.76 (units), this is the threshold we will pass onto R to do the heavy lifting for us.\nUse ggplot() to make a bar graph that color codes the lobster abundance by legality status. We communicate that we want R to color the graph by this variable by passing the argument color = legal within aes(). Manually setting colors is set outside of aes(), but here it is an argument because it is determined by a variable.\n\n\n\nDistill is a publication format for scientific and technical writing, native to the web. Learn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": "posts/2021-08-10-the-casewhen-function/case_when_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-12-27T17:20:34-08:00",
    "input_file": {}
  }
]
