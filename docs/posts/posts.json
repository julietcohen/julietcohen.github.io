[
  {
    "path": "posts/2021-11-05-sstchlorowind/",
    "title": "Visualizing Sea surface temperature, chlorophyll, and wind in the Santa Barbara Channel",
    "description": "Interpretting marine processes by combining multiple data sets.",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-11-05",
    "categories": [
      "-R"
    ],
    "contents": "\r\nData in the Santa Barbara Channel\r\nThe rocky Santa Barbara coast is known for its beautiful weather, incredible surfing, and unique geographic features. Among these are the 4 channel islands which reside between 12 and 27 miles off shore from Santa Barabara, Ventura, and Oxnard. These include Santa Cruz, Anacapa, Santa Rosa, and San Miguel. The Santa Barbara Channel is a hotspot for biodiversity, including dolphins, sunfish, and whales. During certain seasons, whale sightings are quite common from both boat and from the shore. The whales’ feed on krill in the channel, which rely on chorophyll to bloom. We can detect this chlorophyll via satellites, just as we can detect sea surface temperature. While strategizing the best time of year to spot these whales, we might want to consider the timing of these phytoplankton blooms. Do these blooms occur more often when we have warmer ocean temperatures? What time of year would that be? Is the ocean temperature impacted by wind? A few data-driven friends and I decided to combine data about wind, sea surface temperature, and chlorophyll in the Santa Barbara Channel to find the best time of year to go whale watching.\r\nQuestion: How did wind speed affect sea surface temperature and chlorophyll in the Santa Barbara Channel during 2020?\r\nThe National Oceanic Atmospheric Administration (NOAA)\r\nMethods\r\nThe National Oceanic Atmospheric Administration has the perfect datasets to help us out, and they even have a handy application programming interface (API) to do the heavy lifting. Her\r\nThe REDDAP API will pull sea surface temperature and clorophyll data from the NOAA Aquamodis Satelite into Rstudio. In addition, we manually pulled wind speed data from NOAA’s East Buoy, West Buoy, and the Santa Monica Buoy by downloading and decompressing the 2020 Standard Meteorological Data Files available online (see below for links). To coordinate all the data for our analysis, we used the function rbind() to combine the datasets with time as the primary key.\r\n\r\n\r\nlibrary(rerddap)\r\n# used to load in the data from NOAA's website\r\nlibrary(tidyverse)\r\n# used to clean and visualize data\r\nlibrary(here)\r\n# used to read in the data from the current R project\r\nlibrary(lubridate)\r\n# use lubridate to work with datetimes (parsing dates and changing the class)\r\n\r\n\r\n\r\n\r\n\r\n# Read in Aqua Modis Data from their website\r\nrequire(\"rerddap\")\r\n\r\n# Sea Surface Temperature for each Buoy\r\nE_sst <- griddap('erdMWsstd8day_LonPM180', # 8 day composite SST E_buoy\r\n time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\r\n latitude = c(34.0, 34.5), #grid surrounding buoy\r\n longitude = c(-119.5, -120), #grid surrounding buoy\r\n fmt = \"csv\")  %>% \r\n  add_column(location = \"east\") #add ID column\r\n\r\nW_sst <- griddap('erdMWsstd8day_LonPM180', # 8 day composite SST W_buoy\r\n time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\r\n latitude = c(34.0, 34.5), #grid surrounding buoy\r\n longitude = c(-120, -120.5), #grid surrounding buoy\r\n fmt = \"csv\") %>% \r\n  add_column(location = \"west\") #add ID column\r\n\r\nSM_sst <- griddap('erdMWsstd8day_LonPM180', # 8 day composite SST SM_buoy\r\n time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\r\n latitude = c(33.5, 34.0), #grid surrounding buoy\r\n longitude = c(-118.75, -119.25), #grid surrounding buoy\r\n fmt = \"csv\") %>%\r\n  add_column(location = \"SM\") #add ID column\r\n\r\nsst <- rbind(E_sst, W_sst, SM_sst) #bind data\r\n\r\n#----------------------------------------------------------------------------------------\r\n\r\n# Chloro for each Buoy\r\nE_chloro <- griddap('erdMWchla8day_LonPM180',  # 8 day composite Chlorophyll E_buoy\r\n  time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\r\n  latitude = c(34.0, 34.5), #grid surrounding buoy\r\n  longitude = c(-119.5, -120), #grid surrounding buoy\r\n  fmt = \"csv\") %>% \r\n  add_column(location = \"east\") #add location term\r\n\r\nW_chloro <- griddap('erdMWchla8day_LonPM180', # 8 day composite Chlorophyll E_buoy\r\n  time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\r\n  latitude = c(34.0, 34.5), #grid surrounding buoy\r\n  longitude = c(-120, -120.5), #grid surrounding buoy\r\n  fmt = \"csv\") %>% \r\n  add_column(location = \"west\") #add location term\r\n\r\nSM_chloro <- griddap('erdMWchla8day_LonPM180', # 8 day composite Chlorophyll SM_buoy\r\n  time = c('2020-01-01T12:00:00Z','2021-01-01T12:00:00Z'), # Full year time period 2020\r\n  latitude = c(33.5, 34.0), #grid surrounding buoy\r\n  longitude = c(-118.75, -119.25), #grid surrounding buoy\r\n  fmt = \"csv\")%>% \r\n  add_column(location = \"SM\") #add location term\r\n\r\nchloro <- rbind(E_chloro, W_chloro, SM_chloro) #Bind data\r\n\r\n#----------------------------------------------------------------------------------------\r\n\r\n# Wind data for each buoy and data cleaning\r\ntab_E <- read.table(here(\"data\",\"east_wind.txt\"), comment=\"\", header=TRUE) #convert .txt file to .csv\r\nwrite.csv(tab_E, \"east_wind.csv\", row.names=F, quote=F)\r\n\r\nE_wind <- read.csv(here(\"east_wind.csv\")) %>% # read in .csv, select columns and rename\r\n  add_column(location = \"east\") %>% \r\n  select(c(\"X.YY\", \"MM\", \"DD\", \"WSPD\", \"location\"))  %>% \r\n  rename(year = X.YY,\r\n         month = MM,\r\n         day = DD)\r\nE_wind <- E_wind[-c(1),]\r\n  \r\n\r\ntab_W <- read.table(here(\"data\",\"west_wind.txt\"), comment=\"\", header=TRUE) #convert .txt file to .csv\r\nwrite.csv(tab_W, \"west_wind.csv\", row.names=F, quote=F)\r\n\r\nW_wind <- read.csv(here(\"west_wind.csv\"))%>% # read in .csv, select coloumns and rename\r\n  add_column(location = \"west\") %>% \r\n  select(c(\"X.YY\", \"MM\", \"DD\", \"WSPD\", \"location\"))  %>% \r\n  rename(year = X.YY,\r\n         month = MM,\r\n         day = DD)\r\nW_wind <- W_wind[-c(1),]\r\n\r\n\r\ntab_SM <- read.table(here(\"data\",\"SM_wind.txt\"), comment=\"\", header=TRUE) #convert .txt file to .csv\r\nwrite.csv(tab_SM, \"SM_wind.csv\", row.names=F, quote=F)\r\n\r\nSM_wind <- read.csv(here(\"SM_wind.csv\"))%>% # read in .csv, select coloumns and rename\r\n  add_column(location = \"SM\") %>% \r\n  select(c(\"X.YY\", \"MM\", \"DD\", \"WSPD\", \"location\"))  %>% \r\n  rename(year = X.YY,\r\n         month = MM,\r\n         day = DD)\r\nSM_wind <- SM_wind[-c(1),]\r\n\r\nwind <- rbind(E_wind, W_wind, SM_wind) #bind data\r\n\r\n\r\n\r\nWe averaged the wind by month rather than day, as we did for the other 2 variables, because the wind varies more each day by a large margin. Therefore, the wind data on a daily basis shows lots of noise and no interpretable trends. On a monthly scale, however, we can make sense of its broader fluctuations over the year.\r\n\r\n\r\n# clean date format and summarize with daily means for wind\r\n\r\nwind <- wind %>%\r\n  unite(\"date\", year:month:day, sep = \"-\") %>% \r\n  mutate(date = ymd(date, tz = NULL)) %>% \r\n  mutate(WSPD = as.numeric(WSPD))\r\n\r\n# see the data join chunk for na.rm explanation in code comment\r\n\r\nwind_avg <- wind %>% \r\n  group_by(location, date) %>% \r\n  summarize(mean_wind = mean(WSPD, na.rm = T))\r\n\r\n\r\n\r\nHere we cleaned the remotely-sensed sea surface temperature data and summarized it by day:\r\n\r\n\r\n# clean data for sst date\r\nsst_clean <- sst %>% \r\n  mutate(date = ymd_hms(time, tz = \"UTC\")) %>% \r\n  mutate(ymd_date = ymd(date, tz = NULL)) %>% \r\n  mutate(date = ymd_date) %>% \r\n  select(c(\"latitude\", \"longitude\", \"sst\", \"location\", \"date\"))\r\n\r\n\r\n\r\n\r\n\r\n# Clean sst Data and summarize by daily means\r\nfinal_sst <- sst_clean %>% \r\n  filter(sst > 0) %>% #remove NAs\r\n  mutate(sst = (sst * (9/5) + 32 )) %>% #convert to F\r\n  mutate(sst = (sst - 3)) #accounting for SST satellite error through anecdotal and buoy comparison. Jake's field experience justifies this as he has consistently cross-referenced the satellite data with in situ measurements \r\n\r\n# see the data join chunk for na.rm explanation in code comment\r\n\r\nfinal_sst_avg <- final_sst %>% \r\n  group_by(location, date) %>% \r\n  summarize(mean_sst = mean(sst, na.rm = T))\r\n\r\n\r\n\r\nHere we cleaned the remotely-sensed chlorophyll data and summarized it by day:\r\n\r\n\r\n# clean chloro data\r\n# see the data join chunk for na.rm explanation in code comment\r\n\r\nchloro_clean <- chloro %>% \r\n  mutate(date = ymd_hms(time, tz = \"UTC\")) %>% \r\n  mutate(ymd_date = ymd(date, tz = NULL)) %>% \r\n  mutate(date = ymd_date) %>% \r\n  select(c(\"latitude\", \"longitude\", \"chlorophyll\", \"location\", \"date\"))\r\n\r\nfinal_chloro_avg <- chloro_clean %>% \r\n  group_by(location, date) %>%\r\n  summarize(mean_chloro = mean(chlorophyll, na.rm = T))\r\n\r\n\r\n\r\nWe used inner_join() in 2 steps to combine the cleaned data from the 3 variables into one dataframe:\r\n\r\n\r\n# combine daily wind and sst and chloro means\r\n# we decided to use inner join in order to not include any rows that lack values for ANY of the 3 variables. We do not want any NA values in one col and have data in another col, because when we map everything together that data would be represented as if there was a zero value for the variable that had NA. This change reduced the amount of rows by a couple hundred. This was primarily in the SST and cholorophyll data which had plenty of NA's but the wind data did not initially have NA's.\r\n\r\nwind_sst <- inner_join(wind_avg, final_sst_avg, by = c(\"date\", \"location\"))\r\n\r\nchloro_wind_sst <- inner_join(wind_sst, final_chloro_avg, by = c(\"date\", \"location\"))\r\n\r\n\r\n\r\nNow the fun part: visualization! We made 3 plots separated by variable, representing data from all 3 buoys. We marked the sea surface temperature maximum in all 3 plots since the combined data seems to reveal a temporal correlation between sea surface temperature and wind.\r\n\r\n\r\n# Daily Average Sea Surface Temperature from East, West, and Santa Monica Buoys\r\n\r\nggplot(data = chloro_wind_sst, aes(x = date, y = mean_sst, color = location)) +\r\n  geom_line() +\r\n  labs(x = \"Date\",\r\n       y = \"Daily Average Sea Surface Temperature (degC)\",\r\n       title = \"Daily Average Sea Surface Temperature from East, West, and Santa Monica Buoys\",\r\n       color = \"Location\")\r\n\r\n\r\n\r\n\r\n\r\n# Monthly Average Wind from East, West, and Santa Monica Buoys\r\n\r\nmonth_mean <- chloro_wind_sst %>%\r\n  select(location, date, mean_wind) %>%\r\n  mutate(month = month(date, label = TRUE)) %>%\r\n  mutate(month = as.numeric(month)) %>% \r\n  group_by(location, month) %>%\r\n  summarize(mean_wind = mean(mean_wind, na.rm = T)) \r\n\r\nggplot(data = month_mean, aes(x = month, y = mean_wind, color = location)) +\r\n  geom_line() +\r\n  labs(x = \"Month\",\r\n       y = \"Monthly Average Wind Speed (knots)\",\r\n       title = \"Monthly Average Wind Speeds from East, West, and Santa Monica Buoys\",\r\n       color = \"Location\") +\r\n  ylim(0,15) +\r\n  scale_x_discrete(limits=month.abb)\r\n\r\n# Daily Average Chorophyll from East, West, and Santa Monica Buoys\r\n\r\nggplot(data = chloro_wind_sst, aes(x = date, y = mean_chloro, color = location)) +\r\n  geom_line() +\r\n  labs(x = \"Date\",\r\n       y = \"Daily Average Chlorophyll (mg m^-3)\",\r\n       title = \"Daily Average Chlorophyll levels from East, West, and Santa Monica Buoys\",\r\n       color = \"Location\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-05-sstchlorowind/Images/sst.png",
    "last_modified": "2021-11-05T22:00:09-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-01-plotting-shapefiles-in-python/",
    "title": "Plotting Shapefiles in Python",
    "description": "The basics for reading in shapefile data, plotting it, and adding a basemap.",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-11-01",
    "categories": [
      "-Python"
    ],
    "contents": "\r\nPython and Jupyter Notebooks\r\nTransitioning from R to Python can be difficult, as is learning any new language. We have to say goodbye to the exceptionally user-friendly RStudio interface with its git GUI, visible environment, and easily accessible console. Let’s dive into the objected oriented language of Python through Jupyter Notebooks and the mysterious terminal. Personally, I struggled every step of this journey with lots of syntax errors, merge conflicts, and endless Google searching. I hope this post will help hold your hand as we make this change together.\r\nFirst things first, if you’re using Jupyter Notebooks to code in Python, you’re gonna have to install Anaconda Navigator and install conda into your home directory. As a PC user, installing conda really threw me and my laptop for a loop. But what better way to learn to use the terminal besides struggling to install stuff you desperately need for grad school? Errors and trouble-shooting is what makes us stronger data scientists, even if we don’t want to recognize that as we fight the urge to throw our computer out the window.\r\nAfter installing Anaconda, start your journey plotting shapefiles in Python by opening up Jupyter Notebooks. I like to do this from the Anaconda Prompt (the Aanconda terminal), because I’m working on a PC and it’s tricky to get my normal command line or Bash to recognize that conda is indeed on my computer. My favorite flow is as follows:\r\nFrom the start menu, open a terminal through Anaconda Navigator that’s called “Anaconda Prompt”.\r\nInstall geopandas with conda install geopandas in your base environment (which is the default). If that doesn’t work (which would not surpise me if you’re on a PC), create a new environment to do so. I found the steps on this website to be helpful: https://medium.com/analytics-vidhya/fastest-way-to-install-geopandas-in-jupyter-notebook-on-windows-8f734e11fa2b. I tried installing geopandas in my base environment, but it was difficult to install all the correct versions of all the dependencies, so I decided to take the easy route and just make a new environment for geopandas and any other spatial analysis packages I’ll need. Maybe one day I’ll get familiar with version-specific terminal installations and I will be able to install whatever my heart pleases in my base environment.\r\nActivate the environment in which you have installed the geopandas package. I named that environment geo_env, so I type activate geo_env.\r\nNow that I am in my desired environment, I am going to navigate to the folder in my terminal in which I want to open and save my Jupyter Notebook. That command is cd file/path/to/folder. You know this worked if your terminal working directory now has that file path tacked onto the end. This file path step is not required if you want to include relative file paths to import data and save your notebook. Personally, I do not recommend using relative file paths if you can avoid it in any interface.\r\nDownload your spatial data files to this folder to make your life easier in 2 minutes when you import your spatial data in Jupyter Notebook.\r\nOpen Jupyter Notebooks by typing just that: jupyter notebook. This will tell your terminal to pop open Jupyter Notebook in your browser with your folder of choice already open and ready to go.\r\nIn the upper right side, open a new notebook.\r\nTime to import your packages! For plotting shapefiles, you’ll want to import the following packages:\r\nimport pandas as pd\r\nimport numpy as np\r\nimport geopandas as gpd\r\nimport matplotlib.pyplot as plt\r\nimport contextily as ctx\r\nAs a proponent of reproducibility and credditing those who provided the data, I like to include a markdown chunk following my package imports that includes a URL link to where I found my data, along with a citation if necessary and any notes about how I downloaded it:\r\nData source: US Fish and Wildlife https://ecos.fws.gov/ecp/report/table/critical-habitat.html\r\n- contains .shp, .dbf, and .shx files\r\n- I chose the first zip file you see on this site\r\nImporting Data\r\nLet’s import your data! Now is the time you’re gonna be thanking yourself for placing your jupyter notebook in the same folder as your data. We will use geopandas to read in the shapefile with your polygons or lines or points of choice (you will not find a combination of these shapes in the same shapefile, because that’s just how the world works). You might take a look at all the data files and feel a little overwhelmed at the choices due to the way that shapefiles and their metadata are stored separately (.shp, .dbf, .shx, .xml, and so on). In this example we are trying to import a shapefile of polygons, so that .shp file is the only one you need to read in:\r\nImport the data:gdf = gpd.read_file('CRITHAB_POLY.shp')\r\nTake a look at the first rows:print(gdf.head())\r\nAsk Python how many rows and columns are in this dataframe:print(gdf.shape)\r\nMy only complaint with the head() function is that it returns the first rows in a plain text format. If you want to see the first and last few rows of the dataframe in a format that looks more familiar (like how R studio presents dataframes), try just typing gdf.\r\nThis shapefile I read in contains polygons that designate the critical habitat ranges for many endangered and threatened species in the United States. I chose to name it gdf for geodataframe. While you can name objects whatever you want, it is helpful to you and to those reading your code to name things meaningfully. Expect that you will be modifying this dataframe as you go through this mapping process (subsetting columns, filtering for certain factor levels, etc.) so you will likely be tacking on more words to gdf to tell these modified versions apart. Start naming things simply and clearly, and get more specific as you process your data.\r\nIf your dataset has a lot of columns and you want to call them, you can use:\r\nprint(gdf.columns)\r\nWanna check the different factor levels in this dataset? Run the following:status_levels = gdf.listing_st.unique()status_levels\r\nNow you can subset for only “endangered” species, only “threatened” species, etc.\r\nSetting the Coordinate Reference System\r\nAs a last step before you plot, you have to make sure you are in the desired coordinate reference system (CRS). This is pretty standard for all kinds of spatial data, since your data might come with an unfamiliar CRS or have no CRS at all if you are making a mask, a raster, or executing similar processes. For information about coordinate reference systems, check out this guide:https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf\r\nBut you technically do not need to understand many details about datums and CRS’s for mapping shapefiles, so just for now you should know that two common CRS’s are:\r\nWGS84 (EPSG: 4326), which is commonly used for GIS data across the globe or across multiple countries\r\nand\r\nNAD83 (EPSG:4269), which is most commonly used for federal agencies\r\nand\r\nMercator (EPSG: 3857), which is used for tiles from Google Maps, Open Street Maps, and Stamen Maps. I will use this one because I want to overlay my polygons onto a Google basemap.\r\nSet the crs:gdf_3857=gdf.to_crs(epsg=3857)\r\nCheck that the CRS is what you want:print(gdf_3857.crs)\r\nPlotting Shapefiles\r\nUse the plot() function from matplotlib and make the fill depend on the species name: gdf_3857.plot(column='comname',figsize=(20,20))\r\nThe fig size can be whatever you want. 10-20 is usually good enough. This code should show you the polygons in the geometry column colored differently based on their species name from the “comname” column. These polygons are just floating in space, so lets add a basemap to give us geographical context:\r\ngdf_3857.plot(column='comname',figsize=(20,20))ctx.add_basemap(basic_plot)\r\nSet the axis limits to zoom in on just the lower 48 states, rather than viewing the entire world:plt.ylim([2000000,6000000])plt.xlim([-14000000,-6000000])\r\nUse the show() function in matplotlib to tell Python to show the most recently created graph:plt.show()\r\nYou did it! Welcome to the wonderful world of geospatial data in Python.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-11-05T21:49:23-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-10-the-casewhen-function/",
    "title": "Tidy data and the case_when() function in R",
    "description": "A gem within the expansive tidyverse.",
    "author": [
      {
        "name": "Juliet Cohen",
        "url": {}
      }
    ],
    "date": "2021-08-23",
    "categories": [
      "-R"
    ],
    "contents": "\r\n\r\nContents\r\nTidy data\r\ncase_when() and Lobsters\r\nFirst thing first, import your data\r\n\r\n\r\n\r\n\r\nlibrary(tidyverse)\r\n# use this to tidy the data\r\nlibrary(janitor)\r\n# use this to tidy the columns names\r\n\r\n\r\n\r\nTidy data\r\nWhenever we start our journey transforming a dataset into something we can interpret and visualize, we\r\nThis lobster data set is in Tidy format, as is most of the data sets we work with. Tidy data is a structure for data sets that helps R do the most work possible when it comes to analysis, summary statistics, and combining data sets. R’s vectorized functions flourish with rows and columns in Tidy format.\r\nTidy data has each variable in columns, each observation has its own row, and each cell contains a single value. For the lobster data set, each lobster caught has its own row with each column describing one aspect of that lobster. Each column has a succinct title for the variable it contains, and ideally includes underscores where we would normally have spaces and has no capitalization to make our coding as easy as possible. There should be NA in any cells that do not have values, which is a default that many R functions recognize as default. When we utilize this data, we can easily remove these values in our code by referring to them as NA.\r\nTidy format encourages collaboration between people and data sets because we are easily able to combine data from different sources using join functions. If the data contains columns with shared variables, R can easily recognize those columns and associate its rows (observations) with the observations of the complementary data set. Using full_join() is a common join function to utilize as it maintains all data from both sources.\r\nTidy format helps you easily make subsets of your data for specific graphs and summary tables. Consider the filter() and select() functions, which help you subset to only view variable or observations of interest. In these cases, it is especially important to have only one value in each cell and standardize the way you document observations. You always want to record each lobster species with the same spelling, each size with the same number of decimal places, and each date with the same format (such as YYYY-MM-DD). For variables such as length that might need units, always include these units in the column header rather than the cell. This streamlines our coding and keeps cells to a single class. If you include numerical and character values in one cell, it will be documented as a character, which can restrict your analysis process.\r\nYour data isn’t in Tidy format? That’s alright! Check out the tidyr::pivot_longer() and tidyr::pivot_wider() functions to help you help R help you. In the example below, we have a tribble dataset that is not in Tidy format. We know this because there are multiple columns (A:C) that represent different individuals or observations of the same variable (like dog food brands). We can use pivot_longer() to put the column headers into their own column, rename that column, and pivot their values into their own column while maintaining their association with A, B, and C. Although the resulting tidy data may seem more complex at first galnce, it is easier to convert to a graph and structurally is more organized from a data science perspective.\r\nTo demonstrate some simple data tidying, lets make a tribble (which is similar to a dataframe) and manipulate it using the pivot_longer() function. In this tibble, we are\r\n\r\n\r\ndf <- tribble(\r\n  ~name, ~A, ~B, ~C,\r\n  \"dog_1\", 4, 5, 6,\r\n  \"dog_2\", 9, 10, 8\r\n)\r\n\r\ndf\r\n\r\n\r\n# A tibble: 2 x 4\r\n  name      A     B     C\r\n  <chr> <dbl> <dbl> <dbl>\r\n1 dog_1     4     5     6\r\n2 dog_2     9    10     8\r\n\r\nThis dataframe is not in tidy format, because the variable ranking is dispersed between multiple columns. We want a single variable in each column, so lets combine those columns and make it tidier:\r\n\r\n# A tibble: 6 x 3\r\n  name  dog_food_brand ranking\r\n  <chr> <chr>            <dbl>\r\n1 dog_1 A                    4\r\n2 dog_1 B                    5\r\n3 dog_1 C                    6\r\n4 dog_2 A                    9\r\n5 dog_2 B                   10\r\n6 dog_2 C                    8\r\n\r\ncase_when() and Lobsters\r\nUse this function to help tidy your data and prepare it for plotting. case_when() bins continuous data into manually defined categories and add it to your data set in the form of new column.\r\ncase_when() is used in this example to categorize lobsters into size bins based on the legal size minimum for fishing. This function processes each individual lobster we caught in this dataframe and returns if it is large enough to legally harvest from various locations along the Santa Barbara coast.\r\nFirst thing first, import your data\r\n\r\n# A tibble: 6 x 9\r\n   year month date       site  transect replicate size_mm num_ao  area\r\n  <dbl> <dbl> <date>     <chr>    <dbl> <chr>       <dbl>  <dbl> <dbl>\r\n1  2012     8 2012-08-20 IVEE         3 A              70      0   300\r\n2  2012     8 2012-08-20 IVEE         3 B              60      0   300\r\n3  2012     8 2012-08-20 IVEE         3 B              65      0   300\r\n4  2012     8 2012-08-20 IVEE         3 B              70      0   300\r\n5  2012     8 2012-08-20 IVEE         3 B              85      0   300\r\n6  2012     8 2012-08-20 IVEE         3 C              60      0   300\r\n\r\nThis data is ground-breaking! The world needs to see this and understand its implications. In order to plot this fasciating data in a meaningful way, we want to efficiently categorize our lobsters by legality status and color code their relative abundance in our visualization. Considering that the legal minimum size for a lobster is 79.76 (units), this is the threshold we will pass onto R to do the heavy lifting for us.\r\n\r\n\r\nlobsters_legality <- lobsters %>% \r\n  mutate(legal = case_when(\r\n    size_mm >= 79.76 ~ \"yes\",\r\n    size_mm < 79.76 ~ \"no\")) %>% \r\n   group_by(site, legal) %>% \r\n  summarize(site_legal_count = n())\r\n\r\n\r\n\r\nUse ggplot() to make a bar graph that color codes the lobster abundance by legality status. We communicate that we want R to color the graph by this variable by passing the argument color = legal within aes(). Manually setting colors is set outside of aes(), but here it is an argument because it is determined by a variable.\r\n\r\n\r\n\r\nDistill is a publication format for scientific and technical writing, native to the web. Learn more about using Distill at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-10-the-casewhen-function/case_when_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-11-05T21:49:23-07:00",
    "input_file": {}
  },
  {
    "path": "posts/sst_wind_chloro/",
    "title": "Summary Statistics",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Unknown",
        "url": {}
      }
    ],
    "date": "2021-08-10",
    "categories": [
      "-R"
    ],
    "contents": "\r\n\r\nContents\r\nabline function\r\n\r\nabline function\r\nGiven a graph of penguin body mass as a function of penguin flipper length, the geom_smooth(method = lm) function creates a best fit line for the scatterplot.\r\nWithin the function geom_smooth(), you can specify if you want the line to show a confidence interval with se = FALSE or TRUE.\r\nFurthermore, you can add linetype = “dashed” to change the line type.\r\n\r\n\r\n\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": "posts/sst_wind_chloro/Images/bembe.jpg",
    "last_modified": "2021-09-29T13:49:55-07:00",
    "input_file": {}
  }
]
